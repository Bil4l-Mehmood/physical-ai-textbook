"use strict";(self.webpackChunkphysical_ai_textbook=self.webpackChunkphysical_ai_textbook||[]).push([[909],{6878:function(n,r,e){e.r(r),e.d(r,{assets:function(){return l},contentTitle:function(){return t},default:function(){return m},frontMatter:function(){return o},metadata:function(){return s},toc:function(){return c}});var s=JSON.parse('{"id":"chapter-2/2-3-sensors-unity","title":"Sensor Simulation and High-Fidelity Visualization with Unity","description":"Simulate realistic sensors in Gazebo and render robots with photorealistic graphics in Unity","source":"@site/docs/chapter-2/2-3-sensors-unity.md","sourceDirName":"chapter-2","slug":"/chapter-2/2-3-sensors-unity","permalink":"/physical-ai-textbook/docs/chapter-2/2-3-sensors-unity","draft":false,"unlisted":false,"editUrl":"https://github.com/Bil4l-Mehmood/physical-ai-textbook/edit/main/docs/chapter-2/2-3-sensors-unity.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"sidebar_label":"Lesson 2.3: Sensors & Unity","title":"Sensor Simulation and High-Fidelity Visualization with Unity","description":"Simulate realistic sensors in Gazebo and render robots with photorealistic graphics in Unity","duration":90,"difficulty":"Intermediate","hardware":["Ubuntu 22.04 LTS","ROS 2 Humble","Gazebo Harmonic","Unity 2022+ (optional)"],"prerequisites":["Lesson 2.1: Gazebo Fundamentals","Lesson 2.2: URDF/SDF & Physics"]},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2.2: URDF/SDF & Physics","permalink":"/physical-ai-textbook/docs/chapter-2/2-2-urdf-sdf-physics"},"next":{"title":"Lesson 3.1: NVIDIA Isaac Sim","permalink":"/physical-ai-textbook/docs/chapter-3/3-1-isaac-sim-basics"}}'),i=e(4848),a=e(8453);const o={sidebar_position:3,sidebar_label:"Lesson 2.3: Sensors & Unity",title:"Sensor Simulation and High-Fidelity Visualization with Unity",description:"Simulate realistic sensors in Gazebo and render robots with photorealistic graphics in Unity",duration:90,difficulty:"Intermediate",hardware:["Ubuntu 22.04 LTS","ROS 2 Humble","Gazebo Harmonic","Unity 2022+ (optional)"],prerequisites:["Lesson 2.1: Gazebo Fundamentals","Lesson 2.2: URDF/SDF & Physics"]},t="Lesson 2.3: Sensor Simulation & Unity Integration",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Part 1: Gazebo Sensor Simulation",id:"part-1-gazebo-sensor-simulation",level:2},{value:"Camera Sensor",id:"camera-sensor",level:3},{value:"LiDAR Sensor (2D Laser Scan)",id:"lidar-sensor-2d-laser-scan",level:3},{value:"IMU Sensor (Accelerometer + Gyroscope)",id:"imu-sensor-accelerometer--gyroscope",level:3},{value:"Part 2: Reading Sensor Data in ROS 2",id:"part-2-reading-sensor-data-in-ros-2",level:2},{value:"Python Script to Process Camera Images",id:"python-script-to-process-camera-images",level:3},{value:"Python Script to Process LiDAR Data",id:"python-script-to-process-lidar-data",level:3},{value:"Python Script to Process IMU Data",id:"python-script-to-process-imu-data",level:3},{value:"Part 3: Unity Integration for High-Fidelity Visualization",id:"part-3-unity-integration-for-high-fidelity-visualization",level:2},{value:"Setting Up ROS-Unity Bridge",id:"setting-up-ros-unity-bridge",level:3},{value:"Unity Script for Robot Control",id:"unity-script-for-robot-control",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Step 1: Create Sensor-Equipped Robot SDF",id:"step-1-create-sensor-equipped-robot-sdf",level:3},{value:"Step 2: Create ROS 2 Sensor Processing Nodes",id:"step-2-create-ros-2-sensor-processing-nodes",level:3},{value:"Step 3: Launch All Sensors",id:"step-3-launch-all-sensors",level:3},{value:"Step 4: Verify Sensor Data",id:"step-4-verify-sensor-data",level:3},{value:"Exercises",id:"exercises",level:3},{value:"Common Sensor Simulation Issues",id:"common-sensor-simulation-issues",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(n){const r={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.header,{children:(0,i.jsx)(r.h1,{id:"lesson-23-sensor-simulation--unity-integration",children:"Lesson 2.3: Sensor Simulation & Unity Integration"})}),"\n",(0,i.jsxs)(r.admonition,{title:"Lesson Overview",type:"info",children:[(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Duration"}),": 90 minutes | ",(0,i.jsx)(r.strong,{children:"Difficulty"}),": Intermediate | ",(0,i.jsx)(r.strong,{children:"Hardware"}),": Ubuntu 22.04 + ROS 2 Humble + Gazebo + Unity (optional)"]}),(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Prerequisites"}),": Lessons 2.1 and 2.2"]}),(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Learning Outcome"}),": Simulate realistic sensors in Gazebo and visualize robots with high-fidelity graphics in Unity"]})]}),"\n",(0,i.jsx)(r.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(r.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Understand sensor types and their simulation models"}),"\n",(0,i.jsx)(r.li,{children:"Configure camera sensors with realistic parameters"}),"\n",(0,i.jsx)(r.li,{children:"Simulate LiDAR point clouds for navigation"}),"\n",(0,i.jsx)(r.li,{children:"Simulate IMU (accelerometer, gyroscope) data"}),"\n",(0,i.jsx)(r.li,{children:"Publish sensor data to ROS 2 topics"}),"\n",(0,i.jsx)(r.li,{children:"Integrate Gazebo with Unity for photorealistic rendering"}),"\n",(0,i.jsx)(r.li,{children:"Use ROS-Unity bridge for real-time data streaming"}),"\n",(0,i.jsx)(r.li,{children:"Optimize sensor simulation performance"}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"part-1-gazebo-sensor-simulation",children:"Part 1: Gazebo Sensor Simulation"}),"\n",(0,i.jsxs)(r.admonition,{title:"Why Simulate Sensors?",type:"note",children:[(0,i.jsx)(r.p,{children:"Real sensors are expensive and slow to debug. Simulation allows:"}),(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Cost reduction"}),": No hardware damage during development"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Iteration speed"}),": Test algorithms at 100x faster than real-time"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Repeatability"}),": Identical conditions for each test"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Realism"}),": Add noise/lag to match real hardware"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Scaling"}),": Simulate multiple robots simultaneously"]}),"\n"]})]}),"\n",(0,i.jsx)(r.h3,{id:"camera-sensor",children:"Camera Sensor"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-xml",children:'\x3c!-- Add to SDF model --\x3e\r\n<link name="camera_link">\r\n  <inertial>\r\n    <mass>0.1</mass>\r\n    <inertia>\r\n      <ixx>0.001</ixx>\r\n      <iyy>0.001</iyy>\r\n      <izz>0.001</izz>\r\n    </inertia>\r\n  </inertial>\r\n\r\n  <visual name="visual">\r\n    <geometry>\r\n      <box>\r\n        <size>0.05 0.05 0.05</size>\r\n      </box>\r\n    </geometry>\r\n  </visual>\r\n\r\n  <collision name="collision">\r\n    <geometry>\r\n      <box>\r\n        <size>0.05 0.05 0.05</size>\r\n      </box>\r\n    </geometry>\r\n  </collision>\r\n\r\n  \x3c!-- Camera Sensor Plugin --\x3e\r\n  <sensor name="camera" type="camera">\r\n    <pose>0 0 0 0 0 0</pose>\r\n    <update_rate>30</update_rate>  \x3c!-- 30 Hz --\x3e\r\n    <camera>\r\n      <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees --\x3e\r\n      <image>\r\n        <width>640</width>\r\n        <height>480</height>\r\n        <format>R8G8B8</format>\r\n      </image>\r\n      <clip>\r\n        <near>0.01</near>  \x3c!-- Minimum distance in meters --\x3e\r\n        <far>300</far>     \x3c!-- Maximum distance in meters --\x3e\r\n      </clip>\r\n      \x3c!-- Lens distortion (optional) --\x3e\r\n      <lens>\r\n        <type>quadratic</type>\r\n        <scale_to_hfov>true</scale_to_hfov>\r\n        <cutoff_angle>1.5707</cutoff_angle>\r\n        <intrinsics>\r\n          <fx>554.254</fx>   \x3c!-- Focal length in pixels --\x3e\r\n          <fy>554.254</fy>\r\n          <cx>320.0</cx>     \x3c!-- Principal point --\x3e\r\n          <cy>240.0</cy>\r\n          <s>0</s>           \x3c!-- Skew --\x3e\r\n        </intrinsics>\r\n      </lens>\r\n    </camera>\r\n\r\n    \x3c!-- ROS 2 Plugin for publishing camera data --\x3e\r\n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\r\n      <ros>\r\n        \x3c!-- Camera topics will be published under this namespace --\x3e\r\n        <namespace>camera</namespace>\r\n        <remapping>image_raw:=image</remapping>\r\n        <remapping>camera_info:=info</remapping>\r\n      </ros>\r\n      <camera_name>camera</camera_name>\r\n      <frame_name>camera_link</frame_name>\r\n      <hack_baseline>0.07</hack_baseline>  \x3c!-- For stereo --\x3e\r\n      <distortion_k1>0.0</distortion_k1>   \x3c!-- Lens distortion --\x3e\r\n      <distortion_k2>0.0</distortion_k2>\r\n      <distortion_k3>0.0</distortion_k3>\r\n      <distortion_t1>0.0</distortion_t1>\r\n      <distortion_t2>0.0</distortion_t2>\r\n    </plugin>\r\n  </sensor>\r\n</link>\r\n\r\n\x3c!-- Joint to attach camera to robot --\x3e\r\n<joint name="camera_joint" type="fixed">\r\n  <parent>base_link</parent>\r\n  <child>camera_link</child>\r\n  <pose>0.05 0 0.1 0 0 0</pose>  \x3c!-- Mount on front-top of robot --\x3e\r\n</joint>\n'})}),"\n",(0,i.jsx)(r.h3,{id:"lidar-sensor-2d-laser-scan",children:"LiDAR Sensor (2D Laser Scan)"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-xml",children:'\x3c!-- 2D LiDAR for range measurements --\x3e\r\n<link name="lidar_link">\r\n  <inertial>\r\n    <mass>0.05</mass>\r\n    <inertia>\r\n      <ixx>0.0001</ixx>\r\n      <iyy>0.0001</iyy>\r\n      <izz>0.0001</izz>\r\n    </inertia>\r\n  </inertial>\r\n\r\n  <visual name="visual">\r\n    <geometry>\r\n      <cylinder>\r\n        <radius>0.03</radius>\r\n        <length>0.05</length>\r\n      </cylinder>\r\n    </geometry>\r\n  </visual>\r\n\r\n  <collision name="collision">\r\n    <geometry>\r\n      <cylinder>\r\n        <radius>0.03</radius>\r\n        <length>0.05</length>\r\n      </cylinder>\r\n    </geometry>\r\n  </collision>\r\n\r\n  \x3c!-- Ray sensor (LIDAR) --\x3e\r\n  <sensor name="lidar" type="ray">\r\n    <pose>0 0 0 0 0 0</pose>\r\n    <update_rate>40</update_rate>  \x3c!-- 40 Hz --\x3e\r\n    <ray>\r\n      <scan>\r\n        <horizontal>\r\n          <samples>360</samples>      \x3c!-- 360 rays --\x3e\r\n          <resolution>1</resolution>  \x3c!-- 1 degree per ray --\x3e\r\n          <min_angle>-3.14159</min_angle>\r\n          <max_angle>3.14159</max_angle>\r\n        </horizontal>\r\n        <vertical>\r\n          <samples>1</samples>        \x3c!-- 2D LIDAR only --\x3e\r\n        </vertical>\r\n      </scan>\r\n      <range>\r\n        <min>0.2</min>              \x3c!-- 20 cm minimum range --\x3e\r\n        <max>30</max>               \x3c!-- 30 m maximum range --\x3e\r\n        <resolution>0.01</resolution>\r\n      </range>\r\n      \x3c!-- Add noise to match real hardware --\x3e\r\n      <noise>\r\n        <type>gaussian</type>\r\n        <mean>0</mean>\r\n        <stddev>0.01</stddev>  \x3c!-- 1 cm std dev --\x3e\r\n      </noise>\r\n    </ray>\r\n\r\n    \x3c!-- ROS 2 Plugin for LaserScan topic --\x3e\r\n    <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\r\n      <ros>\r\n        <namespace>lidar</namespace>\r\n        <remapping>~/out:=scan</remapping>\r\n      </ros>\r\n      <frame_name>lidar_link</frame_name>\r\n    </plugin>\r\n  </sensor>\r\n</link>\r\n\r\n<joint name="lidar_joint" type="fixed">\r\n  <parent>base_link</parent>\r\n  <child>lidar_link</child>\r\n  <pose>0 0 0.15 0 0 0</pose>\r\n</joint>\n'})}),"\n",(0,i.jsx)(r.h3,{id:"imu-sensor-accelerometer--gyroscope",children:"IMU Sensor (Accelerometer + Gyroscope)"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-xml",children:'\x3c!-- IMU for motion sensing --\x3e\r\n<link name="imu_link">\r\n  <inertial>\r\n    <mass>0.01</mass>\r\n    <inertia>\r\n      <ixx>0.00001</ixx>\r\n      <iyy>0.00001</iyy>\r\n      <izz>0.00001</izz>\r\n    </inertia>\r\n  </inertial>\r\n\r\n  \x3c!-- IMU Sensor --\x3e\r\n  <sensor name="imu" type="imu">\r\n    <pose>0 0 0 0 0 0</pose>\r\n    <update_rate>200</update_rate>  \x3c!-- 200 Hz --\x3e\r\n    <imu>\r\n      <angular_velocity>\r\n        <x>\r\n          <noise type="gaussian">\r\n            <mean>0</mean>\r\n            <stddev>0.002</stddev>  \x3c!-- 0.2 deg/s noise --\x3e\r\n          </noise>\r\n        </x>\r\n        <y>\r\n          <noise type="gaussian">\r\n            <mean>0</mean>\r\n            <stddev>0.002</stddev>\r\n          </noise>\r\n        </y>\r\n        <z>\r\n          <noise type="gaussian">\r\n            <mean>0</mean>\r\n            <stddev>0.002</stddev>\r\n          </noise>\r\n        </z>\r\n      </angular_velocity>\r\n      <linear_acceleration>\r\n        <x>\r\n          <noise type="gaussian">\r\n            <mean>0</mean>\r\n            <stddev>0.017</stddev>  \x3c!-- 0.017 m/s^2 noise --\x3e\r\n          </noise>\r\n        </x>\r\n        <y>\r\n          <noise type="gaussian">\r\n            <mean>0</mean>\r\n            <stddev>0.017</stddev>\r\n          </noise>\r\n        </y>\r\n        <z>\r\n          <noise type="gaussian">\r\n            <mean>0</mean>\r\n            <stddev>0.017</stddev>\r\n          </noise>\r\n        </z>\r\n      </linear_acceleration>\r\n    </imu>\r\n\r\n    \x3c!-- ROS 2 Plugin for IMU data --\x3e\r\n    <plugin name="imu_controller" filename="libgazebo_ros_imu_sensor.so">\r\n      <ros>\r\n        <namespace>imu</namespace>\r\n        <remapping>~/out:=data</remapping>\r\n      </ros>\r\n      <initial_orientation_as_reference>false</initial_orientation_as_reference>\r\n      <frame_name>imu_link</frame_name>\r\n    </plugin>\r\n  </sensor>\r\n</link>\r\n\r\n<joint name="imu_joint" type="fixed">\r\n  <parent>base_link</parent>\r\n  <child>imu_link</child>\r\n</joint>\n'})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"part-2-reading-sensor-data-in-ros-2",children:"Part 2: Reading Sensor Data in ROS 2"}),"\n",(0,i.jsx)(r.h3,{id:"python-script-to-process-camera-images",children:"Python Script to Process Camera Images"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nSubscribe to camera images from Gazebo simulation\r\nand process them with OpenCV\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\n\r\n\r\nclass CameraProcessor(Node):\r\n    \"\"\"Process camera images from Gazebo\"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__('camera_processor')\r\n\r\n        # Subscribe to camera topic\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/image',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # OpenCV bridge for ROS Image \u2194 OpenCV Mat conversion\r\n        self.bridge = CvBridge()\r\n\r\n        # For video output\r\n        self.fourcc = cv2.VideoWriter_fourcc(*'mp4v')\r\n        self.out = None\r\n\r\n        self.get_logger().info('Camera processor initialized')\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Process incoming camera frame\"\"\"\r\n        try:\r\n            # Convert ROS Image to OpenCV format\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\r\n\r\n            # Image processing example: edge detection\r\n            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\r\n            edges = cv2.Canny(gray, 100, 200)\r\n\r\n            # Display processed image\r\n            cv2.imshow('Camera Feed', cv_image)\r\n            cv2.imshow('Edges', edges)\r\n            cv2.waitKey(1)\r\n\r\n            # Log statistics\r\n            self.get_logger().debug(\r\n                f'Image: {cv_image.shape}, Mean color: {cv_image.mean():.1f}'\r\n            )\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Image processing failed: {e}')\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    processor = CameraProcessor()\r\n    rclpy.spin(processor)\r\n    processor.destroy_node()\r\n    rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(r.h3,{id:"python-script-to-process-lidar-data",children:"Python Script to Process LiDAR Data"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nSubscribe to LiDAR laser scans and detect obstacles\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan\r\nimport numpy as np\r\n\r\n\r\nclass LiDARProcessor(Node):\r\n    \"\"\"Process LiDAR scans from Gazebo\"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__('lidar_processor')\r\n\r\n        # Subscribe to laser scan\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan,\r\n            '/lidar/scan',\r\n            self.scan_callback,\r\n            10\r\n        )\r\n\r\n        self.obstacle_threshold = 1.0  # Detect obstacles within 1 meter\r\n        self.get_logger().info('LiDAR processor initialized')\r\n\r\n    def scan_callback(self, msg):\r\n        \"\"\"Process incoming laser scan\"\"\"\r\n        try:\r\n            # msg.ranges is array of distances (float)\r\n            ranges = np.array(msg.ranges)\r\n\r\n            # Filter out invalid readings\r\n            valid_ranges = ranges[(ranges > msg.range_min) & (ranges < msg.range_max)]\r\n\r\n            # Detect obstacles\r\n            obstacles = valid_ranges[valid_ranges < self.obstacle_threshold]\r\n\r\n            if len(obstacles) > 0:\r\n                min_distance = np.min(obstacles)\r\n                self.get_logger().warn(\r\n                    f'\u26a0\ufe0f  Obstacle detected! Minimum distance: {min_distance:.2f}m'\r\n                )\r\n                # Could trigger emergency stop or obstacle avoidance here\r\n\r\n                # Find direction of closest obstacle\r\n                closest_idx = np.argmin(ranges)\r\n                angle = msg.angle_min + closest_idx * msg.angle_increment\r\n                self.get_logger().info(f'Obstacle at angle: {angle:.2f} rad')\r\n\r\n            else:\r\n                self.get_logger().debug('No obstacles in range')\r\n\r\n            # Statistics\r\n            self.get_logger().debug(\r\n                f'LiDAR: {len(valid_ranges)} valid readings, '\r\n                f'min={np.min(valid_ranges):.2f}m, '\r\n                f'max={np.max(valid_ranges):.2f}m'\r\n            )\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Scan processing failed: {e}')\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    processor = LiDARProcessor()\r\n    rclpy.spin(processor)\r\n    processor.destroy_node()\r\n    rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(r.h3,{id:"python-script-to-process-imu-data",children:"Python Script to Process IMU Data"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nSubscribe to IMU data and detect motion/orientation\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Imu\r\nimport numpy as np\r\nimport math\r\n\r\n\r\nclass IMUProcessor(Node):\r\n    """Process IMU data from Gazebo"""\r\n\r\n    def __init__(self):\r\n        super().__init__(\'imu_processor\')\r\n\r\n        # Subscribe to IMU\r\n        self.imu_sub = self.create_subscription(\r\n            Imu,\r\n            \'/imu/data\',\r\n            self.imu_callback,\r\n            10\r\n        )\r\n\r\n        self.get_logger().info(\'IMU processor initialized\')\r\n\r\n    def imu_callback(self, msg):\r\n        """Process incoming IMU data"""\r\n        # Linear acceleration (m/s^2)\r\n        accel = msg.linear_acceleration\r\n        accel_magnitude = math.sqrt(accel.x**2 + accel.y**2 + accel.z**2)\r\n\r\n        # Angular velocity (rad/s)\r\n        angular = msg.angular_velocity\r\n        angular_magnitude = math.sqrt(angular.x**2 + angular.y**2 + angular.z**2)\r\n\r\n        # Orientation (quaternion)\r\n        orientation = msg.orientation\r\n\r\n        # Convert quaternion to Euler angles\r\n        roll, pitch, yaw = self.quaternion_to_euler(\r\n            orientation.x, orientation.y, orientation.z, orientation.w\r\n        )\r\n\r\n        self.get_logger().info(\r\n            f\'Acceleration: {accel_magnitude:.2f} m/s^2 | \'\r\n            f\'Rotation: {angular_magnitude:.2f} rad/s | \'\r\n            f\'Orientation: Roll={roll:.2f}, Pitch={pitch:.2f}, Yaw={yaw:.2f}\'\r\n        )\r\n\r\n    @staticmethod\r\n    def quaternion_to_euler(x, y, z, w):\r\n        """Convert quaternion to Euler angles"""\r\n        # Roll (rotation around X axis)\r\n        sin_roll = 2.0 * (w * x + y * z)\r\n        cos_roll = 1.0 - 2.0 * (x * x + y * y)\r\n        roll = math.atan2(sin_roll, cos_roll)\r\n\r\n        # Pitch (rotation around Y axis)\r\n        sin_pitch = 2.0 * (w * y - z * x)\r\n        sin_pitch = np.clip(sin_pitch, -1.0, 1.0)\r\n        pitch = math.asin(sin_pitch)\r\n\r\n        # Yaw (rotation around Z axis)\r\n        sin_yaw = 2.0 * (w * z + x * y)\r\n        cos_yaw = 1.0 - 2.0 * (y * y + z * z)\r\n        yaw = math.atan2(sin_yaw, cos_yaw)\r\n\r\n        return roll, pitch, yaw\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    processor = IMUProcessor()\r\n    rclpy.spin(processor)\r\n    processor.destroy_node()\r\n    rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"part-3-unity-integration-for-high-fidelity-visualization",children:"Part 3: Unity Integration for High-Fidelity Visualization"}),"\n",(0,i.jsx)(r.admonition,{title:"Why Use Both Gazebo and Unity?",type:"tip",children:(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Gazebo"}),": Fast physics simulation, perfect for algorithm development"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Unity"}),": Photorealistic rendering, human-in-the-loop testing, demos"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Together"}),": Realistic simulation + beautiful visualization"]}),"\n"]})}),"\n",(0,i.jsx)(r.h3,{id:"setting-up-ros-unity-bridge",children:"Setting Up ROS-Unity Bridge"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"# Install ROS-Unity bridge in Unity project\r\n# Download from GitHub: https://github.com/Unity-Technologies/ROS-TCP-Connector\r\n\r\n# In Unity:\r\n# 1. Window \u2192 TextMesh Pro \u2192 Import TMP Essential Resources\r\n# 2. Assets \u2192 Import Package \u2192 Custom Package (ROS-TCP-Connector.unitypackage)\r\n# 3. Add ROSConnection GameObject to scene\r\n# 4. Configure ROS Master URI\n"})}),"\n",(0,i.jsx)(r.h3,{id:"unity-script-for-robot-control",children:"Unity Script for Robot Control"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-csharp",children:'using Unity.Robotics.ROSTCPConnector;\r\nusing RosMessageTypes.Std;\r\nusing RosMessageTypes.Sensor;\r\nusing UnityEngine;\r\n\r\npublic class RobotController : MonoBehaviour\r\n{\r\n    private ROSConnection ros;\r\n\r\n    // Robot transform\r\n    private Transform robotBase;\r\n\r\n    // Sensor displays\r\n    private Texture2D cameraImage;\r\n    private RawImage imageDisplay;\r\n\r\n    void Start()\r\n    {\r\n        // Get ROS connection\r\n        ros = ROSConnection.GetOrCreateInstance();\r\n\r\n        // Register subscribers for sensor topics\r\n        ros.Subscribe<Image>("/camera/image", CameraImageCallback);\r\n        ros.Subscribe<LaserScan>("/lidar/scan", LiDARCallback);\r\n        ros.Subscribe<Imu>("/imu/data", IMUCallback);\r\n\r\n        // Find robot in scene\r\n        robotBase = transform.Find("robot_base");\r\n        imageDisplay = GetComponent<RawImage>();\r\n    }\r\n\r\n    void Update()\r\n    {\r\n        // Example: Move robot with keyboard input\r\n        float moveX = Input.GetAxis("Horizontal");\r\n        float moveZ = Input.GetAxis("Vertical");\r\n\r\n        robotBase.Translate(new Vector3(moveX, 0, moveZ) * 5f * Time.deltaTime);\r\n    }\r\n\r\n    void CameraImageCallback(Image image)\r\n    {\r\n        // Convert ROS Image to Unity Texture2D\r\n        // Process and display in UI\r\n        Debug.Log($"Camera received: {image.width}x{image.height}");\r\n    }\r\n\r\n    void LiDARCallback(LaserScan scan)\r\n    {\r\n        Debug.Log($"LiDAR received: {scan.ranges.Length} rays");\r\n\r\n        // Visualize point cloud\r\n        for (int i = 0; i < scan.ranges.Length; i++)\r\n        {\r\n            float distance = (float)scan.ranges[i];\r\n            float angle = (float)(scan.angle_min + i * scan.angle_increment);\r\n\r\n            // Convert polar to Cartesian\r\n            float x = Mathf.Cos(angle) * distance;\r\n            float z = Mathf.Sin(angle) * distance;\r\n\r\n            // Draw point in world\r\n            Debug.DrawLine(robotBase.position, robotBase.position + new Vector3(x, 0, z), Color.green);\r\n        }\r\n    }\r\n\r\n    void IMUCallback(Imu imuData)\r\n    {\r\n        Debug.Log($"IMU acceleration: {imuData.linear_acceleration.x}");\r\n\r\n        // Could update robot orientation based on IMU\r\n        // robotBase.rotation = ...\r\n    }\r\n}\n'})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Task"}),": Create a robot with camera, LiDAR, and IMU sensors and process all sensor data."]}),"\n",(0,i.jsx)(r.h3,{id:"step-1-create-sensor-equipped-robot-sdf",children:"Step 1: Create Sensor-Equipped Robot SDF"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"# Create complete world with sensors\r\nmkdir -p ~/gazebo_ws/src/worlds\r\ncat > ~/gazebo_ws/src/worlds/sensors_world.sdf << 'EOF'\r\n# Copy complete SDF with camera, LiDAR, IMU from Parts 1-2\r\nEOF\n"})}),"\n",(0,i.jsx)(r.h3,{id:"step-2-create-ros-2-sensor-processing-nodes",children:"Step 2: Create ROS 2 Sensor Processing Nodes"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"mkdir -p ~/gazebo_ws/src/sensor_processing\r\n# Copy camera_processor.py, lidar_processor.py, imu_processor.py\r\nchmod +x ~/gazebo_ws/src/sensor_processing/*.py\n"})}),"\n",(0,i.jsx)(r.h3,{id:"step-3-launch-all-sensors",children:"Step 3: Launch All Sensors"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"# Terminal 1: Gazebo with sensors\r\nros2 launch gazebo_ros gazebo.launch.py world:=$HOME/gazebo_ws/src/worlds/sensors_world.sdf\r\n\r\n# Terminal 2: Camera processor\r\npython3 ~/gazebo_ws/src/sensor_processing/camera_processor.py\r\n\r\n# Terminal 3: LiDAR processor\r\npython3 ~/gazebo_ws/src/sensor_processing/lidar_processor.py\r\n\r\n# Terminal 4: IMU processor\r\npython3 ~/gazebo_ws/src/sensor_processing/imu_processor.py\n"})}),"\n",(0,i.jsx)(r.h3,{id:"step-4-verify-sensor-data",children:"Step 4: Verify Sensor Data"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"# Monitor all active topics\r\nros2 topic list\r\n\r\n# View camera images\r\nros2 topic echo /camera/image_raw | head -20\r\n\r\n# View LiDAR ranges\r\nros2 topic echo /lidar/scan | head -20\r\n\r\n# View IMU data\r\nros2 topic echo /imu/data | head -20\n"})}),"\n",(0,i.jsx)(r.h3,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Obstacle Detection"}),": Detect when LiDAR range < 1 meter"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Motion Detection"}),": Log when IMU acceleration > 2 m/s\xb2"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Object Detection"}),": Apply OpenCV blob detection to camera images"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Fall Detection"}),": Use IMU to detect if robot falls over"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Unity Integration"}),": Create a simple Unity scene displaying sensor data"]}),"\n"]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"common-sensor-simulation-issues",children:"Common Sensor Simulation Issues"}),"\n",(0,i.jsxs)(r.table,{children:[(0,i.jsx)(r.thead,{children:(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.th,{children:"Issue"}),(0,i.jsx)(r.th,{children:"Cause"}),(0,i.jsx)(r.th,{children:"Solution"})]})}),(0,i.jsxs)(r.tbody,{children:[(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"No camera images"}),(0,i.jsx)(r.td,{children:"Plugin not loaded"}),(0,i.jsxs)(r.td,{children:["Check ",(0,i.jsx)(r.code,{children:"libgazebo_ros_camera.so"})," path"]})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"LiDAR no data"}),(0,i.jsx)(r.td,{children:"Ray sensor misconfigured"}),(0,i.jsxs)(r.td,{children:["Verify ",(0,i.jsx)(r.code,{children:"<scan>"})," horizontal samples"]})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"IMU values zero"}),(0,i.jsx)(r.td,{children:"World lacks gravity"}),(0,i.jsxs)(r.td,{children:["Ensure gravity in physics ",(0,i.jsx)(r.code,{children:"<gravity>0 0 -9.81</gravity>"})]})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Noise too high"}),(0,i.jsx)(r.td,{children:"stddev too large"}),(0,i.jsx)(r.td,{children:"For cameras use 0.01, for IMU use 0.002"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Sensor slow"}),(0,i.jsx)(r.td,{children:"Update rate too high"}),(0,i.jsx)(r.td,{children:"Start with 30 Hz for camera, 40 for LiDAR"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"ROS topic empty"}),(0,i.jsx)(r.td,{children:"Plugin namespace wrong"}),(0,i.jsx)(r.td,{children:"Check topic names match plugin config"})]})]})]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(r.p,{children:["\u2705 ",(0,i.jsx)(r.strong,{children:"Sensor Types"}),": Camera (RGB), LiDAR (range), IMU (motion)"]}),"\n",(0,i.jsxs)(r.p,{children:["\u2705 ",(0,i.jsx)(r.strong,{children:"Realistic Simulation"}),": Add noise, lag, and field-of-view constraints"]}),"\n",(0,i.jsxs)(r.p,{children:["\u2705 ",(0,i.jsx)(r.strong,{children:"ROS 2 Publishing"}),": Gazebo plugins automatically publish to ROS topics"]}),"\n",(0,i.jsxs)(r.p,{children:["\u2705 ",(0,i.jsx)(r.strong,{children:"Data Processing"}),": Use Python + OpenCV for real-time sensor processing"]}),"\n",(0,i.jsxs)(r.p,{children:["\u2705 ",(0,i.jsx)(r.strong,{children:"Hybrid Simulation"}),": Gazebo for physics + Unity for photorealism"]}),"\n",(0,i.jsxs)(r.p,{children:["\u2705 ",(0,i.jsx)(r.strong,{children:"Sensor Fusion"}),": Combine camera + LiDAR + IMU for robust perception"]}),"\n",(0,i.jsxs)(r.p,{children:["\u2705 ",(0,i.jsx)(r.strong,{children:"Optimization"}),": Balance accuracy vs. computational cost"]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:["\ud83d\udcd6 ",(0,i.jsx)(r.a,{href:"https://gazebosim.org/api/gazebo_plugins/index.html",children:"Gazebo Sensor Plugins"})]}),"\n",(0,i.jsxs)(r.li,{children:["\ud83d\udcd6 ",(0,i.jsx)(r.a,{href:"https://github.com/Unity-Technologies/ROS-TCP-Connector",children:"ROS-Unity Bridge Documentation"})]}),"\n",(0,i.jsxs)(r.li,{children:["\ud83d\udcd6 ",(0,i.jsx)(r.a,{href:"https://docs.ros.org/en/humble/Concepts/Intermediate/About-ROS-2-Messages.html#sensor-messages",children:"Sensor Message Formats"})]}),"\n",(0,i.jsxs)(r.li,{children:["\ud83d\udcd6 ",(0,i.jsx)(r.a,{href:"https://docs.ros.org/en/humble/Tutorials/Computer-Vision/OpenCV-Python-Example.html",children:"OpenCV with ROS 2"})]}),"\n"]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Chapter 2 Complete!"})," Next: Chapter 3 - The AI-Robot Brain (NVIDIA Isaac)"]}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Questions?"})," See ",(0,i.jsx)(r.a,{href:"/physical-ai-textbook/docs/faq",children:"FAQ"})," or ",(0,i.jsx)(r.a,{href:"https://github.com/physical-ai-course/physical-ai-textbook/discussions",children:"GitHub Discussions"})]})]})}function m(n={}){const{wrapper:r}={...(0,a.R)(),...n.components};return r?(0,i.jsx)(r,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:function(n,r,e){e.d(r,{R:function(){return o},x:function(){return t}});var s=e(6540);const i={},a=s.createContext(i);function o(n){const r=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(r):{...r,...n}},[r,n])}function t(n){let r;return r=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:o(n.components),s.createElement(a.Provider,{value:r},n.children)}}}]);