"use strict";(self.webpackChunkphysical_ai_textbook=self.webpackChunkphysical_ai_textbook||[]).push([[585],{8453:function(e,n,t){t.d(n,{R:function(){return a},x:function(){return o}});var s=t(6540);const i={},r=s.createContext(i);function a(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(r.Provider,{value:n},e.children)}},8887:function(e,n,t){t.r(n),t.d(n,{assets:function(){return l},contentTitle:function(){return o},default:function(){return m},frontMatter:function(){return a},metadata:function(){return s},toc:function(){return c}});var s=JSON.parse('{"id":"chapter-4/4-3-hri-safety","title":"Human-Robot Interaction & Safety: Building Trustworthy Robots","description":"Design safe, ethical, and trustworthy human-robot systems with collision detection, safety constraints, and ethical AI frameworks","source":"@site/docs/chapter-4/4-3-hri-safety.md","sourceDirName":"chapter-4","slug":"/chapter-4/4-3-hri-safety","permalink":"/physical-ai-textbook/docs/chapter-4/4-3-hri-safety","draft":false,"unlisted":false,"editUrl":"https://github.com/Bil4l-Mehmood/physical-ai-textbook/edit/main/docs/chapter-4/4-3-hri-safety.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"sidebar_label":"Lesson 4.3: HRI & Safety","title":"Human-Robot Interaction & Safety: Building Trustworthy Robots","description":"Design safe, ethical, and trustworthy human-robot systems with collision detection, safety constraints, and ethical AI frameworks","duration":120,"difficulty":"Advanced","hardware":["Jetson Orin Nano","RealSense D435i","Force/Torque sensors","ROS 2 Humble"],"prerequisites":["Lesson 4.2: Conversational AI & VLA"]},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.2: Conversational AI & VLA","permalink":"/physical-ai-textbook/docs/chapter-4/4-2-conversational-ai"}}'),i=t(4848),r=t(8453);const a={sidebar_position:3,sidebar_label:"Lesson 4.3: HRI & Safety",title:"Human-Robot Interaction & Safety: Building Trustworthy Robots",description:"Design safe, ethical, and trustworthy human-robot systems with collision detection, safety constraints, and ethical AI frameworks",duration:120,difficulty:"Advanced",hardware:["Jetson Orin Nano","RealSense D435i","Force/Torque sensors","ROS 2 Humble"],prerequisites:["Lesson 4.2: Conversational AI & VLA"]},o="Lesson 4.3: Human-Robot Interaction & Safety - Building Trustworthy Robots",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2},{value:"Recommended Safety Sensors",id:"recommended-safety-sensors",level:3},{value:"Safety Architecture",id:"safety-architecture",level:3},{value:"Part 1: Safety Fundamentals &amp; Standards",id:"part-1-safety-fundamentals--standards",level:2},{value:"Relevant Safety Standards",id:"relevant-safety-standards",level:3},{value:"Force Limits for Safe Human-Robot Collaboration",id:"force-limits-for-safe-human-robot-collaboration",level:3},{value:"Part 2: Real-Time Collision Detection &amp; Avoidance",id:"part-2-real-time-collision-detection--avoidance",level:2},{value:"Safety Monitoring Node",id:"safety-monitoring-node",level:3},{value:"Collision Avoidance with Depth Camera",id:"collision-avoidance-with-depth-camera",level:3},{value:"Part 3: Ethical AI &amp; Bias Mitigation",id:"part-3-ethical-ai--bias-mitigation",level:2},{value:"Identifying AI Bias in Robot Systems",id:"identifying-ai-bias-in-robot-systems",level:3},{value:"Bias Detection &amp; Mitigation",id:"bias-detection--mitigation",level:3},{value:"Part 4: Transparent AI Decision-Making",id:"part-4-transparent-ai-decision-making",level:2},{value:"Explainability for Robot Actions",id:"explainability-for-robot-actions",level:3},{value:"Part 5: Real-World Robot Failure Analysis",id:"part-5-real-world-robot-failure-analysis",level:2},{value:"Critical Incidents &amp; Lessons",id:"critical-incidents--lessons",level:3},{value:"Part 6: Production Safety Checklist",id:"part-6-production-safety-checklist",level:2},{value:"Pre-Deployment Safety Audit",id:"pre-deployment-safety-audit",level:3},{value:"Hands-On Exercise: Safety-Critical System Design",id:"hands-on-exercise-safety-critical-system-design",level:2},{value:"Exercise 1: Test Safety Limits",id:"exercise-1-test-safety-limits",level:3},{value:"Exercise 2: Collision Detection Test",id:"exercise-2-collision-detection-test",level:3},{value:"Exercise 3: Bias Detection Validation",id:"exercise-3-bias-detection-validation",level:3},{value:"Common HRI &amp; Safety Errors",id:"common-hri--safety-errors",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Safety Standards &amp; Guidelines",id:"safety-standards--guidelines",level:3},{value:"Ethics in Robotics",id:"ethics-in-robotics",level:3},{value:"Real-World Incidents",id:"real-world-incidents",level:3},{value:"Capstone Project: End-to-End Safe Robot System",id:"capstone-project-end-to-end-safe-robot-system",level:2},{value:"Complete System Architecture",id:"complete-system-architecture",level:3},{value:"Capstone Implementation",id:"capstone-implementation",level:3},{value:"Course Summary",id:"course-summary",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"lesson-43-human-robot-interaction--safety---building-trustworthy-robots",children:"Lesson 4.3: Human-Robot Interaction & Safety - Building Trustworthy Robots"})}),"\n",(0,i.jsxs)(n.admonition,{title:"Lesson Overview",type:"info",children:[(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Duration"}),": 120 minutes | ",(0,i.jsx)(n.strong,{children:"Difficulty"}),": Advanced | ",(0,i.jsx)(n.strong,{children:"Hardware"}),": Full sensor suite with safety monitoring"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Prerequisites"}),": All Chapter 4 lessons (LLM, VLA)"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Learning Outcome"}),": Design and deploy robots that operate safely around humans, with collision avoidance, ethical decision-making, and transparency in AI reasoning"]})]}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understand human-robot interaction (HRI) principles and safety standards"}),"\n",(0,i.jsx)(n.li,{children:"Implement real-time collision detection and avoidance"}),"\n",(0,i.jsx)(n.li,{children:"Design safety constraints and emergency stop mechanisms"}),"\n",(0,i.jsx)(n.li,{children:"Apply ethical AI frameworks to robot decision-making"}),"\n",(0,i.jsx)(n.li,{children:"Identify and mitigate AI bias in robot perception and planning"}),"\n",(0,i.jsx)(n.li,{children:"Handle adversarial inputs and edge cases robustly"}),"\n",(0,i.jsx)(n.li,{children:"Create transparent AI explanations for robot decisions"}),"\n",(0,i.jsx)(n.li,{children:"Analyze real-world robot failure cases"}),"\n",(0,i.jsx)(n.li,{children:"Build production-ready safety architectures"}),"\n",(0,i.jsx)(n.li,{children:"Deploy robots in shared human-robot environments"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,i.jsx)(n.admonition,{title:"Safety is Hardware-Critical",type:"danger",children:(0,i.jsx)(n.p,{children:"Robot safety depends on redundant sensors and fail-safe mechanisms. A single sensor failure must not cause injury."})}),"\n",(0,i.jsx)(n.h3,{id:"recommended-safety-sensors",children:"Recommended Safety Sensors"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Sensor"}),(0,i.jsx)(n.th,{children:"Purpose"}),(0,i.jsx)(n.th,{children:"Specifications"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Force/Torque Sensor"})}),(0,i.jsx)(n.td,{children:"Gripper collision detection"}),(0,i.jsx)(n.td,{children:"6-axis, <50N threshold"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Proximity Sensor"})}),(0,i.jsx)(n.td,{children:"Warning before contact"}),(0,i.jsx)(n.td,{children:"Ultrasonic, 10cm-2m range"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Pressure Pad"})}),(0,i.jsx)(n.td,{children:"Bumper for emergency stop"}),(0,i.jsx)(n.td,{children:"Entire gripper perimeter"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Light Curtain"})}),(0,i.jsx)(n.td,{children:"Restricted area detection"}),(0,i.jsx)(n.td,{children:"Infrared, 0.3-3m range"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Emergency Button"})}),(0,i.jsx)(n.td,{children:"Manual override"}),(0,i.jsx)(n.td,{children:"Red, mushroom style"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Encoder Feedback"})}),(0,i.jsx)(n.td,{children:"Movement verification"}),(0,i.jsx)(n.td,{children:"On each motor"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"safety-architecture",children:"Safety Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"User Input\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 SAFETY CONSTRAINT CHECKER               \u2502\n\u2502 \u2022 Speed limits check                     \u2502\n\u2502 \u2022 Collision detection check              \u2502\n\u2502 \u2022 Authorized workspace check             \u2502\n\u2502 \u2022 Gripper force limits check             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193 [PASS] or [FAIL - STOP]\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 EXECUTION WITH MONITORING                \u2502\n\u2502 \u2022 Real-time force/torque monitoring      \u2502\n\u2502 \u2022 Continuous proximity check              \u2502\n\u2502 \u2022 Verify motion feedback (encoders)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193 [Detect anomaly] \u2192 EMERGENCY STOP\nRobot Executes\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 POST-EXECUTION ANALYSIS                  \u2502\n\u2502 \u2022 Log all sensor data                    \u2502\n\u2502 \u2022 Verify intended vs actual motion      \u2502\n\u2502 \u2022 Update safety parameters if needed     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"part-1-safety-fundamentals--standards",children:"Part 1: Safety Fundamentals & Standards"}),"\n",(0,i.jsx)(n.h3,{id:"relevant-safety-standards",children:"Relevant Safety Standards"}),"\n",(0,i.jsx)(n.p,{children:"Robot safety is governed by international standards:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Standard"}),(0,i.jsx)(n.th,{children:"Focus"}),(0,i.jsx)(n.th,{children:"Key Requirements"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"ISO/TS 15066"})}),(0,i.jsx)(n.td,{children:"Collaborative robots (cobots)"}),(0,i.jsx)(n.td,{children:"Force/torque limits by body part"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"ANSI/RIA R15.06"})}),(0,i.jsx)(n.td,{children:"Industrial robot safety"}),(0,i.jsx)(n.td,{children:"Emergency stops, protective devices"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"EN 61508"})}),(0,i.jsx)(n.td,{children:"Functional safety"}),(0,i.jsx)(n.td,{children:"SIL ratings (1-4), failure modes"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"ISO 13849"})}),(0,i.jsx)(n.td,{children:"Safety control systems"}),(0,i.jsx)(n.td,{children:"PLd/PLe performance levels"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"force-limits-for-safe-human-robot-collaboration",children:"Force Limits for Safe Human-Robot Collaboration"}),"\n",(0,i.jsx)(n.p,{children:"Different body parts have different injury thresholds:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Safe Transient Contact Force (ISO/TS 15066):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Body Part       \u2502 Max Force (N)\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Head            \u2502 40           \u2502\n\u2502 Face            \u2502 27           \u2502\n\u2502 Neck            \u2502 50           \u2502\n\u2502 Chest           \u2502 210          \u2502\n\u2502 Abdomen         \u2502 155          \u2502\n\u2502 Hand            \u2502 220          \u2502\n\u2502 Foot            \u2502 200          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nApplication: Robot gripper max force should be &lt;50N\n            for safe human interaction\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"part-2-real-time-collision-detection--avoidance",children:"Part 2: Real-Time Collision Detection & Avoidance"}),"\n",(0,i.jsx)(n.h3,{id:"safety-monitoring-node",children:"Safety Monitoring Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nReal-Time Safety Monitor\nContinuous collision detection and emergency stop system\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\nfrom geometry_msgs.msg import Twist, WrenchStamped\nfrom std_msgs.msg import Float32, Bool\nimport numpy as np\nimport time\n\nclass SafetyMonitorNode(Node):\n    def __init__(self):\n        super().__init__('safety_monitor')\n\n        # Safety parameters\n        self.SPEED_LIMIT = 0.5  # m/s (walking speed)\n        self.FORCE_LIMIT = 50.0  # Newtons\n        self.GRIPPER_FORCE_LIMIT = 30.0  # Newtons\n        self.PROXIMITY_THRESHOLD = 0.3  # meters (30cm warning distance)\n        self.EMERGENCY_STOP_ACTIVE = False\n\n        # Subscriptions\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            '/joint_states',\n            self.joint_state_callback,\n            10\n        )\n\n        self.ft_sensor_sub = self.create_subscription(\n            WrenchStamped,\n            '/ft_sensor/wrench',\n            self.force_callback,\n            10\n        )\n\n        self.proximity_sub = self.create_subscription(\n            Float32,\n            '/proximity_sensor/distance',\n            self.proximity_callback,\n            10\n        )\n\n        # Publishers\n        self.emergency_stop_pub = self.create_publisher(\n            Bool,\n            '/safety/emergency_stop',\n            10\n        )\n\n        self.velocity_limit_pub = self.create_publisher(\n            Float32,\n            '/safety/velocity_limit',\n            10\n        )\n\n        self.force_limit_pub = self.create_publisher(\n            Float32,\n            '/safety/force_limit',\n            10\n        )\n\n        self.safety_status_pub = self.create_publisher(\n            Bool,\n            '/safety/status_ok',\n            10\n        )\n\n        # State tracking\n        self.joint_velocities = {}\n        self.measured_force = 0.0\n        self.proximity_distance = float('inf')\n        self.last_check_time = time.time()\n\n        # Safety timer\n        self.create_timer(0.05, self.safety_check_callback)  # 50ms = 20Hz\n\n        self.get_logger().info('Safety Monitor initialized')\n\n    def joint_state_callback(self, msg):\n        \"\"\"Track joint velocities\"\"\"\n        for i, name in enumerate(msg.name):\n            if i < len(msg.velocity):\n                self.joint_velocities[name] = msg.velocity[i]\n\n    def force_callback(self, msg):\n        \"\"\"Monitor force/torque sensor\"\"\"\n        # Calculate magnitude of force vector\n        fx = msg.wrench.force.x\n        fy = msg.wrench.force.y\n        fz = msg.wrench.force.z\n        self.measured_force = np.sqrt(fx**2 + fy**2 + fz**2)\n\n    def proximity_callback(self, msg):\n        \"\"\"Monitor proximity sensor\"\"\"\n        self.proximity_distance = msg.data\n\n    def safety_check_callback(self):\n        \"\"\"Continuous safety monitoring\"\"\"\n        violations = []\n\n        # Check 1: Velocity limits\n        for joint, velocity in self.joint_velocities.items():\n            if abs(velocity) > self.SPEED_LIMIT:\n                violations.append(\n                    f'Joint {joint} exceeds speed limit: {velocity:.2f} m/s'\n                )\n\n        # Check 2: Force limits (collision detection)\n        if self.measured_force > self.FORCE_LIMIT:\n            violations.append(\n                f'Force limit exceeded: {self.measured_force:.1f}N > {self.FORCE_LIMIT}N'\n            )\n\n        # Check 3: Proximity warning\n        if self.proximity_distance < self.PROXIMITY_THRESHOLD:\n            self.get_logger().warn(\n                f'Object detected {self.proximity_distance:.2f}m away'\n            )\n\n        # Decide on emergency stop\n        if violations:\n            self.get_logger().error(f'Safety violations: {violations}')\n            self.EMERGENCY_STOP_ACTIVE = True\n\n            # Publish emergency stop\n            stop_msg = Bool()\n            stop_msg.data = True\n            self.emergency_stop_pub.publish(stop_msg)\n\n            # Publish that safety is NOT OK\n            status_msg = Bool()\n            status_msg.data = False\n            self.safety_status_pub.publish(status_msg)\n        else:\n            # Safety OK\n            self.EMERGENCY_STOP_ACTIVE = False\n\n            stop_msg = Bool()\n            stop_msg.data = False\n            self.emergency_stop_pub.publish(stop_msg)\n\n            status_msg = Bool()\n            status_msg.data = True\n            self.safety_status_pub.publish(status_msg)\n\n            # Publish safe velocity limit\n            vel_msg = Float32()\n            vel_msg.data = self.SPEED_LIMIT\n            self.velocity_limit_pub.publish(vel_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    monitor = SafetyMonitorNode()\n    rclpy.spin(monitor)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"collision-avoidance-with-depth-camera",children:"Collision Avoidance with Depth Camera"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nReal-Time Obstacle Avoidance\nUses depth camera to detect obstacles and modify planned trajectory\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass CollisionAvoidanceNode(Node):\n    def __init__(self):\n        super().__init__('collision_avoidance')\n\n        self.bridge = CvBridge()\n\n        # Subscribe to depth camera\n        self.depth_sub = self.create_subscription(\n            Image,\n            '/camera/depth/image_rect_raw',\n            self.depth_callback,\n            10\n        )\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(\n            Twist,\n            '/cmd_vel_safe',  # Safety-checked velocity\n            10\n        )\n\n        self.obstacle_pub = self.create_publisher(\n            Image,\n            '/obstacles/debug',\n            10\n        )\n\n        # Safety parameters\n        self.DANGER_DISTANCE = 0.3  # 30cm - immediate danger\n        self.WARNING_DISTANCE = 0.5  # 50cm - warning\n        self.MAX_FORWARD_SPEED = 0.5  # m/s\n\n    def depth_callback(self, msg):\n        \"\"\"Process depth image for obstacle detection\"\"\"\n        try:\n            # Convert depth image\n            depth_image = self.bridge.imgmsg_to_cv2(msg)\n\n            # Convert to meters (depth images are often in mm)\n            depth_meters = depth_image.astype(np.float32) / 1000.0\n\n            # Find obstacles in front (center region of image)\n            h, w = depth_image.shape\n            center_region = depth_meters[h//2-50:h//2+50, w//2-100:w//2+100]\n\n            # Find minimum distance (closest obstacle)\n            min_distance = np.nanmin(center_region)\n\n            # Detect obstacles\n            danger_zone = depth_meters < self.DANGER_DISTANCE\n            warning_zone = depth_meters < self.WARNING_DISTANCE\n\n            # Create debug visualization\n            debug_image = np.zeros((h, w, 3), dtype=np.uint8)\n            debug_image[danger_zone] = [0, 0, 255]  # Red for danger\n            debug_image[warning_zone] = [0, 255, 255]  # Yellow for warning\n\n            # Publish debug image\n            debug_msg = self.bridge.cv2_to_imgmsg(debug_image, 'bgr8')\n            self.obstacle_pub.publish(debug_msg)\n\n            # Generate safe velocity command\n            safe_velocity = self._compute_safe_velocity(\n                min_distance\n            )\n\n            vel_msg = Twist()\n            vel_msg.linear.x = safe_velocity\n            self.cmd_vel_pub.publish(vel_msg)\n\n            self.get_logger().debug(\n                f'Obstacle distance: {min_distance:.2f}m, '\n                f'Safe velocity: {safe_velocity:.2f}m/s'\n            )\n\n        except Exception as e:\n            self.get_logger().error(f'Collision avoidance error: {str(e)}')\n\n    def _compute_safe_velocity(self, obstacle_distance):\n        \"\"\"\n        Compute safe forward velocity based on obstacle distance\n        Slows down as obstacles approach\n        \"\"\"\n        if obstacle_distance < self.DANGER_DISTANCE:\n            return 0.0  # Stop immediately\n\n        if obstacle_distance < self.WARNING_DISTANCE:\n            # Linear scale from 0 to max speed\n            fraction = (obstacle_distance - self.DANGER_DISTANCE) / \\\n                      (self.WARNING_DISTANCE - self.DANGER_DISTANCE)\n            return fraction * self.MAX_FORWARD_SPEED\n\n        return self.MAX_FORWARD_SPEED\n\ndef main(args=None):\n    rclpy.init(args=args)\n    avoidance = CollisionAvoidanceNode()\n    rclpy.spin(avoidance)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"part-3-ethical-ai--bias-mitigation",children:"Part 3: Ethical AI & Bias Mitigation"}),"\n",(0,i.jsx)(n.h3,{id:"identifying-ai-bias-in-robot-systems",children:"Identifying AI Bias in Robot Systems"}),"\n",(0,i.jsx)(n.p,{children:"Robots inherit biases from training data. This is critical to address:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Bias Sources in Robot Perception:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Bias Type           \u2502 Impact      \u2502 Mitigation          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Gender bias         \u2502 Fails on    \u2502 Balanced training   \u2502\n\u2502 (person detection)  \u2502 non-binary  \u2502 data, fairness test \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Racial bias         \u2502 Lower      \u2502 Cross-racial        \u2502\n\u2502 (face recognition)  \u2502 accuracy   \u2502 validation set      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Age bias            \u2502 Fails on    \u2502 Include all age     \u2502\n\u2502 (activity detection)\u2502 children    \u2502 groups in training  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Environmental bias  \u2502 Fails      \u2502 Test in varied      \u2502\n\u2502 (lighting, context) \u2502 in novel    \u2502 environments        \u2502\n\u2502                     \u2502 settings    \u2502                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h3,{id:"bias-detection--mitigation",children:"Bias Detection & Mitigation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nAI Bias Detection Framework\nIdentifies and mitigates biases in robot perception\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass BiasDetectionNode(Node):\n    def __init__(self):\n        super().__init__('bias_detection')\n\n        self.bridge = CvBridge()\n\n        # Fairness metrics\n        self.demographic_performance = {\n            'male': [],\n            'female': [],\n            'non-binary': [],\n            'caucasian': [],\n            'african': [],\n            'asian': [],\n            'hispanic': [],\n            'child': [],\n            'adult': [],\n            'elderly': []\n        }\n\n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/color/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Publisher for bias report\n        self.bias_report_pub = self.create_publisher(\n            String,\n            '/robot/bias_analysis',\n            10\n        )\n\n        self.get_logger().info('Bias Detection initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Analyze image for demographic representation\"\"\"\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n            # Run detection (simplified example)\n            detections = self._detect_people(cv_image)\n\n            # Analyze demographic distribution\n            for detection in detections:\n                demographics = self._classify_demographics(detection)\n                for demo_type, value in demographics.items():\n                    if demo_type in self.demographic_performance:\n                        self.demographic_performance[demo_type].append(value)\n\n            # Generate fairness report periodically\n            if len(self.demographic_performance['male']) % 100 == 0:\n                self._generate_fairness_report()\n\n        except Exception as e:\n            self.get_logger().error(f'Bias detection error: {str(e)}')\n\n    def _detect_people(self, image):\n        \"\"\"Placeholder for person detection\"\"\"\n        # In real system, use YOLO or similar\n        return []\n\n    def _classify_demographics(self, detection):\n        \"\"\"\n        Classify demographics of detected person\n        Returns dict of demographic attributes\n        \"\"\"\n        return {\n            'gender': 'unknown',  # Placeholder\n            'age_group': 'unknown',\n            'skin_tone': 'unknown'\n        }\n\n    def _generate_fairness_report(self):\n        \"\"\"Generate fairness analysis report\"\"\"\n        report = \"=== AI Fairness Report ===\\n\"\n\n        # Calculate performance per demographic\n        for demo_group in self.demographic_performance:\n            values = self.demographic_performance[demo_group]\n            if len(values) > 0:\n                accuracy = np.mean(values) if all(isinstance(v, (int, float))\n                                                  for v in values) else 0.0\n                count = len(values)\n\n                report += f\"{demo_group}: {accuracy:.1%} accuracy ({count} samples)\\n\"\n\n        # Flag disparities (>5% difference)\n        accuracies = {}\n        for demo_group in self.demographic_performance:\n            values = self.demographic_performance[demo_group]\n            if len(values) > 10:  # Require minimum samples\n                accuracies[demo_group] = np.mean(values)\n\n        if len(accuracies) > 1:\n            max_acc = max(accuracies.values())\n            min_acc = min(accuracies.values())\n            disparity = max_acc - min_acc\n\n            if disparity > 0.05:\n                report += f\"\\n\u26a0\ufe0f  WARNING: {disparity:.1%} accuracy disparity detected\\n\"\n                report += f\"Best: {max(accuracies, key=accuracies.get)}\\n\"\n                report += f\"Worst: {min(accuracies, key=accuracies.get)}\\n\"\n\n        self.get_logger().info(report)\n\n        # Publish report\n        report_msg = String()\n        report_msg.data = report\n        self.bias_report_pub.publish(report_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    bias_node = BiasDetectionNode()\n    rclpy.spin(bias_node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"part-4-transparent-ai-decision-making",children:"Part 4: Transparent AI Decision-Making"}),"\n",(0,i.jsx)(n.h3,{id:"explainability-for-robot-actions",children:"Explainability for Robot Actions"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nExplainable AI (XAI) for Robots\nMakes robot decisions transparent and interpretable\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\n\nclass ExplainableAINode(Node):\n    def __init__(self):\n        super().__init__(\'explainable_ai\')\n\n        # Subscribe to robot decisions\n        self.decision_sub = self.create_subscription(\n            String,\n            \'/robot/decision\',\n            self.explain_decision,\n            10\n        )\n\n        # Publisher for explanations\n        self.explanation_pub = self.create_publisher(\n            String,\n            \'/robot/explanation\',\n            10\n        )\n\n        self.get_logger().info(\'Explainable AI initialized\')\n\n    def explain_decision(self, msg):\n        """\n        Generate human-readable explanation for robot decision\n        """\n        try:\n            decision = json.loads(msg.data)\n\n            explanation = self._generate_explanation(decision)\n\n            # Publish explanation\n            exp_msg = String()\n            exp_msg.data = explanation\n            self.explanation_pub.publish(exp_msg)\n\n            self.get_logger().info(f\'Explanation: {explanation}\')\n\n        except Exception as e:\n            self.get_logger().error(f\'XAI error: {str(e)}\')\n\n    def _generate_explanation(self, decision):\n        """\n        Generate human-interpretable explanation for decision\n        """\n        action = decision.get(\'action\', \'unknown\')\n        confidence = decision.get(\'confidence\', 0.0)\n        reason = decision.get(\'reason\', \'\')\n        alternatives = decision.get(\'alternatives\', [])\n\n        explanation = f"""\nDecision: {action}\nConfidence: {confidence:.1%}\nReasoning: {reason}\n\nWhy not alternatives?\n"""\n\n        for alt in alternatives:\n            alt_name = alt.get(\'action\', \'unknown\')\n            alt_reason = alt.get(\'reason\', \'\')\n            explanation += f"- {alt_name}: {alt_reason}\\n"\n\n        explanation += f"""\nHow to override:\n1. Say "Stop" for emergency stop\n2. Say "Alternative: <action>" to request different action\n3. Hold red button on gripper for manual control\n        """\n\n        return explanation\n\ndef main(args=None):\n    rclpy.init(args=args)\n    xai_node = ExplainableAINode()\n    rclpy.spin(xai_node)\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"part-5-real-world-robot-failure-analysis",children:"Part 5: Real-World Robot Failure Analysis"}),"\n",(0,i.jsx)(n.h3,{id:"critical-incidents--lessons",children:"Critical Incidents & Lessons"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"CASE STUDY 1: Collaborative Robot Injury (2021)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Incident: Worker's hand crushed by robot gripper\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Root Cause: Force sensor miscalibrated          \u2502\n\u2502             Safety thresholds not validated     \u2502\n\u2502                                                 \u2502\n\u2502 Lessons:                                         \u2502\n\u2502 \u2717 Sensor calibration missed                    \u2502\n\u2502 \u2717 Force limits not tested                      \u2502\n\u2502 \u2717 No emergency stop accessible                 \u2502\n\u2502 \u2717 Training incomplete for new workers          \u2502\n\u2502                                                 \u2502\n\u2502 Changes Made:                                   \u2502\n\u2502 \u2713 Daily sensor calibration check               \u2502\n\u2502 \u2713 Force validation in test environment         \u2502\n\u2502 \u2713 Emergency buttons on robot + pendant        \u2502\n\u2502 \u2713 Mandatory safety training quarterly         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nCASE STUDY 2: Autonomous Vehicle Failure (2018)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Incident: Pedestrian not detected at night      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Root Cause: Neural network trained on daytime  \u2502\n\u2502             images only. Dark-skin pedestrian  \u2502\n\u2502             not in training data.              \u2502\n\u2502                                                 \u2502\n\u2502 Lessons:                                         \u2502\n\u2502 \u2717 Unbalanced training dataset                  \u2502\n\u2502 \u2717 No testing in varied lighting                \u2502\n\u2502 \u2717 Demographic testing not performed           \u2502\n\u2502 \u2717 Confidence scores not calibrated            \u2502\n\u2502                                                 \u2502\n\u2502 Changes Made:                                   \u2502\n\u2502 \u2713 Night-time and mixed-lighting training      \u2502\n\u2502 \u2713 Diverse demographic representation          \u2502\n\u2502 \u2713 Cross-condition validation testing          \u2502\n\u2502 \u2713 Uncertainty quantification added            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"part-6-production-safety-checklist",children:"Part 6: Production Safety Checklist"}),"\n",(0,i.jsx)(n.h3,{id:"pre-deployment-safety-audit",children:"Pre-Deployment Safety Audit"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nProduction Safety Checklist\nComprehensive audit before robot deployment\n\"\"\"\n\nSAFETY_CHECKLIST = {\n    'Hardware': {\n        'emergency_stop': {\n            'description': 'Emergency stop button functional',\n            'test': 'Press button, robot stops within 100ms',\n            'status': False\n        },\n        'force_sensors': {\n            'description': 'Force/torque sensors calibrated',\n            'test': 'Zero reading without load, verify linearity',\n            'status': False\n        },\n        'proximity_sensors': {\n            'description': 'Proximity sensors detect obstacles',\n            'test': 'Place object at 1m, 0.5m, 0.1m distances',\n            'status': False\n        },\n        'motor_encoders': {\n            'description': 'Joint encoders report accurate position',\n            'test': 'Move to known positions, verify readings',\n            'status': False\n        },\n    },\n\n    'Software': {\n        'safety_limits': {\n            'description': 'Velocity and force limits enforced',\n            'test': 'Command exceeding limits, verify rejection',\n            'status': False\n        },\n        'collision_detection': {\n            'description': 'Collision detection working',\n            'test': 'Apply 100N force, robot stops within 50ms',\n            'status': False\n        },\n        'emergency_response': {\n            'description': 'Emergency stop cuts motor power',\n            'test': 'Trigger emergency, motors de-energize',\n            'status': False\n        },\n        'sensor_validation': {\n            'description': 'All sensor inputs are validated',\n            'test': 'Check for sensor failure handling',\n            'status': False\n        },\n    },\n\n    'Perception': {\n        'object_detection': {\n            'description': 'Object detection >95% accuracy on test set',\n            'test': 'Run inference on 100 test images',\n            'status': False\n        },\n        'demographic_fairness': {\n            'description': 'Detection accuracy balanced across demographics',\n            'test': 'Accuracy difference &lt;5% across groups',\n            'status': False\n        },\n        'adversarial_robustness': {\n            'description': 'Perception robust to adversarial inputs',\n            'test': 'Add noise/rotation, verify degradation &lt;10%',\n            'status': False\n        },\n    },\n\n    'Planning': {\n        'trajectory_safety': {\n            'description': 'Planned trajectories avoid collisions',\n            'test': 'Generate 100 plans, verify none hit obstacles',\n            'status': False\n        },\n        'action_validation': {\n            'description': 'All planned actions are within robot limits',\n            'test': 'Check gripper force, reach, speed constraints',\n            'status': False\n        },\n    },\n\n    'HRI': {\n        'human_detection': {\n            'description': 'Robot detects human presence',\n            'test': 'Human in workspace, robot slows down',\n            'status': False\n        },\n        'speed_limits_human_present': {\n            'description': 'Speed &lt;0.3 m/s when humans nearby',\n            'test': 'Measure actual speed in human workspace',\n            'status': False\n        },\n        'verbal_feedback': {\n            'description': 'Robot announces actions to humans',\n            'test': 'Robot speaks: \"Moving to table\"',\n            'status': False\n        },\n    },\n\n    'Testing': {\n        'functional_testing': {\n            'description': '100 test cases passed',\n            'test': 'Run full regression test suite',\n            'status': False\n        },\n        'stress_testing': {\n            'description': 'Robot stable under 8-hour load',\n            'test': 'Continuous operation, monitor for failures',\n            'status': False\n        },\n        'failure_modes': {\n            'description': 'All known failure modes documented',\n            'test': 'Compare against ISO/TS 15066 requirements',\n            'status': False\n        },\n    },\n\n    'Documentation': {\n        'safety_manual': {\n            'description': 'Safety manual complete and reviewed',\n            'test': 'Safety officer approves all procedures',\n            'status': False\n        },\n        'maintenance_schedule': {\n            'description': 'Maintenance procedures documented',\n            'test': 'Monthly safety checks scheduled',\n            'status': False\n        },\n        'incident_response': {\n            'description': 'Incident response procedures documented',\n            'test': 'Safety team briefed on procedures',\n            'status': False\n        },\n    }\n}\n\ndef print_checklist():\n    \"\"\"Print safety checklist with current status\"\"\"\n    passed = 0\n    total = 0\n\n    for category, items in SAFETY_CHECKLIST.items():\n        print(f\"\\n{'='*60}\")\n        print(f\"Category: {category}\")\n        print(f\"{'='*60}\")\n\n        for item_name, item_data in items.items():\n            status = '\u2713' if item_data['status'] else '\u2717'\n            total += 1\n            if item_data['status']:\n                passed += 1\n\n            print(f\"\\n{status} {item_data['description']}\")\n            print(f\"  Test: {item_data['test']}\")\n\n    print(f\"\\n{'='*60}\")\n    print(f\"OVERALL: {passed}/{total} checks passed ({100*passed/total:.1f}%)\")\n    print(f\"{'='*60}\")\n\n    if passed == total:\n        print(\"\\n\u2713 ROBOT APPROVED FOR DEPLOYMENT\")\n    else:\n        print(f\"\\n\u2717 {total - passed} checks remaining before deployment\")\n\nif __name__ == '__main__':\n    print_checklist()\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"hands-on-exercise-safety-critical-system-design",children:"Hands-On Exercise: Safety-Critical System Design"}),"\n",(0,i.jsx)(n.h3,{id:"exercise-1-test-safety-limits",children:"Exercise 1: Test Safety Limits"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Terminal 1: Safety monitor\nros2 run robot_safety safety_monitor.py\n\n# Terminal 2: Test exceeding speed limit\nros2 topic pub /joint_states sensor_msgs/JointState \\\n  "header: {frame_id: base_link}\n   name: [joint1, joint2, joint3]\n   velocity: [1.0, 0.5, 0.8]"  # Exceeds 0.5 m/s limit\n\n# Expected: Safety monitor detects violation, publishes emergency stop\n# Verify: /safety/emergency_stop = True\n'})}),"\n",(0,i.jsx)(n.h3,{id:"exercise-2-collision-detection-test",children:"Exercise 2: Collision Detection Test"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Terminal 1: Collision avoidance\nros2 run robot_safety collision_avoidance.py\n\n# Terminal 2: Simulate obstacle approaching\n# Publish depth image with obstacle at 0.2m (danger zone)\nros2 topic pub /camera/depth/image_rect_raw \\\n  sensor_msgs/Image "header: {frame_id: camera}"\n\n# Expected: Robot velocity reduced to 0\n# Verify: /cmd_vel_safe.linear.x = 0\n'})}),"\n",(0,i.jsx)(n.h3,{id:"exercise-3-bias-detection-validation",children:"Exercise 3: Bias Detection Validation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create balanced test dataset\npython3 test_bias_detection.py \\\n  --test_images=1000 \\\n  --demographics_balanced=true\n\n# Expected output:\n# Male accuracy: 96.2%\n# Female accuracy: 95.8%\n# Non-binary accuracy: 94.1%\n# Disparity: 2.1% (acceptable &lt;5%)\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"common-hri--safety-errors",children:"Common HRI & Safety Errors"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Error"}),(0,i.jsx)(n.th,{children:"Cause"}),(0,i.jsx)(n.th,{children:"Fix"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:'"Force sensor shows 0N constantly"'})}),(0,i.jsx)(n.td,{children:"Sensor miscalibrated"}),(0,i.jsx)(n.td,{children:"Run zero calibration with no load"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:'"Robot doesn\'t stop on obstacle"'})}),(0,i.jsx)(n.td,{children:"Depth camera not connected"}),(0,i.jsx)(n.td,{children:"Check USB, verify /camera/depth/image_rect_raw topic"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:'"Safety limits ignored"'})}),(0,i.jsx)(n.td,{children:"No safety layer in motion control"}),(0,i.jsx)(n.td,{children:"Add SafetyMonitorNode before motor commands"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:'"Low accuracy on dark skin"'})}),(0,i.jsx)(n.td,{children:"Training data imbalance"}),(0,i.jsx)(n.td,{children:"Add dark-skinned people to training set, retrain"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:'"Bias detection not catching bias"'})}),(0,i.jsx)(n.td,{children:"Insufficient test samples"}),(0,i.jsx)(n.td,{children:"Collect 100+ samples per demographic group"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:'"Explanation too technical"'})}),(0,i.jsx)(n.td,{children:"Not human-readable"}),(0,i.jsx)(n.td,{children:"Simplify language, remove jargon"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Safety First, Features Second"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Safety is not a feature, it's a fundamental requirement"}),"\n",(0,i.jsx)(n.li,{children:"Redundant sensors + fail-safe mechanisms are essential"}),"\n",(0,i.jsx)(n.li,{children:"Regular audits prevent incidents before they happen"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Ethical AI Matters"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Biases in training data become biases in robot behavior"}),"\n",(0,i.jsx)(n.li,{children:"Demographic testing should be mandatory"}),"\n",(0,i.jsx)(n.li,{children:"Explainability builds user trust"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Real-Time Monitoring is Critical"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Collision detection must run at 20+ Hz"}),"\n",(0,i.jsx)(n.li,{children:"Force limits prevent serious injuries"}),"\n",(0,i.jsx)(n.li,{children:"Emergency stops must be fail-safe (de-energize, not stop)"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Standards Exist for a Reason"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"ISO/TS 15066 specifies safe force levels"}),"\n",(0,i.jsx)(n.li,{children:"ISO 13849 defines safety architecture"}),"\n",(0,i.jsx)(n.li,{children:"Follow standards, don't create your own"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Humans + Robots = New Safety Domain"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Collaborative robots need different controls than industrial robots"}),"\n",(0,i.jsx)(n.li,{children:"Speed must reduce when humans are nearby"}),"\n",(0,i.jsx)(n.li,{children:"Transparency in decision-making builds acceptance"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsx)(n.h3,{id:"safety-standards--guidelines",children:"Safety Standards & Guidelines"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://www.iso.org/standard/62996.html",children:"ISO/TS 15066: Collaborative Robots Safety"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://www.ansi.org/",children:"ANSI/RIA R15.06: Industrial Robot Safety"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://www.iso.org/standard/54646.html",children:"ISO 13849-1: Safety Control Systems"})}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"ethics-in-robotics",children:"Ethics in Robotics"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://standards.ieee.org/standard/7001-2018.html",children:"IEEE Standards for Ethical AI"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/1811.02159",children:"Algorithmic Bias in Computer Vision"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://fairmlbook.org/",children:"Fairness in Machine Learning"})}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"real-world-incidents",children:"Real-World Incidents"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://www.nhtsa.gov/",children:"NHTSA Tesla Autopilot Investigation"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/1801.08289",children:"Amazon Rekognition Bias Study"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://www.nist.gov/robotics/",children:"Robot Safety Case Studies"})}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"capstone-project-end-to-end-safe-robot-system",children:"Capstone Project: End-to-End Safe Robot System"}),"\n",(0,i.jsx)(n.h3,{id:"complete-system-architecture",children:"Complete System Architecture"}),"\n",(0,i.jsx)(n.p,{children:"You now have all components to build a complete, safe conversational robot:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    HUMANOID ROBOT SYSTEM                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                              \u2502\n\u2502  CHAPTER 1: Foundation (ROS 2)                              \u2502\n\u2502  \u2514\u2500 Robot description, packages, communication              \u2502\n\u2502                                                              \u2502\n\u2502  CHAPTER 2: Perception (Gazebo + Sensors)                  \u2502\n\u2502  \u2514\u2500 Physics simulation, camera/LiDAR/IMU integration       \u2502\n\u2502                                                              \u2502\n\u2502  CHAPTER 3: Navigation (Isaac Sim + SLAM + Vision)        \u2502\n\u2502  \u2514\u2500 Photorealistic simulation, SLAM mapping, object detect  \u2502\n\u2502                                                              \u2502\n\u2502  CHAPTER 4: Intelligence (LLM + VLA + Safety)              \u2502\n\u2502  \u251c\u2500 Natural language understanding (Lesson 4.1)            \u2502\n\u2502  \u251c\u2500 Multimodal perception + conversation (Lesson 4.2)      \u2502\n\u2502  \u2514\u2500 Safety + Ethics + HRI (Lesson 4.3) \u2190 YOU ARE HERE     \u2502\n\u2502                                                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 FINAL SYSTEM: Safe, Intelligent, Conversational Robot       \u2502\n\u2502                                                              \u2502\n\u2502 Input: Natural language commands from humans                \u2502\n\u2502 Process: Understand intent \u2192 Perceive environment          \u2502\n\u2502         \u2192 Check safety \u2192 Plan action \u2192 Execute             \u2502\n\u2502 Output: Robot performs task safely while explaining itself  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h3,{id:"capstone-implementation",children:"Capstone Implementation"}),"\n",(0,i.jsx)(n.p,{children:"Create a complete safe robot system integrating all 4 chapters:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nCAPSTONE: Complete Safe Humanoid Robot System\nIntegration of all 13 lessons from the course\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport json\n\nclass CapstoneRobotSystem(Node):\n    def __init__(self):\n        super().__init__('capstone_robot')\n\n        # Subscriptions from all components\n        self.speech_sub = self.create_subscription(\n            String, '/speech/transcription', self.on_speech, 10)\n        self.vision_sub = self.create_subscription(\n            String, '/robot/answer', self.on_vision, 10)\n        self.safety_sub = self.create_subscription(\n            String, '/safety/status_ok', self.on_safety, 10)\n\n        # Publishers to all components\n        self.query_pub = self.create_publisher(String, '/robot/query', 10)\n        self.plan_pub = self.create_publisher(String, '/robot/plan', 10)\n        self.response_pub = self.create_publisher(String, '/robot/response', 10)\n        self.vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n\n        self.get_logger().info(\n            'Capstone Robot System initialized.\\n'\n            'This robot successfully integrates:\\n'\n            '\u2713 ROS 2 Foundation (Ch 1)\\n'\n            '\u2713 Physics Simulation (Ch 2)\\n'\n            '\u2713 Navigation & Perception (Ch 3)\\n'\n            '\u2713 Language & Safety (Ch 4)\\n'\n            'Ready for deployment!'\n        )\n\n    def on_speech(self, msg):\n        \"\"\"User speaks to robot\"\"\"\n        self.get_logger().info(f'Human: {msg.data}')\n        # Rest of implementation uses Lessons 4.1-4.3\n\nif __name__ == '__main__':\n    rclpy.init()\n    system = CapstoneRobotSystem()\n    rclpy.spin(system)\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"course-summary",children:"Course Summary"}),"\n",(0,i.jsxs)(n.p,{children:["You've completed the ",(0,i.jsx)(n.strong,{children:"13-week Physical AI & Humanoid Robotics course"}),". You now understand:"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Week 1-4 (Ch 1)"}),": ROS 2 ecosystem and robot communication\n",(0,i.jsx)(n.strong,{children:"Week 5-8 (Ch 2)"}),": Physics simulation and digital twins\n",(0,i.jsx)(n.strong,{children:"Week 9-11 (Ch 3)"}),": Perception, navigation, and computer vision\n",(0,i.jsx)(n.strong,{children:"Week 12-13 (Ch 4)"}),": Large language models, multimodal AI, and safety"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"You can now:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Design and build complete robot systems"}),"\n",(0,i.jsx)(n.li,{children:"Integrate perception, planning, and control"}),"\n",(0,i.jsx)(n.li,{children:"Deploy AI safely in human environments"}),"\n",(0,i.jsx)(n.li,{children:"Debug and optimize robot systems"}),"\n",(0,i.jsx)(n.li,{children:"Understand the ethical implications of robot AI"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Next Steps:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Deploy on real hardware (Jetson Orin Nano humanoid)"}),"\n",(0,i.jsx)(n.li,{children:"Collect real-world data to improve perception"}),"\n",(0,i.jsx)(n.li,{children:"Fine-tune language models on robot-specific tasks"}),"\n",(0,i.jsx)(n.li,{children:"Contribute to open-source robotics projects"}),"\n",(0,i.jsx)(n.li,{children:"Consider graduate studies in robotics AI"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"The most important lesson"}),": ",(0,i.jsx)(n.em,{children:"Always prioritize human safety over robot capabilities."})]}),"\n",(0,i.jsx)(n.p,{children:"Congratulations on completing the course! \ud83c\udf93"})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);