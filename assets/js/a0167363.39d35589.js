"use strict";(self.webpackChunkphysical_ai_textbook=self.webpackChunkphysical_ai_textbook||[]).push([[706],{3509:function(e,n,r){r.r(n),r.d(n,{assets:function(){return l},contentTitle:function(){return o},default:function(){return p},frontMatter:function(){return a},metadata:function(){return i},toc:function(){return c}});var i=JSON.parse('{"id":"chapter-4/4-2-conversational-ai","title":"Conversational AI & Vision-Language-Action: Multimodal Robot Intelligence","description":"Integrate multimodal AI models to create robots that see, listen, understand, and act on natural language commands","source":"@site/docs/chapter-4/4-2-conversational-ai.md","sourceDirName":"chapter-4","slug":"/chapter-4/4-2-conversational-ai","permalink":"/physical-ai-textbook/docs/chapter-4/4-2-conversational-ai","draft":false,"unlisted":false,"editUrl":"https://github.com/Bil4l-Mehmood/physical-ai-textbook/edit/main/docs/chapter-4/4-2-conversational-ai.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"Lesson 4.2: Conversational AI & VLA","title":"Conversational AI & Vision-Language-Action: Multimodal Robot Intelligence","description":"Integrate multimodal AI models to create robots that see, listen, understand, and act on natural language commands","duration":120,"difficulty":"Advanced","hardware":["Jetson Orin Nano 8GB","RealSpeaker USB Mic Array","RealSense D435i","ROS 2 Humble"],"prerequisites":["Lesson 4.1: LLM Integration for Robotics"]},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.1: LLM Integration","permalink":"/physical-ai-textbook/docs/chapter-4/4-1-llm-brain"},"next":{"title":"Lesson 4.3: HRI & Safety","permalink":"/physical-ai-textbook/docs/chapter-4/4-3-hri-safety"}}'),t=r(4848),s=r(8453);const a={sidebar_position:2,sidebar_label:"Lesson 4.2: Conversational AI & VLA",title:"Conversational AI & Vision-Language-Action: Multimodal Robot Intelligence",description:"Integrate multimodal AI models to create robots that see, listen, understand, and act on natural language commands",duration:120,difficulty:"Advanced",hardware:["Jetson Orin Nano 8GB","RealSpeaker USB Mic Array","RealSense D435i","ROS 2 Humble"],prerequisites:["Lesson 4.1: LLM Integration for Robotics"]},o="Lesson 4.2: Conversational AI & Vision-Language-Action - Multimodal Robot Intelligence",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2},{value:"RealSpeaker USB Microphone Array",id:"realspeaker-usb-microphone-array",level:3},{value:"Complete Hardware Stack for VLA",id:"complete-hardware-stack-for-vla",level:3},{value:"Part 1: Vision-Language Models (VLMs) Fundamentals",id:"part-1-vision-language-models-vlms-fundamentals",level:2},{value:"What VLMs Can Do",id:"what-vlms-can-do",level:3},{value:"VLM Architecture Comparison",id:"vlm-architecture-comparison",level:3},{value:"Key VLM Capabilities for Robotics",id:"key-vlm-capabilities-for-robotics",level:3},{value:"Part 2: RealSpeaker Integration for Robot Hearing",id:"part-2-realspeaker-integration-for-robot-hearing",level:2},{value:"Installing RealSpeaker Drivers",id:"installing-realspeaker-drivers",level:3},{value:"RealSpeaker ROS 2 Node",id:"realspeaker-ros-2-node",level:3},{value:"Part 3: Speech-to-Text Pipeline",id:"part-3-speech-to-text-pipeline",level:2},{value:"Cloud-Based STT (Google Cloud Speech-to-Text)",id:"cloud-based-stt-google-cloud-speech-to-text",level:3},{value:"Part 4: Vision-Language Model Integration",id:"part-4-vision-language-model-integration",level:2},{value:"LLaVA (Open-Source VLM) on Jetson",id:"llava-open-source-vlm-on-jetson",level:3},{value:"Part 5: Text-to-Speech Output",id:"part-5-text-to-speech-output",level:2},{value:"TTS Implementation",id:"tts-implementation",level:3},{value:"Part 6: Complete VLA Agent Pipeline",id:"part-6-complete-vla-agent-pipeline",level:2},{value:"End-to-End Conversational Robot",id:"end-to-end-conversational-robot",level:3},{value:"Hands-On Exercise: Build a Conversational Robot",id:"hands-on-exercise-build-a-conversational-robot",level:2},{value:"Exercise 1: Voice-Controlled Navigation",id:"exercise-1-voice-controlled-navigation",level:3},{value:"Exercise 2: Visual Question Answering",id:"exercise-2-visual-question-answering",level:3},{value:"Exercise 3: Multi-Step Conversational Task",id:"exercise-3-multi-step-conversational-task",level:3},{value:"Common VLA Errors &amp; Debugging",id:"common-vla-errors--debugging",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Vision-Language Model Papers",id:"vision-language-model-papers",level:3},{value:"Robot Conversation &amp; HRI",id:"robot-conversation--hri",level:3},{value:"Implementation Resources",id:"implementation-resources",level:3},{value:"Next Lesson",id:"next-lesson",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"lesson-42-conversational-ai--vision-language-action---multimodal-robot-intelligence",children:"Lesson 4.2: Conversational AI & Vision-Language-Action - Multimodal Robot Intelligence"})}),"\n",(0,t.jsxs)(n.admonition,{title:"Lesson Overview",type:"info",children:[(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Duration"}),": 120 minutes | ",(0,t.jsx)(n.strong,{children:"Difficulty"}),": Advanced | ",(0,t.jsx)(n.strong,{children:"Hardware"}),": Jetson + microphone + camera"]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Prerequisites"}),": Lesson 4.1 (LLM integration), Chapter 3 (perception)"]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Learning Outcome"}),": Build robots that perceive their environment visually, understand spoken language, reason about multimodal inputs, and execute appropriate actions through a unified VLA (Vision-Language-Action) pipeline"]})]}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand vision-language models (VLMs) and multimodal learning"}),"\n",(0,t.jsx)(n.li,{children:"Compare VLM architectures (LLaVA, GPT-4V, Gemini Vision, Claude Vision)"}),"\n",(0,t.jsx)(n.li,{children:"Process images with language models for scene understanding"}),"\n",(0,t.jsx)(n.li,{children:"Integrate RealSpeaker USB microphone array for speech capture"}),"\n",(0,t.jsx)(n.li,{children:"Implement speech-to-text and text-to-speech pipelines"}),"\n",(0,t.jsx)(n.li,{children:"Build VLA agents combining perception, reasoning, and action"}),"\n",(0,t.jsx)(n.li,{children:"Create end-to-end conversational robot systems"}),"\n",(0,t.jsx)(n.li,{children:"Handle multimodal input errors and edge cases"}),"\n",(0,t.jsx)(n.li,{children:"Optimize latency for real-time interaction"}),"\n",(0,t.jsx)(n.li,{children:"Deploy production conversational robots on edge devices"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,t.jsxs)(n.admonition,{title:"Multimodal Perception Stack",type:"note",children:[(0,t.jsx)(n.p,{children:"True robot intelligence requires simultaneous processing of:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision"}),": Real-time camera feeds (RGB-D)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio"}),": Speech capture with directional beamforming"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language"}),": Natural language understanding with reasoning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action"}),": Coordinated robot movement and manipulation"]}),"\n"]})]}),"\n",(0,t.jsx)(n.h3,{id:"realspeaker-usb-microphone-array",children:"RealSpeaker USB Microphone Array"}),"\n",(0,t.jsx)(n.p,{children:"The RealSpeaker Mic Array v2.0 is the standard for robot voice input:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Component"}),(0,t.jsx)(n.th,{children:"Specification"}),(0,t.jsx)(n.th,{children:"Impact"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Mic Count"})}),(0,t.jsx)(n.td,{children:"6 \xd7 MEMS microphones"}),(0,t.jsx)(n.td,{children:"Beamforming for speech direction"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Frequency"})}),(0,t.jsx)(n.td,{children:"20Hz - 20kHz"}),(0,t.jsx)(n.td,{children:"Full human speech spectrum"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"SNR"})}),(0,t.jsx)(n.td,{children:">60dB"}),(0,t.jsx)(n.td,{children:"Noise suppression capability"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Beam Angle"})}),(0,t.jsx)(n.td,{children:"\xb145\xb0"}),(0,t.jsx)(n.td,{children:"Pick up speech from front arc"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"USB"})}),(0,t.jsx)(n.td,{children:"2.0 (480 Mbps)"}),(0,t.jsx)(n.td,{children:"Works with all Jetson models"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Power"})}),(0,t.jsx)(n.td,{children:"100mA @ 5V"}),(0,t.jsx)(n.td,{children:"Powered via USB, no separate supply"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Latency"})}),(0,t.jsx)(n.td,{children:"<50ms"}),(0,t.jsx)(n.td,{children:"Minimal delay for real-time interaction"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"complete-hardware-stack-for-vla",children:"Complete Hardware Stack for VLA"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Robot Hardware Stack:\r\n\u251c\u2500 Camera (RealSense D435i)\r\n\u2502  \u251c\u2500 RGB input: 640\xd7480 @ 30Hz\r\n\u2502  \u251c\u2500 Depth input: 640\xd7480 @ 30Hz\r\n\u2502  \u2514\u2500 Output: /camera/color/image_raw, /camera/depth/image_rect_raw\r\n\u251c\u2500 Microphone (RealSpeaker Mic Array v2)\r\n\u2502  \u251c\u2500 Audio capture: 48kHz, 16-bit\r\n\u2502  \u2514\u2500 Output: /audio/raw (raw PCM audio)\r\n\u251c\u2500 Processor (Jetson Orin Nano)\r\n\u2502  \u251c\u2500 Vision processing: 8 CUDA cores\r\n\u2502  \u251c\u2500 Language processing: Local or cloud LLM\r\n\u2502  \u2514\u2500 Speech processing: Local or cloud TTS/STT\r\n\u2514\u2500 Actuators (Robot-specific)\r\n   \u251c\u2500 Motors: /motor/[left|right]_wheel, /motor/gripper\r\n   \u2514\u2500 Feedback: /encoder/[left|right]_wheel, /gripper/force\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-1-vision-language-models-vlms-fundamentals",children:"Part 1: Vision-Language Models (VLMs) Fundamentals"}),"\n",(0,t.jsx)(n.h3,{id:"what-vlms-can-do",children:"What VLMs Can Do"}),"\n",(0,t.jsx)(n.p,{children:"VLMs are neural networks trained on billions of image-text pairs, understanding the relationship between visual content and language:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Traditional AI:\r\nImage \u2192 Detector \u2192 "person"\r\n        \u2192 Classifier \u2192 "sitting"\r\n        \u2192 Analyzer \u2192 "on chair"\r\nUser asks: "What is the person doing?"\r\nRobot: "Can\'t directly answer from detection outputs"\r\n\r\nVision-Language Model:\r\nImage + Question: "What is the person doing?"\r\n          \u2193\r\n        [VLM]\r\n          \u2193\r\nOutput: "The person is sitting on a chair and reading a book"\r\n(Generated from visual understanding + language)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"vlm-architecture-comparison",children:"VLM Architecture Comparison"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Model"}),(0,t.jsx)(n.th,{children:"Provider"}),(0,t.jsx)(n.th,{children:"Size"}),(0,t.jsx)(n.th,{children:"Latency"}),(0,t.jsx)(n.th,{children:"Accuracy"}),(0,t.jsx)(n.th,{children:"Cost"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"LLaVA 1.5"})}),(0,t.jsx)(n.td,{children:"Open source"}),(0,t.jsx)(n.td,{children:"7B"}),(0,t.jsx)(n.td,{children:"500-1000ms"}),(0,t.jsx)(n.td,{children:"85%"}),(0,t.jsx)(n.td,{children:"Free (local)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"GPT-4V"})}),(0,t.jsx)(n.td,{children:"OpenAI"}),(0,t.jsx)(n.td,{children:"Proprietary"}),(0,t.jsx)(n.td,{children:"800-2000ms"}),(0,t.jsx)(n.td,{children:"95%"}),(0,t.jsx)(n.td,{children:"$0.01/image"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Gemini Pro Vision"})}),(0,t.jsx)(n.td,{children:"Google"}),(0,t.jsx)(n.td,{children:"Proprietary"}),(0,t.jsx)(n.td,{children:"600-1500ms"}),(0,t.jsx)(n.td,{children:"92%"}),(0,t.jsx)(n.td,{children:"Free tier limited"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Claude 3 Vision"})}),(0,t.jsx)(n.td,{children:"Anthropic"}),(0,t.jsx)(n.td,{children:"Proprietary"}),(0,t.jsx)(n.td,{children:"700-1500ms"}),(0,t.jsx)(n.td,{children:"94%"}),(0,t.jsx)(n.td,{children:"$0.003/image"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"key-vlm-capabilities-for-robotics",children:"Key VLM Capabilities for Robotics"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Visual Question Answering (VQA)"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Image: [robot camera feed]\r\nQuestion: "What objects can I grasp?"\r\nVLM Output: "I see a red cube, blue cylinder, and yellow ball.\r\n            The red cube appears graspable. The blue cylinder is\r\n            too large. The yellow ball is on the shelf (unreachable)."\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Scene Understanding"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Image: [robot view of room]\r\nQuestion: "Is the path to the door clear?"\r\nVLM Output: "The path has a chair at coordinates (2.5m, 1.2m)\r\n            blocking direct access. Recommend going around the left side."\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Spatial Reasoning"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Image: [table with objects]\r\nQuestion: "What\'s to the left of the blue cube?"\r\nVLM Output: "A red sphere and a green pen are positioned to the\r\n            left of the blue cube."\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-2-realspeaker-integration-for-robot-hearing",children:"Part 2: RealSpeaker Integration for Robot Hearing"}),"\n",(0,t.jsx)(n.h3,{id:"installing-realspeaker-drivers",children:"Installing RealSpeaker Drivers"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Clone RealSpeaker repository\r\ngit clone https://github.com/respeaker/respeaker_ros.git\r\ncd respeaker_ros\r\n\r\n# Install dependencies\r\nsudo apt-get install -y libsndfile1-dev alsa-utils pulseaudio\r\n\r\n# Build ROS 2 package\r\nsource /opt/ros/humble/setup.bash\r\ncolcon build --symlink-install\r\n\r\n# Test microphone\r\narecord -D plughw:CARD=seeed2micvoicec,DEV=0 -r 16000 -f S16_LE test.wav\r\n# If successful, you'll hear audio recorded\n"})}),"\n",(0,t.jsx)(n.h3,{id:"realspeaker-ros-2-node",children:"RealSpeaker ROS 2 Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nRealSpeaker Microphone Integration for ROS 2\r\nCaptures multi-channel audio and publishes as ROS messages\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom audio_common_msgs.msg import AudioData\r\nimport pyaudio\r\nimport numpy as np\r\nfrom collections import deque\r\n\r\nclass RealSpeakerNode(Node):\r\n    def __init__(self):\r\n        super().__init__('respeaker_node')\r\n\r\n        # Audio parameters\r\n        self.CHUNK = 1024  # Samples per frame\r\n        self.FORMAT = pyaudio.paInt16\r\n        self.CHANNELS = 6  # RealSpeaker has 6 microphones\r\n        self.RATE = 16000  # 16 kHz sampling rate\r\n\r\n        # Initialize PyAudio\r\n        self.audio = pyaudio.PyAudio()\r\n\r\n        # Open audio stream from RealSpeaker\r\n        try:\r\n            self.stream = self.audio.open(\r\n                format=self.FORMAT,\r\n                channels=self.CHANNELS,\r\n                rate=self.RATE,\r\n                input=True,\r\n                input_device_index=self._find_respeaker_device(),\r\n                frames_per_buffer=self.CHUNK,\r\n                exceptions=False\r\n            )\r\n        except Exception as e:\r\n            self.get_logger().error(f'Failed to open RealSpeaker: {str(e)}')\r\n            return\r\n\r\n        # Publishers\r\n        self.raw_audio_pub = self.create_publisher(\r\n            AudioData,\r\n            '/respeaker/raw_audio',\r\n            10\r\n        )\r\n\r\n        self.direction_pub = self.create_publisher(\r\n            String,\r\n            '/respeaker/sound_direction',\r\n            10\r\n        )\r\n\r\n        # Sound direction buffer (for beamforming analysis)\r\n        self.direction_history = deque(maxlen=10)\r\n\r\n        # Timer for audio capture\r\n        self.create_timer(\r\n            self.CHUNK / self.RATE,  # Timer period = one frame\r\n            self.capture_audio_callback\r\n        )\r\n\r\n        self.get_logger().info('RealSpeaker initialized (6-channel capture)')\r\n\r\n    def _find_respeaker_device(self):\r\n        \"\"\"Find RealSpeaker device index\"\"\"\r\n        for i in range(self.audio.get_device_count()):\r\n            info = self.audio.get_device_info_by_index(i)\r\n            if 'seeed' in info['name'].lower() or 'respeaker' in info['name'].lower():\r\n                self.get_logger().info(f'Found RealSpeaker at index {i}')\r\n                return i\r\n\r\n        self.get_logger().warn(\r\n            'RealSpeaker not found, using default input device'\r\n        )\r\n        return None\r\n\r\n    def capture_audio_callback(self):\r\n        \"\"\"Capture audio frame from RealSpeaker\"\"\"\r\n        try:\r\n            # Read audio frame\r\n            data = self.stream.read(self.CHUNK, exception_on_overflow=False)\r\n\r\n            # Convert to numpy array\r\n            audio_data = np.frombuffer(data, dtype=np.int16)\r\n            audio_data = audio_data.reshape(self.CHUNK, self.CHANNELS)\r\n\r\n            # Publish raw audio\r\n            audio_msg = AudioData()\r\n            audio_msg.data = audio_data.tobytes()\r\n            self.raw_audio_pub.publish(audio_msg)\r\n\r\n            # Estimate sound direction using cross-correlation\r\n            direction = self._estimate_direction(audio_data)\r\n            if direction:\r\n                direction_msg = String()\r\n                direction_msg.data = direction\r\n                self.direction_pub.publish(direction_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Audio capture error: {str(e)}')\r\n\r\n    def _estimate_direction(self, audio_data):\r\n        \"\"\"\r\n        Estimate sound direction using time-difference-of-arrival (TDOA)\r\n        RealSpeaker mics are arranged in circular pattern\r\n        \"\"\"\r\n        # Calculate energy in each frequency band\r\n        energy = np.mean(np.abs(audio_data), axis=0)\r\n\r\n        # Find channel with highest energy (rough direction estimate)\r\n        max_channel = np.argmax(energy)\r\n\r\n        # Map channel to direction (RealSpeaker mic layout)\r\n        directions = [\r\n            'front',      # Channel 0\r\n            'front-left', # Channel 1\r\n            'left',       # Channel 2\r\n            'rear-left',  # Channel 3\r\n            'rear-right', # Channel 4\r\n            'right'       # Channel 5\r\n        ]\r\n\r\n        direction = directions[max_channel]\r\n        self.direction_history.append(direction)\r\n\r\n        # Return majority direction from last 10 frames\r\n        from collections import Counter\r\n        if len(self.direction_history) >= 5:\r\n            majority_direction = Counter(self.direction_history).most_common(1)[0][0]\r\n            return majority_direction\r\n\r\n        return None\r\n\r\n    def __del__(self):\r\n        if hasattr(self, 'stream'):\r\n            self.stream.stop_stream()\r\n            self.stream.close()\r\n        if hasattr(self, 'audio'):\r\n            self.audio.terminate()\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = RealSpeakerNode()\r\n    rclpy.spin(node)\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-3-speech-to-text-pipeline",children:"Part 3: Speech-to-Text Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"cloud-based-stt-google-cloud-speech-to-text",children:"Cloud-Based STT (Google Cloud Speech-to-Text)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nSpeech-to-Text using Google Cloud Speech-to-Text API\r\nReal-time streaming transcription from RealSpeaker\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom audio_common_msgs.msg import AudioData\r\nimport google.cloud.speech as speech\r\nimport queue\r\n\r\nclass SpeechToTextNode(Node):\r\n    def __init__(self):\r\n        super().__init__('speech_to_text')\r\n\r\n        # Initialize Google Speech-to-Text client\r\n        self.client = speech.SpeechClient()\r\n        self.config = speech.RecognitionConfig(\r\n            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\r\n            sample_rate_hertz=16000,\r\n            language_code='en-US',\r\n            model='latest_long',\r\n            use_enhanced=True,\r\n            enable_automatic_punctuation=True\r\n        )\r\n\r\n        # Audio queue for streaming\r\n        self.audio_queue = queue.Queue()\r\n\r\n        # Subscribe to audio\r\n        self.audio_sub = self.create_subscription(\r\n            AudioData,\r\n            '/respeaker/raw_audio',\r\n            self.audio_callback,\r\n            10\r\n        )\r\n\r\n        # Publish transcription\r\n        self.text_pub = self.create_publisher(\r\n            String,\r\n            '/speech/transcription',\r\n            10\r\n        )\r\n\r\n        self.confidence_pub = self.create_publisher(\r\n            String,\r\n            '/speech/confidence',\r\n            10\r\n        )\r\n\r\n        # Start streaming recognition in background thread\r\n        import threading\r\n        self.streaming_thread = threading.Thread(\r\n            target=self._streaming_recognize_loop,\r\n            daemon=True\r\n        )\r\n        self.streaming_thread.start()\r\n\r\n        self.get_logger().info('Speech-to-Text initialized')\r\n\r\n    def audio_callback(self, msg):\r\n        \"\"\"Queue incoming audio for STT processing\"\"\"\r\n        self.audio_queue.put(msg.data)\r\n\r\n    def _streaming_recognize_loop(self):\r\n        \"\"\"Continuous streaming speech recognition\"\"\"\r\n        def request_generator():\r\n            while True:\r\n                try:\r\n                    # Get audio from queue with timeout\r\n                    audio_content = self.audio_queue.get(timeout=1.0)\r\n\r\n                    yield speech.StreamingRecognizeRequest(\r\n                        audio_content=audio_content\r\n                    )\r\n                except queue.Empty:\r\n                    continue\r\n                except Exception as e:\r\n                    self.get_logger().error(f'STT error: {str(e)}')\r\n                    break\r\n\r\n        try:\r\n            # Start streaming request\r\n            requests = request_generator()\r\n            responses = self.client.streaming_recognize(\r\n                self.config,\r\n                requests\r\n            )\r\n\r\n            for response in responses:\r\n                if not response.results:\r\n                    continue\r\n\r\n                result = response.results[0]\r\n\r\n                if result.is_final:\r\n                    # Final transcription\r\n                    transcript = result.alternatives[0].transcript\r\n                    confidence = result.alternatives[0].confidence\r\n\r\n                    # Publish transcription\r\n                    text_msg = String()\r\n                    text_msg.data = transcript\r\n                    self.text_pub.publish(text_msg)\r\n\r\n                    # Publish confidence score\r\n                    conf_msg = String()\r\n                    conf_msg.data = f'{confidence:.2%}'\r\n                    self.confidence_pub.publish(conf_msg)\r\n\r\n                    self.get_logger().info(\r\n                        f'Speech: \"{transcript}\" ({confidence:.2%})'\r\n                    )\r\n                else:\r\n                    # Interim (partial) result\r\n                    interim = result.alternatives[0].transcript\r\n                    self.get_logger().debug(f'Interim: \"{interim}\"')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Streaming recognition failed: {str(e)}')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = SpeechToTextNode()\r\n    rclpy.spin(node)\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-4-vision-language-model-integration",children:"Part 4: Vision-Language Model Integration"}),"\n",(0,t.jsx)(n.h3,{id:"llava-open-source-vlm-on-jetson",children:"LLaVA (Open-Source VLM) on Jetson"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nVision-Language Model (LLaVA) Integration for ROS 2\r\nProcesses camera images with language reasoning\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nfrom PIL import Image as PILImage\r\nimport torch\r\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\r\nimport time\r\n\r\nclass LLaVAVisionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('llava_vision')\r\n\r\n        self.bridge = CvBridge()\r\n\r\n        # Load LLaVA model (lightweight 7B version for Jetson)\r\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n        self.get_logger().info(f'Using device: {self.device}')\r\n\r\n        try:\r\n            model_id = 'llava-hf/llava-1.5-7b-hf'\r\n            self.processor = AutoProcessor.from_pretrained(model_id)\r\n            self.model = LlavaForConditionalGeneration.from_pretrained(\r\n                model_id,\r\n                torch_dtype=torch.float16,\r\n                device_map='auto'\r\n            )\r\n            self.get_logger().info(f'LLaVA model loaded')\r\n        except Exception as e:\r\n            self.get_logger().error(f'Failed to load LLaVA: {str(e)}')\r\n            return\r\n\r\n        # Subscribe to camera and text input\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/color/image_raw',\r\n            self.image_callback,\r\n            1  # Single queuing for latest frame\r\n        )\r\n\r\n        self.query_sub = self.create_subscription(\r\n            String,\r\n            '/robot/query',\r\n            self.query_callback,\r\n            10\r\n        )\r\n\r\n        # Publishers\r\n        self.answer_pub = self.create_publisher(\r\n            String,\r\n            '/robot/answer',\r\n            10\r\n        )\r\n\r\n        self.latency_pub = self.create_publisher(\r\n            String,\r\n            '/robot/inference_latency',\r\n            10\r\n        )\r\n\r\n        # Store latest image\r\n        self.latest_image = None\r\n        self.latest_query = None\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Store latest camera image\"\"\"\r\n        try:\r\n            self.latest_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\r\n        except Exception as e:\r\n            self.get_logger().error(f'Image conversion error: {str(e)}')\r\n\r\n    def query_callback(self, msg):\r\n        \"\"\"Process VLM query when image is available\"\"\"\r\n        if self.latest_image is None:\r\n            self.get_logger().warn('No image available yet')\r\n            return\r\n\r\n        query = msg.data\r\n        self.get_logger().info(f'VLM Query: \"{query}\"')\r\n\r\n        try:\r\n            # Convert OpenCV image to PIL\r\n            pil_image = PILImage.fromarray(\r\n                cv2.cvtColor(self.latest_image, cv2.COLOR_BGR2RGB)\r\n            )\r\n\r\n            # Prepare input for VLM\r\n            prompt = f'<image>\\n{query}'\r\n\r\n            start_time = time.time()\r\n\r\n            inputs = self.processor(\r\n                text=prompt,\r\n                images=pil_image,\r\n                return_tensors='pt'\r\n            ).to(self.device)\r\n\r\n            # Generate answer\r\n            with torch.no_grad():\r\n                output = self.model.generate(\r\n                    **inputs,\r\n                    max_new_tokens=200,\r\n                    do_sample=False\r\n                )\r\n\r\n            answer = self.processor.decode(\r\n                output[0],\r\n                skip_special_tokens=True\r\n            )\r\n\r\n            latency = (time.time() - start_time) * 1000\r\n\r\n            # Extract answer from response\r\n            answer = answer.split('Assistant:')[-1].strip()\r\n\r\n            # Publish answer\r\n            answer_msg = String()\r\n            answer_msg.data = answer\r\n            self.answer_pub.publish(answer_msg)\r\n\r\n            # Publish latency\r\n            latency_msg = String()\r\n            latency_msg.data = f'{latency:.1f}ms'\r\n            self.latency_pub.publish(latency_msg)\r\n\r\n            self.get_logger().info(\r\n                f'Answer: \"{answer}\" (Latency: {latency:.1f}ms)'\r\n            )\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'VLM processing error: {str(e)}')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = LLaVAVisionNode()\r\n    rclpy.spin(node)\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-5-text-to-speech-output",children:"Part 5: Text-to-Speech Output"}),"\n",(0,t.jsx)(n.h3,{id:"tts-implementation",children:"TTS Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nText-to-Speech for Robot Responses\r\nConverts planned actions back to natural language\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport pyttsx3\r\nimport threading\r\n\r\nclass TextToSpeechNode(Node):\r\n    def __init__(self):\r\n        super().__init__('text_to_speech')\r\n\r\n        # Initialize TTS engine\r\n        self.engine = pyttsx3.init()\r\n        self.engine.setProperty('rate', 150)  # Speech rate (words/min)\r\n        self.engine.setProperty('volume', 0.9)  # Volume 0-1\r\n\r\n        # Set voice (prefer female voice if available)\r\n        voices = self.engine.getProperty('voices')\r\n        if len(voices) > 1:\r\n            self.engine.setProperty('voice', voices[1].id)\r\n\r\n        # Subscribe to text to speak\r\n        self.tts_sub = self.create_subscription(\r\n            String,\r\n            '/robot/response',\r\n            self.tts_callback,\r\n            10\r\n        )\r\n\r\n        self.get_logger().info('Text-to-Speech initialized')\r\n\r\n    def tts_callback(self, msg):\r\n        \"\"\"Speak response in background thread\"\"\"\r\n        text = msg.data\r\n\r\n        # Run TTS in background to not block ROS 2\r\n        tts_thread = threading.Thread(\r\n            target=self._speak,\r\n            args=(text,),\r\n            daemon=True\r\n        )\r\n        tts_thread.start()\r\n\r\n    def _speak(self, text):\r\n        \"\"\"Speak text asynchronously\"\"\"\r\n        try:\r\n            self.get_logger().info(f'Speaking: \"{text}\"')\r\n            self.engine.say(text)\r\n            self.engine.runAndWait()\r\n        except Exception as e:\r\n            self.get_logger().error(f'TTS error: {str(e)}')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = TextToSpeechNode()\r\n    rclpy.spin(node)\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-6-complete-vla-agent-pipeline",children:"Part 6: Complete VLA Agent Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"end-to-end-conversational-robot",children:"End-to-End Conversational Robot"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nVision-Language-Action Agent\r\nComplete pipeline: Hear \u2192 Understand \u2192 See \u2192 Reason \u2192 Act\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\nimport json\r\nimport time\r\n\r\nclass VLAAgent(Node):\r\n    def __init__(self):\r\n        super().__init__('vla_agent')\r\n\r\n        # Subscriptions\r\n        self.speech_sub = self.create_subscription(\r\n            String,\r\n            '/speech/transcription',\r\n            self.on_speech,\r\n            10\r\n        )\r\n\r\n        self.vision_sub = self.create_subscription(\r\n            String,\r\n            '/robot/answer',\r\n            self.on_vision_answer,\r\n            10\r\n        )\r\n\r\n        # Publishers\r\n        self.query_pub = self.create_publisher(String, '/robot/query', 10)\r\n        self.plan_pub = self.create_publisher(String, '/robot/plan', 10)\r\n        self.command_pub = self.create_publisher(String, '/robot/command', 10)\r\n        self.response_pub = self.create_publisher(String, '/robot/response', 10)\r\n        self.vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n\r\n        # State machine\r\n        self.state = 'idle'  # idle, listening, understanding, planning, executing\r\n        self.last_query = None\r\n        self.last_vision_answer = None\r\n\r\n        self.get_logger().info('VLA Agent initialized')\r\n\r\n    def on_speech(self, msg):\r\n        \"\"\"Process speech input\"\"\"\r\n        speech = msg.data\r\n        self.get_logger().info(f'[SPEECH] User: \"{speech}\"')\r\n\r\n        if self.state == 'idle' or self.state == 'executing':\r\n            self.state = 'understanding'\r\n            self.process_command(speech)\r\n\r\n    def process_command(self, command):\r\n        \"\"\"\r\n        Multi-step processing:\r\n        1. Ask vision system about environment\r\n        2. Generate plan based on vision + command\r\n        3. Execute plan\r\n        \"\"\"\r\n\r\n        # Step 1: Get scene understanding from vision\r\n        self.get_logger().info('[VLA] Step 1: Requesting scene understanding...')\r\n        self.last_query = command\r\n\r\n        query_msg = String()\r\n        query_msg.data = f'What do you see in this image? Help me: {command}'\r\n        self.query_pub.publish(query_msg)\r\n\r\n        # Wait for vision response (or timeout)\r\n        self.state = 'understanding'\r\n\r\n    def on_vision_answer(self, msg):\r\n        \"\"\"Process vision system response\"\"\"\r\n        vision_answer = msg.data\r\n        self.get_logger().info(f'[VISION] \"{vision_answer}\"')\r\n\r\n        if self.state != 'understanding':\r\n            return\r\n\r\n        self.last_vision_answer = vision_answer\r\n\r\n        # Step 2: Generate action plan based on speech + vision\r\n        self.get_logger().info('[VLA] Step 2: Generating action plan...')\r\n        self.generate_plan(self.last_query, vision_answer)\r\n\r\n    def generate_plan(self, command, vision_context):\r\n        \"\"\"Generate robot action plan\"\"\"\r\n\r\n        # Create context-aware prompt\r\n        plan_prompt = f\"\"\"Based on the user command and scene understanding, generate a robot action plan.\r\n\r\nUser Command: \"{command}\"\r\n\r\nScene Understanding: \"{vision_context}\"\r\n\r\nRobot Capabilities:\r\n- move_forward(distance) - Move forward X meters\r\n- turn(angle) - Turn X degrees\r\n- grasp(object) - Grasp detected object\r\n- release() - Open gripper\r\n- say(text) - Speak to user\r\n\r\nGenerate a JSON action sequence:\r\n{{\r\n  \"reasoning\": \"explanation\",\r\n  \"actions\": [\r\n    {{\"type\": \"move_forward\", \"distance\": 2.0}},\r\n    {{\"type\": \"grasp\", \"object\": \"cube\"}}\r\n  ],\r\n  \"say\": \"I've completed the task\"\r\n}}\"\"\"\r\n\r\n        # In real system, send to LLM for planning\r\n        # For demo, use simple rule-based planning\r\n        plan = {\r\n            'reasoning': f'Executing: {command}',\r\n            'actions': [\r\n                {'type': 'move_forward', 'distance': 1.0},\r\n                {'type': 'say', 'text': f'Understood. {vision_context}'}\r\n            ]\r\n        }\r\n\r\n        # Step 3: Execute plan\r\n        self.state = 'executing'\r\n        self.get_logger().info('[VLA] Step 3: Executing action plan...')\r\n        self.execute_plan(plan)\r\n\r\n    def execute_plan(self, plan):\r\n        \"\"\"Execute generated action plan\"\"\"\r\n        for action in plan['actions']:\r\n            action_type = action['type']\r\n\r\n            if action_type == 'move_forward':\r\n                self.get_logger().info(\r\n                    f'Moving forward {action[\"distance\"]}m'\r\n                )\r\n                twist = Twist()\r\n                twist.linear.x = 0.5\r\n                self.vel_pub.publish(twist)\r\n                time.sleep(action['distance'] / 0.5)\r\n                twist.linear.x = 0.0\r\n                self.vel_pub.publish(twist)\r\n\r\n            elif action_type == 'say':\r\n                response_msg = String()\r\n                response_msg.data = action['text']\r\n                self.response_pub.publish(response_msg)\r\n\r\n            elif action_type == 'grasp':\r\n                self.get_logger().info(\r\n                    f'Grasping: {action[\"object\"]}'\r\n                )\r\n\r\n        self.state = 'idle'\r\n        self.get_logger().info('[VLA] Plan executed. Ready for next command.')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    agent = VLAAgent()\r\n    rclpy.spin(agent)\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-exercise-build-a-conversational-robot",children:"Hands-On Exercise: Build a Conversational Robot"}),"\n",(0,t.jsx)(n.h3,{id:"exercise-1-voice-controlled-navigation",children:"Exercise 1: Voice-Controlled Navigation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: RealSpeaker mic\r\nros2 run perception_nodes respeaker_node.py\r\n\r\n# Terminal 2: Speech-to-text\r\nros2 run speech_nodes speech_to_text.py\r\n\r\n# Terminal 3: VLA Agent\r\nros2 run robot_agents vla_agent.py\r\n\r\n# Terminal 4: Test voice command\r\nros2 topic pub /robot/query std_msgs/String \"data: 'What objects are in front of me?'\"\r\n\r\n# Expected: Robot processes audio \u2192 converts to text \u2192 queries vision \u2192 speaks response\n"})}),"\n",(0,t.jsx)(n.h3,{id:"exercise-2-visual-question-answering",children:"Exercise 2: Visual Question Answering"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n# Test VLM with different questions\r\n\r\nimport subprocess\r\nimport time\r\n\r\nquestions = [\r\n    "What color is the cube?",\r\n    "Is the path to the door clear?",\r\n    "How many objects do you see?",\r\n    "What\'s to the left of the red cube?",\r\n    "Can I grasp the item on the shelf?"\r\n]\r\n\r\nfor q in questions:\r\n    print(f"\\n[QUERY] {q}")\r\n    # Publish question\r\n    subprocess.run([\r\n        \'ros2\', \'topic\', \'pub\', \'/robot/query\',\r\n        \'std_msgs/String\', f\'data: "{q}"\'\r\n    ])\r\n    time.sleep(3)  # Wait for answer\n'})}),"\n",(0,t.jsx)(n.h3,{id:"exercise-3-multi-step-conversational-task",children:"Exercise 3: Multi-Step Conversational Task"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Simulate multi-turn conversation\r\n\r\nUser: "Is there a cup on the table?"\r\nRobot: [Looks at camera] "Yes, I see a white cup on the left side of the table."\r\n\r\nUser: "Pick it up and place it in the sink"\r\nRobot: [Plans path + grasping] "I\'ll navigate to the table, grasp the cup,\r\n       and move to the sink." [Executes]\r\n       [After success] "Task complete! The cup is now in the sink."\r\n\r\nUser: "What did you just do?"\r\nRobot: [References memory] "I navigated to the table, grasped a white cup,\r\n       and placed it in the sink as you requested."\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"common-vla-errors--debugging",children:"Common VLA Errors & Debugging"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Error"}),(0,t.jsx)(n.th,{children:"Cause"}),(0,t.jsx)(n.th,{children:"Fix"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:'"No audio device found"'})}),(0,t.jsx)(n.td,{children:"RealSpeaker not detected"}),(0,t.jsxs)(n.td,{children:["Check USB connection, run ",(0,t.jsx)(n.code,{children:"arecord -l"})]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:'"VLM timeout (>30s)"'})}),(0,t.jsx)(n.td,{children:"Model loading from internet"}),(0,t.jsx)(n.td,{children:"Pre-download LLaVA weights, use smaller model"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:'"Speech recognition failure"'})}),(0,t.jsx)(n.td,{children:"Noisy environment or accent"}),(0,t.jsx)(n.td,{children:"Adjust STT confidence threshold, fine-tune on local speech"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:'"Plan hallucination"'})}),(0,t.jsx)(n.td,{children:"LLM generates invalid actions"}),(0,t.jsx)(n.td,{children:"Use constraint checking, validate actions before execution"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Audio-vision sync issue"})}),(0,t.jsx)(n.td,{children:"Different frame rates"}),(0,t.jsx)(n.td,{children:"Add timestamp synchronization between audio/video"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"CUDA out of memory"})}),(0,t.jsx)(n.td,{children:"VLM + Vision pipeline too large"}),(0,t.jsx)(n.td,{children:"Use int8 quantization, reduce image resolution"})]})]})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Vision-Language Models Enable True Multimodal Robots"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"VLMs understand images in natural language context"}),"\n",(0,t.jsx)(n.li,{children:"LLaVA is free and runs locally on Jetson"}),"\n",(0,t.jsx)(n.li,{children:"Cloud VLMs (GPT-4V) offer higher accuracy but cost $0.01/image"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Hearing + Speech Requires Multiple Components"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"RealSpeaker beamforming captures speech direction"}),"\n",(0,t.jsx)(n.li,{children:"Speech-to-text converts audio to commands"}),"\n",(0,t.jsx)(n.li,{children:"Text-to-speech provides natural robot responses"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"VLA Pipeline: Hear \u2192 Understand \u2192 See \u2192 Reason \u2192 Act"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Speech captured and transcribed"}),"\n",(0,t.jsx)(n.li,{children:"Vision models answer questions about scene"}),"\n",(0,t.jsx)(n.li,{children:"LLMs generate action plans"}),"\n",(0,t.jsx)(n.li,{children:"Robot executes and reports results"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Real-Time Constraints Are Critical"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Total latency must be <2 seconds for natural conversation"}),"\n",(0,t.jsx)(n.li,{children:"Break down into: STT (500ms) + Vision (1000ms) + Planning (200ms) + TTS (500ms)"}),"\n",(0,t.jsx)(n.li,{children:"Optimize slowest components first"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Edge Deployment is Practical"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"LLaVA 7B runs on Jetson in 500-1000ms"}),"\n",(0,t.jsx)(n.li,{children:"Local TTS via pyttsx3 is instant"}),"\n",(0,t.jsx)(n.li,{children:"Offline capability with graceful cloud fallback"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,t.jsx)(n.h3,{id:"vision-language-model-papers",children:"Vision-Language Model Papers"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2304.08485",children:"LLaVA: Large Language and Vision Assistant"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://cdn.openai.com/papers/GPTV_System_Card.pdf",children:"GPT-4V System Card"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2312.11805",children:"Gemini: A Family of Highly Capable Multimodal Models"})}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"robot-conversation--hri",children:"Robot Conversation & HRI"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2310.09549",children:"Towards Natural Human-Robot Interaction"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2202.07308",children:"Real-time Dialogue Systems for Robots"})}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"implementation-resources",children:"Implementation Resources"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/haotian-liu/LLaVA",children:"LLaVA GitHub"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/respeaker/respeaker_ros",children:"RealSpeaker Documentation"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://cloud.google.com/speech-to-text",children:"Google Cloud Speech-to-Text"})}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"next-lesson",children:"Next Lesson"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Lesson 4.3: Human-Robot Interaction & Safety Framework"})}),"\n",(0,t.jsxs)(n.p,{children:["Now that your robot can see, hear, and speak, we'll add the critical layer: ",(0,t.jsx)(n.strong,{children:"safety"}),". You'll learn:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Collision detection and emergency stop mechanisms"}),"\n",(0,t.jsx)(n.li,{children:"Safe human-robot collaboration"}),"\n",(0,t.jsx)(n.li,{children:"Ethical AI frameworks and bias mitigation"}),"\n",(0,t.jsx)(n.li,{children:"Real-world failure analysis from robot incidents"}),"\n",(0,t.jsx)(n.li,{children:"Production-ready safety checklist"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The robot must be safe BEFORE it's smart."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:function(e,n,r){r.d(n,{R:function(){return a},x:function(){return o}});var i=r(6540);const t={},s=i.createContext(t);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);