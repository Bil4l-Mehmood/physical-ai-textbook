"use strict";(self.webpackChunkphysical_ai_textbook=self.webpackChunkphysical_ai_textbook||[]).push([[585],{8453:function(e,n,r){r.d(n,{R:function(){return a},x:function(){return o}});var t=r(6540);const s={},i=t.createContext(s);function a(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(i.Provider,{value:n},e.children)}},8887:function(e,n,r){r.r(n),r.d(n,{assets:function(){return l},contentTitle:function(){return o},default:function(){return m},frontMatter:function(){return a},metadata:function(){return t},toc:function(){return c}});var t=JSON.parse('{"id":"chapter-4/4-3-hri-safety","title":"Human-Robot Interaction & Safety: Building Trustworthy Robots","description":"Design safe, ethical, and trustworthy human-robot systems with collision detection, safety constraints, and ethical AI frameworks","source":"@site/docs/chapter-4/4-3-hri-safety.md","sourceDirName":"chapter-4","slug":"/chapter-4/4-3-hri-safety","permalink":"/physical-ai-textbook/docs/chapter-4/4-3-hri-safety","draft":false,"unlisted":false,"editUrl":"https://github.com/Bil4l-Mehmood/physical-ai-textbook/edit/main/docs/chapter-4/4-3-hri-safety.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"sidebar_label":"Lesson 4.3: HRI & Safety","title":"Human-Robot Interaction & Safety: Building Trustworthy Robots","description":"Design safe, ethical, and trustworthy human-robot systems with collision detection, safety constraints, and ethical AI frameworks","duration":120,"difficulty":"Advanced","hardware":["Jetson Orin Nano","RealSense D435i","Force/Torque sensors","ROS 2 Humble"],"prerequisites":["Lesson 4.2: Conversational AI & VLA"]},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.2: Conversational AI & VLA","permalink":"/physical-ai-textbook/docs/chapter-4/4-2-conversational-ai"}}'),s=r(4848),i=r(8453);const a={sidebar_position:3,sidebar_label:"Lesson 4.3: HRI & Safety",title:"Human-Robot Interaction & Safety: Building Trustworthy Robots",description:"Design safe, ethical, and trustworthy human-robot systems with collision detection, safety constraints, and ethical AI frameworks",duration:120,difficulty:"Advanced",hardware:["Jetson Orin Nano","RealSense D435i","Force/Torque sensors","ROS 2 Humble"],prerequisites:["Lesson 4.2: Conversational AI & VLA"]},o="Lesson 4.3: Human-Robot Interaction & Safety - Building Trustworthy Robots",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2},{value:"Recommended Safety Sensors",id:"recommended-safety-sensors",level:3},{value:"Safety Architecture",id:"safety-architecture",level:3},{value:"Part 1: Safety Fundamentals &amp; Standards",id:"part-1-safety-fundamentals--standards",level:2},{value:"Relevant Safety Standards",id:"relevant-safety-standards",level:3},{value:"Force Limits for Safe Human-Robot Collaboration",id:"force-limits-for-safe-human-robot-collaboration",level:3},{value:"Part 2: Real-Time Collision Detection &amp; Avoidance",id:"part-2-real-time-collision-detection--avoidance",level:2},{value:"Safety Monitoring Node",id:"safety-monitoring-node",level:3},{value:"Collision Avoidance with Depth Camera",id:"collision-avoidance-with-depth-camera",level:3},{value:"Part 3: Ethical AI &amp; Bias Mitigation",id:"part-3-ethical-ai--bias-mitigation",level:2},{value:"Identifying AI Bias in Robot Systems",id:"identifying-ai-bias-in-robot-systems",level:3},{value:"Bias Detection &amp; Mitigation",id:"bias-detection--mitigation",level:3},{value:"Part 4: Transparent AI Decision-Making",id:"part-4-transparent-ai-decision-making",level:2},{value:"Explainability for Robot Actions",id:"explainability-for-robot-actions",level:3},{value:"Part 5: Real-World Robot Failure Analysis",id:"part-5-real-world-robot-failure-analysis",level:2},{value:"Critical Incidents &amp; Lessons",id:"critical-incidents--lessons",level:3},{value:"Part 6: Production Safety Checklist",id:"part-6-production-safety-checklist",level:2},{value:"Pre-Deployment Safety Audit",id:"pre-deployment-safety-audit",level:3},{value:"Hands-On Exercise: Safety-Critical System Design",id:"hands-on-exercise-safety-critical-system-design",level:2},{value:"Exercise 1: Test Safety Limits",id:"exercise-1-test-safety-limits",level:3},{value:"Exercise 2: Collision Detection Test",id:"exercise-2-collision-detection-test",level:3},{value:"Exercise 3: Bias Detection Validation",id:"exercise-3-bias-detection-validation",level:3},{value:"Common HRI &amp; Safety Errors",id:"common-hri--safety-errors",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Safety Standards &amp; Guidelines",id:"safety-standards--guidelines",level:3},{value:"Ethics in Robotics",id:"ethics-in-robotics",level:3},{value:"Real-World Incidents",id:"real-world-incidents",level:3},{value:"Capstone Project: End-to-End Safe Robot System",id:"capstone-project-end-to-end-safe-robot-system",level:2},{value:"Complete System Architecture",id:"complete-system-architecture",level:3},{value:"Capstone Implementation",id:"capstone-implementation",level:3},{value:"Course Summary",id:"course-summary",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lesson-43-human-robot-interaction--safety---building-trustworthy-robots",children:"Lesson 4.3: Human-Robot Interaction & Safety - Building Trustworthy Robots"})}),"\n",(0,s.jsxs)(n.admonition,{title:"Lesson Overview",type:"info",children:[(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Duration"}),": 120 minutes | ",(0,s.jsx)(n.strong,{children:"Difficulty"}),": Advanced | ",(0,s.jsx)(n.strong,{children:"Hardware"}),": Full sensor suite with safety monitoring"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prerequisites"}),": All Chapter 4 lessons (LLM, VLA)"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Learning Outcome"}),": Design and deploy robots that operate safely around humans, with collision avoidance, ethical decision-making, and transparency in AI reasoning"]})]}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand human-robot interaction (HRI) principles and safety standards"}),"\n",(0,s.jsx)(n.li,{children:"Implement real-time collision detection and avoidance"}),"\n",(0,s.jsx)(n.li,{children:"Design safety constraints and emergency stop mechanisms"}),"\n",(0,s.jsx)(n.li,{children:"Apply ethical AI frameworks to robot decision-making"}),"\n",(0,s.jsx)(n.li,{children:"Identify and mitigate AI bias in robot perception and planning"}),"\n",(0,s.jsx)(n.li,{children:"Handle adversarial inputs and edge cases robustly"}),"\n",(0,s.jsx)(n.li,{children:"Create transparent AI explanations for robot decisions"}),"\n",(0,s.jsx)(n.li,{children:"Analyze real-world robot failure cases"}),"\n",(0,s.jsx)(n.li,{children:"Build production-ready safety architectures"}),"\n",(0,s.jsx)(n.li,{children:"Deploy robots in shared human-robot environments"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,s.jsx)(n.admonition,{title:"Safety is Hardware-Critical",type:"danger",children:(0,s.jsx)(n.p,{children:"Robot safety depends on redundant sensors and fail-safe mechanisms. A single sensor failure must not cause injury."})}),"\n",(0,s.jsx)(n.h3,{id:"recommended-safety-sensors",children:"Recommended Safety Sensors"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Sensor"}),(0,s.jsx)(n.th,{children:"Purpose"}),(0,s.jsx)(n.th,{children:"Specifications"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Force/Torque Sensor"})}),(0,s.jsx)(n.td,{children:"Gripper collision detection"}),(0,s.jsx)(n.td,{children:"6-axis, <50N threshold"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Proximity Sensor"})}),(0,s.jsx)(n.td,{children:"Warning before contact"}),(0,s.jsx)(n.td,{children:"Ultrasonic, 10cm-2m range"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Pressure Pad"})}),(0,s.jsx)(n.td,{children:"Bumper for emergency stop"}),(0,s.jsx)(n.td,{children:"Entire gripper perimeter"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Light Curtain"})}),(0,s.jsx)(n.td,{children:"Restricted area detection"}),(0,s.jsx)(n.td,{children:"Infrared, 0.3-3m range"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Emergency Button"})}),(0,s.jsx)(n.td,{children:"Manual override"}),(0,s.jsx)(n.td,{children:"Red, mushroom style"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Encoder Feedback"})}),(0,s.jsx)(n.td,{children:"Movement verification"}),(0,s.jsx)(n.td,{children:"On each motor"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"safety-architecture",children:"Safety Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"User Input\r\n    \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 SAFETY CONSTRAINT CHECKER               \u2502\r\n\u2502 \u2022 Speed limits check                     \u2502\r\n\u2502 \u2022 Collision detection check              \u2502\r\n\u2502 \u2022 Authorized workspace check             \u2502\r\n\u2502 \u2022 Gripper force limits check             \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u2193 [PASS] or [FAIL - STOP]\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 EXECUTION WITH MONITORING                \u2502\r\n\u2502 \u2022 Real-time force/torque monitoring      \u2502\r\n\u2502 \u2022 Continuous proximity check              \u2502\r\n\u2502 \u2022 Verify motion feedback (encoders)      \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n    \u2193 [Detect anomaly] \u2192 EMERGENCY STOP\r\nRobot Executes\r\n    \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 POST-EXECUTION ANALYSIS                  \u2502\r\n\u2502 \u2022 Log all sensor data                    \u2502\r\n\u2502 \u2022 Verify intended vs actual motion      \u2502\r\n\u2502 \u2022 Update safety parameters if needed     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-1-safety-fundamentals--standards",children:"Part 1: Safety Fundamentals & Standards"}),"\n",(0,s.jsx)(n.h3,{id:"relevant-safety-standards",children:"Relevant Safety Standards"}),"\n",(0,s.jsx)(n.p,{children:"Robot safety is governed by international standards:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Standard"}),(0,s.jsx)(n.th,{children:"Focus"}),(0,s.jsx)(n.th,{children:"Key Requirements"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"ISO/TS 15066"})}),(0,s.jsx)(n.td,{children:"Collaborative robots (cobots)"}),(0,s.jsx)(n.td,{children:"Force/torque limits by body part"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"ANSI/RIA R15.06"})}),(0,s.jsx)(n.td,{children:"Industrial robot safety"}),(0,s.jsx)(n.td,{children:"Emergency stops, protective devices"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"EN 61508"})}),(0,s.jsx)(n.td,{children:"Functional safety"}),(0,s.jsx)(n.td,{children:"SIL ratings (1-4), failure modes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"ISO 13849"})}),(0,s.jsx)(n.td,{children:"Safety control systems"}),(0,s.jsx)(n.td,{children:"PLd/PLe performance levels"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"force-limits-for-safe-human-robot-collaboration",children:"Force Limits for Safe Human-Robot Collaboration"}),"\n",(0,s.jsx)(n.p,{children:"Different body parts have different injury thresholds:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Safe Transient Contact Force (ISO/TS 15066):\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Body Part       \u2502 Max Force (N)\u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 Head            \u2502 40           \u2502\r\n\u2502 Face            \u2502 27           \u2502\r\n\u2502 Neck            \u2502 50           \u2502\r\n\u2502 Chest           \u2502 210          \u2502\r\n\u2502 Abdomen         \u2502 155          \u2502\r\n\u2502 Hand            \u2502 220          \u2502\r\n\u2502 Foot            \u2502 200          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nApplication: Robot gripper max force should be &lt;50N\r\n            for safe human interaction\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-2-real-time-collision-detection--avoidance",children:"Part 2: Real-Time Collision Detection & Avoidance"}),"\n",(0,s.jsx)(n.h3,{id:"safety-monitoring-node",children:"Safety Monitoring Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nReal-Time Safety Monitor\r\nContinuous collision detection and emergency stop system\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import JointState\r\nfrom geometry_msgs.msg import Twist, WrenchStamped\r\nfrom std_msgs.msg import Float32, Bool\r\nimport numpy as np\r\nimport time\r\n\r\nclass SafetyMonitorNode(Node):\r\n    def __init__(self):\r\n        super().__init__('safety_monitor')\r\n\r\n        # Safety parameters\r\n        self.SPEED_LIMIT = 0.5  # m/s (walking speed)\r\n        self.FORCE_LIMIT = 50.0  # Newtons\r\n        self.GRIPPER_FORCE_LIMIT = 30.0  # Newtons\r\n        self.PROXIMITY_THRESHOLD = 0.3  # meters (30cm warning distance)\r\n        self.EMERGENCY_STOP_ACTIVE = False\r\n\r\n        # Subscriptions\r\n        self.joint_state_sub = self.create_subscription(\r\n            JointState,\r\n            '/joint_states',\r\n            self.joint_state_callback,\r\n            10\r\n        )\r\n\r\n        self.ft_sensor_sub = self.create_subscription(\r\n            WrenchStamped,\r\n            '/ft_sensor/wrench',\r\n            self.force_callback,\r\n            10\r\n        )\r\n\r\n        self.proximity_sub = self.create_subscription(\r\n            Float32,\r\n            '/proximity_sensor/distance',\r\n            self.proximity_callback,\r\n            10\r\n        )\r\n\r\n        # Publishers\r\n        self.emergency_stop_pub = self.create_publisher(\r\n            Bool,\r\n            '/safety/emergency_stop',\r\n            10\r\n        )\r\n\r\n        self.velocity_limit_pub = self.create_publisher(\r\n            Float32,\r\n            '/safety/velocity_limit',\r\n            10\r\n        )\r\n\r\n        self.force_limit_pub = self.create_publisher(\r\n            Float32,\r\n            '/safety/force_limit',\r\n            10\r\n        )\r\n\r\n        self.safety_status_pub = self.create_publisher(\r\n            Bool,\r\n            '/safety/status_ok',\r\n            10\r\n        )\r\n\r\n        # State tracking\r\n        self.joint_velocities = {}\r\n        self.measured_force = 0.0\r\n        self.proximity_distance = float('inf')\r\n        self.last_check_time = time.time()\r\n\r\n        # Safety timer\r\n        self.create_timer(0.05, self.safety_check_callback)  # 50ms = 20Hz\r\n\r\n        self.get_logger().info('Safety Monitor initialized')\r\n\r\n    def joint_state_callback(self, msg):\r\n        \"\"\"Track joint velocities\"\"\"\r\n        for i, name in enumerate(msg.name):\r\n            if i < len(msg.velocity):\r\n                self.joint_velocities[name] = msg.velocity[i]\r\n\r\n    def force_callback(self, msg):\r\n        \"\"\"Monitor force/torque sensor\"\"\"\r\n        # Calculate magnitude of force vector\r\n        fx = msg.wrench.force.x\r\n        fy = msg.wrench.force.y\r\n        fz = msg.wrench.force.z\r\n        self.measured_force = np.sqrt(fx**2 + fy**2 + fz**2)\r\n\r\n    def proximity_callback(self, msg):\r\n        \"\"\"Monitor proximity sensor\"\"\"\r\n        self.proximity_distance = msg.data\r\n\r\n    def safety_check_callback(self):\r\n        \"\"\"Continuous safety monitoring\"\"\"\r\n        violations = []\r\n\r\n        # Check 1: Velocity limits\r\n        for joint, velocity in self.joint_velocities.items():\r\n            if abs(velocity) > self.SPEED_LIMIT:\r\n                violations.append(\r\n                    f'Joint {joint} exceeds speed limit: {velocity:.2f} m/s'\r\n                )\r\n\r\n        # Check 2: Force limits (collision detection)\r\n        if self.measured_force > self.FORCE_LIMIT:\r\n            violations.append(\r\n                f'Force limit exceeded: {self.measured_force:.1f}N > {self.FORCE_LIMIT}N'\r\n            )\r\n\r\n        # Check 3: Proximity warning\r\n        if self.proximity_distance < self.PROXIMITY_THRESHOLD:\r\n            self.get_logger().warn(\r\n                f'Object detected {self.proximity_distance:.2f}m away'\r\n            )\r\n\r\n        # Decide on emergency stop\r\n        if violations:\r\n            self.get_logger().error(f'Safety violations: {violations}')\r\n            self.EMERGENCY_STOP_ACTIVE = True\r\n\r\n            # Publish emergency stop\r\n            stop_msg = Bool()\r\n            stop_msg.data = True\r\n            self.emergency_stop_pub.publish(stop_msg)\r\n\r\n            # Publish that safety is NOT OK\r\n            status_msg = Bool()\r\n            status_msg.data = False\r\n            self.safety_status_pub.publish(status_msg)\r\n        else:\r\n            # Safety OK\r\n            self.EMERGENCY_STOP_ACTIVE = False\r\n\r\n            stop_msg = Bool()\r\n            stop_msg.data = False\r\n            self.emergency_stop_pub.publish(stop_msg)\r\n\r\n            status_msg = Bool()\r\n            status_msg.data = True\r\n            self.safety_status_pub.publish(status_msg)\r\n\r\n            # Publish safe velocity limit\r\n            vel_msg = Float32()\r\n            vel_msg.data = self.SPEED_LIMIT\r\n            self.velocity_limit_pub.publish(vel_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    monitor = SafetyMonitorNode()\r\n    rclpy.spin(monitor)\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"collision-avoidance-with-depth-camera",children:"Collision Avoidance with Depth Camera"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nReal-Time Obstacle Avoidance\r\nUses depth camera to detect obstacles and modify planned trajectory\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom geometry_msgs.msg import Twist\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\n\r\nclass CollisionAvoidanceNode(Node):\r\n    def __init__(self):\r\n        super().__init__('collision_avoidance')\r\n\r\n        self.bridge = CvBridge()\r\n\r\n        # Subscribe to depth camera\r\n        self.depth_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/depth/image_rect_raw',\r\n            self.depth_callback,\r\n            10\r\n        )\r\n\r\n        # Publishers\r\n        self.cmd_vel_pub = self.create_publisher(\r\n            Twist,\r\n            '/cmd_vel_safe',  # Safety-checked velocity\r\n            10\r\n        )\r\n\r\n        self.obstacle_pub = self.create_publisher(\r\n            Image,\r\n            '/obstacles/debug',\r\n            10\r\n        )\r\n\r\n        # Safety parameters\r\n        self.DANGER_DISTANCE = 0.3  # 30cm - immediate danger\r\n        self.WARNING_DISTANCE = 0.5  # 50cm - warning\r\n        self.MAX_FORWARD_SPEED = 0.5  # m/s\r\n\r\n    def depth_callback(self, msg):\r\n        \"\"\"Process depth image for obstacle detection\"\"\"\r\n        try:\r\n            # Convert depth image\r\n            depth_image = self.bridge.imgmsg_to_cv2(msg)\r\n\r\n            # Convert to meters (depth images are often in mm)\r\n            depth_meters = depth_image.astype(np.float32) / 1000.0\r\n\r\n            # Find obstacles in front (center region of image)\r\n            h, w = depth_image.shape\r\n            center_region = depth_meters[h//2-50:h//2+50, w//2-100:w//2+100]\r\n\r\n            # Find minimum distance (closest obstacle)\r\n            min_distance = np.nanmin(center_region)\r\n\r\n            # Detect obstacles\r\n            danger_zone = depth_meters < self.DANGER_DISTANCE\r\n            warning_zone = depth_meters < self.WARNING_DISTANCE\r\n\r\n            # Create debug visualization\r\n            debug_image = np.zeros((h, w, 3), dtype=np.uint8)\r\n            debug_image[danger_zone] = [0, 0, 255]  # Red for danger\r\n            debug_image[warning_zone] = [0, 255, 255]  # Yellow for warning\r\n\r\n            # Publish debug image\r\n            debug_msg = self.bridge.cv2_to_imgmsg(debug_image, 'bgr8')\r\n            self.obstacle_pub.publish(debug_msg)\r\n\r\n            # Generate safe velocity command\r\n            safe_velocity = self._compute_safe_velocity(\r\n                min_distance\r\n            )\r\n\r\n            vel_msg = Twist()\r\n            vel_msg.linear.x = safe_velocity\r\n            self.cmd_vel_pub.publish(vel_msg)\r\n\r\n            self.get_logger().debug(\r\n                f'Obstacle distance: {min_distance:.2f}m, '\r\n                f'Safe velocity: {safe_velocity:.2f}m/s'\r\n            )\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Collision avoidance error: {str(e)}')\r\n\r\n    def _compute_safe_velocity(self, obstacle_distance):\r\n        \"\"\"\r\n        Compute safe forward velocity based on obstacle distance\r\n        Slows down as obstacles approach\r\n        \"\"\"\r\n        if obstacle_distance < self.DANGER_DISTANCE:\r\n            return 0.0  # Stop immediately\r\n\r\n        if obstacle_distance < self.WARNING_DISTANCE:\r\n            # Linear scale from 0 to max speed\r\n            fraction = (obstacle_distance - self.DANGER_DISTANCE) / \\\r\n                      (self.WARNING_DISTANCE - self.DANGER_DISTANCE)\r\n            return fraction * self.MAX_FORWARD_SPEED\r\n\r\n        return self.MAX_FORWARD_SPEED\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    avoidance = CollisionAvoidanceNode()\r\n    rclpy.spin(avoidance)\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-3-ethical-ai--bias-mitigation",children:"Part 3: Ethical AI & Bias Mitigation"}),"\n",(0,s.jsx)(n.h3,{id:"identifying-ai-bias-in-robot-systems",children:"Identifying AI Bias in Robot Systems"}),"\n",(0,s.jsx)(n.p,{children:"Robots inherit biases from training data. This is critical to address:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Bias Sources in Robot Perception:\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Bias Type           \u2502 Impact      \u2502 Mitigation          \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 Gender bias         \u2502 Fails on    \u2502 Balanced training   \u2502\r\n\u2502 (person detection)  \u2502 non-binary  \u2502 data, fairness test \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 Racial bias         \u2502 Lower      \u2502 Cross-racial        \u2502\r\n\u2502 (face recognition)  \u2502 accuracy   \u2502 validation set      \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 Age bias            \u2502 Fails on    \u2502 Include all age     \u2502\r\n\u2502 (activity detection)\u2502 children    \u2502 groups in training  \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 Environmental bias  \u2502 Fails      \u2502 Test in varied      \u2502\r\n\u2502 (lighting, context) \u2502 in novel    \u2502 environments        \u2502\r\n\u2502                     \u2502 settings    \u2502                     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h3,{id:"bias-detection--mitigation",children:"Bias Detection & Mitigation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nAI Bias Detection Framework\r\nIdentifies and mitigates biases in robot perception\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\n\r\nclass BiasDetectionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('bias_detection')\r\n\r\n        self.bridge = CvBridge()\r\n\r\n        # Fairness metrics\r\n        self.demographic_performance = {\r\n            'male': [],\r\n            'female': [],\r\n            'non-binary': [],\r\n            'caucasian': [],\r\n            'african': [],\r\n            'asian': [],\r\n            'hispanic': [],\r\n            'child': [],\r\n            'adult': [],\r\n            'elderly': []\r\n        }\r\n\r\n        # Subscribe to camera\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/color/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher for bias report\r\n        self.bias_report_pub = self.create_publisher(\r\n            String,\r\n            '/robot/bias_analysis',\r\n            10\r\n        )\r\n\r\n        self.get_logger().info('Bias Detection initialized')\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Analyze image for demographic representation\"\"\"\r\n        try:\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\r\n\r\n            # Run detection (simplified example)\r\n            detections = self._detect_people(cv_image)\r\n\r\n            # Analyze demographic distribution\r\n            for detection in detections:\r\n                demographics = self._classify_demographics(detection)\r\n                for demo_type, value in demographics.items():\r\n                    if demo_type in self.demographic_performance:\r\n                        self.demographic_performance[demo_type].append(value)\r\n\r\n            # Generate fairness report periodically\r\n            if len(self.demographic_performance['male']) % 100 == 0:\r\n                self._generate_fairness_report()\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Bias detection error: {str(e)}')\r\n\r\n    def _detect_people(self, image):\r\n        \"\"\"Placeholder for person detection\"\"\"\r\n        # In real system, use YOLO or similar\r\n        return []\r\n\r\n    def _classify_demographics(self, detection):\r\n        \"\"\"\r\n        Classify demographics of detected person\r\n        Returns dict of demographic attributes\r\n        \"\"\"\r\n        return {\r\n            'gender': 'unknown',  # Placeholder\r\n            'age_group': 'unknown',\r\n            'skin_tone': 'unknown'\r\n        }\r\n\r\n    def _generate_fairness_report(self):\r\n        \"\"\"Generate fairness analysis report\"\"\"\r\n        report = \"=== AI Fairness Report ===\\n\"\r\n\r\n        # Calculate performance per demographic\r\n        for demo_group in self.demographic_performance:\r\n            values = self.demographic_performance[demo_group]\r\n            if len(values) > 0:\r\n                accuracy = np.mean(values) if all(isinstance(v, (int, float))\r\n                                                  for v in values) else 0.0\r\n                count = len(values)\r\n\r\n                report += f\"{demo_group}: {accuracy:.1%} accuracy ({count} samples)\\n\"\r\n\r\n        # Flag disparities (>5% difference)\r\n        accuracies = {}\r\n        for demo_group in self.demographic_performance:\r\n            values = self.demographic_performance[demo_group]\r\n            if len(values) > 10:  # Require minimum samples\r\n                accuracies[demo_group] = np.mean(values)\r\n\r\n        if len(accuracies) > 1:\r\n            max_acc = max(accuracies.values())\r\n            min_acc = min(accuracies.values())\r\n            disparity = max_acc - min_acc\r\n\r\n            if disparity > 0.05:\r\n                report += f\"\\n\u26a0\ufe0f  WARNING: {disparity:.1%} accuracy disparity detected\\n\"\r\n                report += f\"Best: {max(accuracies, key=accuracies.get)}\\n\"\r\n                report += f\"Worst: {min(accuracies, key=accuracies.get)}\\n\"\r\n\r\n        self.get_logger().info(report)\r\n\r\n        # Publish report\r\n        report_msg = String()\r\n        report_msg.data = report\r\n        self.bias_report_pub.publish(report_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    bias_node = BiasDetectionNode()\r\n    rclpy.spin(bias_node)\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-4-transparent-ai-decision-making",children:"Part 4: Transparent AI Decision-Making"}),"\n",(0,s.jsx)(n.h3,{id:"explainability-for-robot-actions",children:"Explainability for Robot Actions"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nExplainable AI (XAI) for Robots\r\nMakes robot decisions transparent and interpretable\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport json\r\n\r\nclass ExplainableAINode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'explainable_ai\')\r\n\r\n        # Subscribe to robot decisions\r\n        self.decision_sub = self.create_subscription(\r\n            String,\r\n            \'/robot/decision\',\r\n            self.explain_decision,\r\n            10\r\n        )\r\n\r\n        # Publisher for explanations\r\n        self.explanation_pub = self.create_publisher(\r\n            String,\r\n            \'/robot/explanation\',\r\n            10\r\n        )\r\n\r\n        self.get_logger().info(\'Explainable AI initialized\')\r\n\r\n    def explain_decision(self, msg):\r\n        """\r\n        Generate human-readable explanation for robot decision\r\n        """\r\n        try:\r\n            decision = json.loads(msg.data)\r\n\r\n            explanation = self._generate_explanation(decision)\r\n\r\n            # Publish explanation\r\n            exp_msg = String()\r\n            exp_msg.data = explanation\r\n            self.explanation_pub.publish(exp_msg)\r\n\r\n            self.get_logger().info(f\'Explanation: {explanation}\')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'XAI error: {str(e)}\')\r\n\r\n    def _generate_explanation(self, decision):\r\n        """\r\n        Generate human-interpretable explanation for decision\r\n        """\r\n        action = decision.get(\'action\', \'unknown\')\r\n        confidence = decision.get(\'confidence\', 0.0)\r\n        reason = decision.get(\'reason\', \'\')\r\n        alternatives = decision.get(\'alternatives\', [])\r\n\r\n        explanation = f"""\r\nDecision: {action}\r\nConfidence: {confidence:.1%}\r\nReasoning: {reason}\r\n\r\nWhy not alternatives?\r\n"""\r\n\r\n        for alt in alternatives:\r\n            alt_name = alt.get(\'action\', \'unknown\')\r\n            alt_reason = alt.get(\'reason\', \'\')\r\n            explanation += f"- {alt_name}: {alt_reason}\\n"\r\n\r\n        explanation += f"""\r\nHow to override:\r\n1. Say "Stop" for emergency stop\r\n2. Say "Alternative: <action>" to request different action\r\n3. Hold red button on gripper for manual control\r\n        """\r\n\r\n        return explanation\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    xai_node = ExplainableAINode()\r\n    rclpy.spin(xai_node)\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-5-real-world-robot-failure-analysis",children:"Part 5: Real-World Robot Failure Analysis"}),"\n",(0,s.jsx)(n.h3,{id:"critical-incidents--lessons",children:"Critical Incidents & Lessons"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"CASE STUDY 1: Collaborative Robot Injury (2021)\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Incident: Worker's hand crushed by robot gripper\u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 Root Cause: Force sensor miscalibrated          \u2502\r\n\u2502             Safety thresholds not validated     \u2502\r\n\u2502                                                 \u2502\r\n\u2502 Lessons:                                         \u2502\r\n\u2502 \u2717 Sensor calibration missed                    \u2502\r\n\u2502 \u2717 Force limits not tested                      \u2502\r\n\u2502 \u2717 No emergency stop accessible                 \u2502\r\n\u2502 \u2717 Training incomplete for new workers          \u2502\r\n\u2502                                                 \u2502\r\n\u2502 Changes Made:                                   \u2502\r\n\u2502 \u2713 Daily sensor calibration check               \u2502\r\n\u2502 \u2713 Force validation in test environment         \u2502\r\n\u2502 \u2713 Emergency buttons on robot + pendant        \u2502\r\n\u2502 \u2713 Mandatory safety training quarterly         \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nCASE STUDY 2: Autonomous Vehicle Failure (2018)\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Incident: Pedestrian not detected at night      \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 Root Cause: Neural network trained on daytime  \u2502\r\n\u2502             images only. Dark-skin pedestrian  \u2502\r\n\u2502             not in training data.              \u2502\r\n\u2502                                                 \u2502\r\n\u2502 Lessons:                                         \u2502\r\n\u2502 \u2717 Unbalanced training dataset                  \u2502\r\n\u2502 \u2717 No testing in varied lighting                \u2502\r\n\u2502 \u2717 Demographic testing not performed           \u2502\r\n\u2502 \u2717 Confidence scores not calibrated            \u2502\r\n\u2502                                                 \u2502\r\n\u2502 Changes Made:                                   \u2502\r\n\u2502 \u2713 Night-time and mixed-lighting training      \u2502\r\n\u2502 \u2713 Diverse demographic representation          \u2502\r\n\u2502 \u2713 Cross-condition validation testing          \u2502\r\n\u2502 \u2713 Uncertainty quantification added            \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-6-production-safety-checklist",children:"Part 6: Production Safety Checklist"}),"\n",(0,s.jsx)(n.h3,{id:"pre-deployment-safety-audit",children:"Pre-Deployment Safety Audit"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nProduction Safety Checklist\r\nComprehensive audit before robot deployment\r\n\"\"\"\r\n\r\nSAFETY_CHECKLIST = {\r\n    'Hardware': {\r\n        'emergency_stop': {\r\n            'description': 'Emergency stop button functional',\r\n            'test': 'Press button, robot stops within 100ms',\r\n            'status': False\r\n        },\r\n        'force_sensors': {\r\n            'description': 'Force/torque sensors calibrated',\r\n            'test': 'Zero reading without load, verify linearity',\r\n            'status': False\r\n        },\r\n        'proximity_sensors': {\r\n            'description': 'Proximity sensors detect obstacles',\r\n            'test': 'Place object at 1m, 0.5m, 0.1m distances',\r\n            'status': False\r\n        },\r\n        'motor_encoders': {\r\n            'description': 'Joint encoders report accurate position',\r\n            'test': 'Move to known positions, verify readings',\r\n            'status': False\r\n        },\r\n    },\r\n\r\n    'Software': {\r\n        'safety_limits': {\r\n            'description': 'Velocity and force limits enforced',\r\n            'test': 'Command exceeding limits, verify rejection',\r\n            'status': False\r\n        },\r\n        'collision_detection': {\r\n            'description': 'Collision detection working',\r\n            'test': 'Apply 100N force, robot stops within 50ms',\r\n            'status': False\r\n        },\r\n        'emergency_response': {\r\n            'description': 'Emergency stop cuts motor power',\r\n            'test': 'Trigger emergency, motors de-energize',\r\n            'status': False\r\n        },\r\n        'sensor_validation': {\r\n            'description': 'All sensor inputs are validated',\r\n            'test': 'Check for sensor failure handling',\r\n            'status': False\r\n        },\r\n    },\r\n\r\n    'Perception': {\r\n        'object_detection': {\r\n            'description': 'Object detection >95% accuracy on test set',\r\n            'test': 'Run inference on 100 test images',\r\n            'status': False\r\n        },\r\n        'demographic_fairness': {\r\n            'description': 'Detection accuracy balanced across demographics',\r\n            'test': 'Accuracy difference &lt;5% across groups',\r\n            'status': False\r\n        },\r\n        'adversarial_robustness': {\r\n            'description': 'Perception robust to adversarial inputs',\r\n            'test': 'Add noise/rotation, verify degradation &lt;10%',\r\n            'status': False\r\n        },\r\n    },\r\n\r\n    'Planning': {\r\n        'trajectory_safety': {\r\n            'description': 'Planned trajectories avoid collisions',\r\n            'test': 'Generate 100 plans, verify none hit obstacles',\r\n            'status': False\r\n        },\r\n        'action_validation': {\r\n            'description': 'All planned actions are within robot limits',\r\n            'test': 'Check gripper force, reach, speed constraints',\r\n            'status': False\r\n        },\r\n    },\r\n\r\n    'HRI': {\r\n        'human_detection': {\r\n            'description': 'Robot detects human presence',\r\n            'test': 'Human in workspace, robot slows down',\r\n            'status': False\r\n        },\r\n        'speed_limits_human_present': {\r\n            'description': 'Speed &lt;0.3 m/s when humans nearby',\r\n            'test': 'Measure actual speed in human workspace',\r\n            'status': False\r\n        },\r\n        'verbal_feedback': {\r\n            'description': 'Robot announces actions to humans',\r\n            'test': 'Robot speaks: \"Moving to table\"',\r\n            'status': False\r\n        },\r\n    },\r\n\r\n    'Testing': {\r\n        'functional_testing': {\r\n            'description': '100 test cases passed',\r\n            'test': 'Run full regression test suite',\r\n            'status': False\r\n        },\r\n        'stress_testing': {\r\n            'description': 'Robot stable under 8-hour load',\r\n            'test': 'Continuous operation, monitor for failures',\r\n            'status': False\r\n        },\r\n        'failure_modes': {\r\n            'description': 'All known failure modes documented',\r\n            'test': 'Compare against ISO/TS 15066 requirements',\r\n            'status': False\r\n        },\r\n    },\r\n\r\n    'Documentation': {\r\n        'safety_manual': {\r\n            'description': 'Safety manual complete and reviewed',\r\n            'test': 'Safety officer approves all procedures',\r\n            'status': False\r\n        },\r\n        'maintenance_schedule': {\r\n            'description': 'Maintenance procedures documented',\r\n            'test': 'Monthly safety checks scheduled',\r\n            'status': False\r\n        },\r\n        'incident_response': {\r\n            'description': 'Incident response procedures documented',\r\n            'test': 'Safety team briefed on procedures',\r\n            'status': False\r\n        },\r\n    }\r\n}\r\n\r\ndef print_checklist():\r\n    \"\"\"Print safety checklist with current status\"\"\"\r\n    passed = 0\r\n    total = 0\r\n\r\n    for category, items in SAFETY_CHECKLIST.items():\r\n        print(f\"\\n{'='*60}\")\r\n        print(f\"Category: {category}\")\r\n        print(f\"{'='*60}\")\r\n\r\n        for item_name, item_data in items.items():\r\n            status = '\u2713' if item_data['status'] else '\u2717'\r\n            total += 1\r\n            if item_data['status']:\r\n                passed += 1\r\n\r\n            print(f\"\\n{status} {item_data['description']}\")\r\n            print(f\"  Test: {item_data['test']}\")\r\n\r\n    print(f\"\\n{'='*60}\")\r\n    print(f\"OVERALL: {passed}/{total} checks passed ({100*passed/total:.1f}%)\")\r\n    print(f\"{'='*60}\")\r\n\r\n    if passed == total:\r\n        print(\"\\n\u2713 ROBOT APPROVED FOR DEPLOYMENT\")\r\n    else:\r\n        print(f\"\\n\u2717 {total - passed} checks remaining before deployment\")\r\n\r\nif __name__ == '__main__':\r\n    print_checklist()\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-exercise-safety-critical-system-design",children:"Hands-On Exercise: Safety-Critical System Design"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-1-test-safety-limits",children:"Exercise 1: Test Safety Limits"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Terminal 1: Safety monitor\r\nros2 run robot_safety safety_monitor.py\r\n\r\n# Terminal 2: Test exceeding speed limit\r\nros2 topic pub /joint_states sensor_msgs/JointState \\\r\n  "header: {frame_id: base_link}\r\n   name: [joint1, joint2, joint3]\r\n   velocity: [1.0, 0.5, 0.8]"  # Exceeds 0.5 m/s limit\r\n\r\n# Expected: Safety monitor detects violation, publishes emergency stop\r\n# Verify: /safety/emergency_stop = True\n'})}),"\n",(0,s.jsx)(n.h3,{id:"exercise-2-collision-detection-test",children:"Exercise 2: Collision Detection Test"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Terminal 1: Collision avoidance\r\nros2 run robot_safety collision_avoidance.py\r\n\r\n# Terminal 2: Simulate obstacle approaching\r\n# Publish depth image with obstacle at 0.2m (danger zone)\r\nros2 topic pub /camera/depth/image_rect_raw \\\r\n  sensor_msgs/Image "header: {frame_id: camera}"\r\n\r\n# Expected: Robot velocity reduced to 0\r\n# Verify: /cmd_vel_safe.linear.x = 0\n'})}),"\n",(0,s.jsx)(n.h3,{id:"exercise-3-bias-detection-validation",children:"Exercise 3: Bias Detection Validation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create balanced test dataset\r\npython3 test_bias_detection.py \\\r\n  --test_images=1000 \\\r\n  --demographics_balanced=true\r\n\r\n# Expected output:\r\n# Male accuracy: 96.2%\r\n# Female accuracy: 95.8%\r\n# Non-binary accuracy: 94.1%\r\n# Disparity: 2.1% (acceptable &lt;5%)\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"common-hri--safety-errors",children:"Common HRI & Safety Errors"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Error"}),(0,s.jsx)(n.th,{children:"Cause"}),(0,s.jsx)(n.th,{children:"Fix"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:'"Force sensor shows 0N constantly"'})}),(0,s.jsx)(n.td,{children:"Sensor miscalibrated"}),(0,s.jsx)(n.td,{children:"Run zero calibration with no load"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:'"Robot doesn\'t stop on obstacle"'})}),(0,s.jsx)(n.td,{children:"Depth camera not connected"}),(0,s.jsx)(n.td,{children:"Check USB, verify /camera/depth/image_rect_raw topic"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:'"Safety limits ignored"'})}),(0,s.jsx)(n.td,{children:"No safety layer in motion control"}),(0,s.jsx)(n.td,{children:"Add SafetyMonitorNode before motor commands"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:'"Low accuracy on dark skin"'})}),(0,s.jsx)(n.td,{children:"Training data imbalance"}),(0,s.jsx)(n.td,{children:"Add dark-skinned people to training set, retrain"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:'"Bias detection not catching bias"'})}),(0,s.jsx)(n.td,{children:"Insufficient test samples"}),(0,s.jsx)(n.td,{children:"Collect 100+ samples per demographic group"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:'"Explanation too technical"'})}),(0,s.jsx)(n.td,{children:"Not human-readable"}),(0,s.jsx)(n.td,{children:"Simplify language, remove jargon"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Safety First, Features Second"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Safety is not a feature, it's a fundamental requirement"}),"\n",(0,s.jsx)(n.li,{children:"Redundant sensors + fail-safe mechanisms are essential"}),"\n",(0,s.jsx)(n.li,{children:"Regular audits prevent incidents before they happen"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Ethical AI Matters"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Biases in training data become biases in robot behavior"}),"\n",(0,s.jsx)(n.li,{children:"Demographic testing should be mandatory"}),"\n",(0,s.jsx)(n.li,{children:"Explainability builds user trust"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Real-Time Monitoring is Critical"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Collision detection must run at 20+ Hz"}),"\n",(0,s.jsx)(n.li,{children:"Force limits prevent serious injuries"}),"\n",(0,s.jsx)(n.li,{children:"Emergency stops must be fail-safe (de-energize, not stop)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Standards Exist for a Reason"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"ISO/TS 15066 specifies safe force levels"}),"\n",(0,s.jsx)(n.li,{children:"ISO 13849 defines safety architecture"}),"\n",(0,s.jsx)(n.li,{children:"Follow standards, don't create your own"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Humans + Robots = New Safety Domain"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Collaborative robots need different controls than industrial robots"}),"\n",(0,s.jsx)(n.li,{children:"Speed must reduce when humans are nearby"}),"\n",(0,s.jsx)(n.li,{children:"Transparency in decision-making builds acceptance"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsx)(n.h3,{id:"safety-standards--guidelines",children:"Safety Standards & Guidelines"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.iso.org/standard/62996.html",children:"ISO/TS 15066: Collaborative Robots Safety"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.ansi.org/",children:"ANSI/RIA R15.06: Industrial Robot Safety"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.iso.org/standard/54646.html",children:"ISO 13849-1: Safety Control Systems"})}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ethics-in-robotics",children:"Ethics in Robotics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://standards.ieee.org/standard/7001-2018.html",children:"IEEE Standards for Ethical AI"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1811.02159",children:"Algorithmic Bias in Computer Vision"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://fairmlbook.org/",children:"Fairness in Machine Learning"})}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"real-world-incidents",children:"Real-World Incidents"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.nhtsa.gov/",children:"NHTSA Tesla Autopilot Investigation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1801.08289",children:"Amazon Rekognition Bias Study"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.nist.gov/robotics/",children:"Robot Safety Case Studies"})}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"capstone-project-end-to-end-safe-robot-system",children:"Capstone Project: End-to-End Safe Robot System"}),"\n",(0,s.jsx)(n.h3,{id:"complete-system-architecture",children:"Complete System Architecture"}),"\n",(0,s.jsx)(n.p,{children:"You now have all components to build a complete, safe conversational robot:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                    HUMANOID ROBOT SYSTEM                     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                                                              \u2502\r\n\u2502  CHAPTER 1: Foundation (ROS 2)                              \u2502\r\n\u2502  \u2514\u2500 Robot description, packages, communication              \u2502\r\n\u2502                                                              \u2502\r\n\u2502  CHAPTER 2: Perception (Gazebo + Sensors)                  \u2502\r\n\u2502  \u2514\u2500 Physics simulation, camera/LiDAR/IMU integration       \u2502\r\n\u2502                                                              \u2502\r\n\u2502  CHAPTER 3: Navigation (Isaac Sim + SLAM + Vision)        \u2502\r\n\u2502  \u2514\u2500 Photorealistic simulation, SLAM mapping, object detect  \u2502\r\n\u2502                                                              \u2502\r\n\u2502  CHAPTER 4: Intelligence (LLM + VLA + Safety)              \u2502\r\n\u2502  \u251c\u2500 Natural language understanding (Lesson 4.1)            \u2502\r\n\u2502  \u251c\u2500 Multimodal perception + conversation (Lesson 4.2)      \u2502\r\n\u2502  \u2514\u2500 Safety + Ethics + HRI (Lesson 4.3) \u2190 YOU ARE HERE     \u2502\r\n\u2502                                                              \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 FINAL SYSTEM: Safe, Intelligent, Conversational Robot       \u2502\r\n\u2502                                                              \u2502\r\n\u2502 Input: Natural language commands from humans                \u2502\r\n\u2502 Process: Understand intent \u2192 Perceive environment          \u2502\r\n\u2502         \u2192 Check safety \u2192 Plan action \u2192 Execute             \u2502\r\n\u2502 Output: Robot performs task safely while explaining itself  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h3,{id:"capstone-implementation",children:"Capstone Implementation"}),"\n",(0,s.jsx)(n.p,{children:"Create a complete safe robot system integrating all 4 chapters:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nCAPSTONE: Complete Safe Humanoid Robot System\r\nIntegration of all 13 lessons from the course\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\nimport json\r\n\r\nclass CapstoneRobotSystem(Node):\r\n    def __init__(self):\r\n        super().__init__('capstone_robot')\r\n\r\n        # Subscriptions from all components\r\n        self.speech_sub = self.create_subscription(\r\n            String, '/speech/transcription', self.on_speech, 10)\r\n        self.vision_sub = self.create_subscription(\r\n            String, '/robot/answer', self.on_vision, 10)\r\n        self.safety_sub = self.create_subscription(\r\n            String, '/safety/status_ok', self.on_safety, 10)\r\n\r\n        # Publishers to all components\r\n        self.query_pub = self.create_publisher(String, '/robot/query', 10)\r\n        self.plan_pub = self.create_publisher(String, '/robot/plan', 10)\r\n        self.response_pub = self.create_publisher(String, '/robot/response', 10)\r\n        self.vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n\r\n        self.get_logger().info(\r\n            'Capstone Robot System initialized.\\n'\r\n            'This robot successfully integrates:\\n'\r\n            '\u2713 ROS 2 Foundation (Ch 1)\\n'\r\n            '\u2713 Physics Simulation (Ch 2)\\n'\r\n            '\u2713 Navigation & Perception (Ch 3)\\n'\r\n            '\u2713 Language & Safety (Ch 4)\\n'\r\n            'Ready for deployment!'\r\n        )\r\n\r\n    def on_speech(self, msg):\r\n        \"\"\"User speaks to robot\"\"\"\r\n        self.get_logger().info(f'Human: {msg.data}')\r\n        # Rest of implementation uses Lessons 4.1-4.3\r\n\r\nif __name__ == '__main__':\r\n    rclpy.init()\r\n    system = CapstoneRobotSystem()\r\n    rclpy.spin(system)\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"course-summary",children:"Course Summary"}),"\n",(0,s.jsxs)(n.p,{children:["You've completed the ",(0,s.jsx)(n.strong,{children:"13-week Physical AI & Humanoid Robotics course"}),". You now understand:"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Week 1-4 (Ch 1)"}),": ROS 2 ecosystem and robot communication\r\n",(0,s.jsx)(n.strong,{children:"Week 5-8 (Ch 2)"}),": Physics simulation and digital twins\r\n",(0,s.jsx)(n.strong,{children:"Week 9-11 (Ch 3)"}),": Perception, navigation, and computer vision\r\n",(0,s.jsx)(n.strong,{children:"Week 12-13 (Ch 4)"}),": Large language models, multimodal AI, and safety"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"You can now:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Design and build complete robot systems"}),"\n",(0,s.jsx)(n.li,{children:"Integrate perception, planning, and control"}),"\n",(0,s.jsx)(n.li,{children:"Deploy AI safely in human environments"}),"\n",(0,s.jsx)(n.li,{children:"Debug and optimize robot systems"}),"\n",(0,s.jsx)(n.li,{children:"Understand the ethical implications of robot AI"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Next Steps:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Deploy on real hardware (Jetson Orin Nano humanoid)"}),"\n",(0,s.jsx)(n.li,{children:"Collect real-world data to improve perception"}),"\n",(0,s.jsx)(n.li,{children:"Fine-tune language models on robot-specific tasks"}),"\n",(0,s.jsx)(n.li,{children:"Contribute to open-source robotics projects"}),"\n",(0,s.jsx)(n.li,{children:"Consider graduate studies in robotics AI"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The most important lesson"}),": ",(0,s.jsx)(n.em,{children:"Always prioritize human safety over robot capabilities."})]}),"\n",(0,s.jsx)(n.p,{children:"Congratulations on completing the course! \ud83c\udf93"})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);