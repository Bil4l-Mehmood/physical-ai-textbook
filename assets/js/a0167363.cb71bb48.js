"use strict";(self.webpackChunkphysical_ai_textbook=self.webpackChunkphysical_ai_textbook||[]).push([[706],{3509:function(e,n,i){i.r(n),i.d(n,{assets:function(){return l},contentTitle:function(){return o},default:function(){return p},frontMatter:function(){return a},metadata:function(){return t},toc:function(){return c}});var t=JSON.parse('{"id":"chapter-4/4-2-conversational-ai","title":"Conversational AI & Vision-Language-Action: Multimodal Robot Intelligence","description":"Integrate multimodal AI models to create robots that see, listen, understand, and act on natural language commands","source":"@site/docs/chapter-4/4-2-conversational-ai.md","sourceDirName":"chapter-4","slug":"/chapter-4/4-2-conversational-ai","permalink":"/physical-ai-textbook/docs/chapter-4/4-2-conversational-ai","draft":false,"unlisted":false,"editUrl":"https://github.com/Bil4l-Mehmood/physical-ai-textbook/edit/main/docs/chapter-4/4-2-conversational-ai.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"Lesson 4.2: Conversational AI & VLA","title":"Conversational AI & Vision-Language-Action: Multimodal Robot Intelligence","description":"Integrate multimodal AI models to create robots that see, listen, understand, and act on natural language commands","duration":120,"difficulty":"Advanced","hardware":["Jetson Orin Nano 8GB","RealSpeaker USB Mic Array","RealSense D435i","ROS 2 Humble"],"prerequisites":["Lesson 4.1: LLM Integration for Robotics"]},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.1: LLM Integration","permalink":"/physical-ai-textbook/docs/chapter-4/4-1-llm-brain"},"next":{"title":"Lesson 4.3: HRI & Safety","permalink":"/physical-ai-textbook/docs/chapter-4/4-3-hri-safety"}}'),r=i(4848),s=i(8453);const a={sidebar_position:2,sidebar_label:"Lesson 4.2: Conversational AI & VLA",title:"Conversational AI & Vision-Language-Action: Multimodal Robot Intelligence",description:"Integrate multimodal AI models to create robots that see, listen, understand, and act on natural language commands",duration:120,difficulty:"Advanced",hardware:["Jetson Orin Nano 8GB","RealSpeaker USB Mic Array","RealSense D435i","ROS 2 Humble"],prerequisites:["Lesson 4.1: LLM Integration for Robotics"]},o="Lesson 4.2: Conversational AI & Vision-Language-Action - Multimodal Robot Intelligence",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2},{value:"RealSpeaker USB Microphone Array",id:"realspeaker-usb-microphone-array",level:3},{value:"Complete Hardware Stack for VLA",id:"complete-hardware-stack-for-vla",level:3},{value:"Part 1: Vision-Language Models (VLMs) Fundamentals",id:"part-1-vision-language-models-vlms-fundamentals",level:2},{value:"What VLMs Can Do",id:"what-vlms-can-do",level:3},{value:"VLM Architecture Comparison",id:"vlm-architecture-comparison",level:3},{value:"Key VLM Capabilities for Robotics",id:"key-vlm-capabilities-for-robotics",level:3},{value:"Part 2: RealSpeaker Integration for Robot Hearing",id:"part-2-realspeaker-integration-for-robot-hearing",level:2},{value:"Installing RealSpeaker Drivers",id:"installing-realspeaker-drivers",level:3},{value:"RealSpeaker ROS 2 Node",id:"realspeaker-ros-2-node",level:3},{value:"Part 3: Speech-to-Text Pipeline",id:"part-3-speech-to-text-pipeline",level:2},{value:"Cloud-Based STT (Google Cloud Speech-to-Text)",id:"cloud-based-stt-google-cloud-speech-to-text",level:3},{value:"Part 4: Vision-Language Model Integration",id:"part-4-vision-language-model-integration",level:2},{value:"LLaVA (Open-Source VLM) on Jetson",id:"llava-open-source-vlm-on-jetson",level:3},{value:"Part 5: Text-to-Speech Output",id:"part-5-text-to-speech-output",level:2},{value:"TTS Implementation",id:"tts-implementation",level:3},{value:"Part 6: Complete VLA Agent Pipeline",id:"part-6-complete-vla-agent-pipeline",level:2},{value:"End-to-End Conversational Robot",id:"end-to-end-conversational-robot",level:3},{value:"Hands-On Exercise: Build a Conversational Robot",id:"hands-on-exercise-build-a-conversational-robot",level:2},{value:"Exercise 1: Voice-Controlled Navigation",id:"exercise-1-voice-controlled-navigation",level:3},{value:"Exercise 2: Visual Question Answering",id:"exercise-2-visual-question-answering",level:3},{value:"Exercise 3: Multi-Step Conversational Task",id:"exercise-3-multi-step-conversational-task",level:3},{value:"Common VLA Errors &amp; Debugging",id:"common-vla-errors--debugging",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Vision-Language Model Papers",id:"vision-language-model-papers",level:3},{value:"Robot Conversation &amp; HRI",id:"robot-conversation--hri",level:3},{value:"Implementation Resources",id:"implementation-resources",level:3},{value:"Next Lesson",id:"next-lesson",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"lesson-42-conversational-ai--vision-language-action---multimodal-robot-intelligence",children:"Lesson 4.2: Conversational AI & Vision-Language-Action - Multimodal Robot Intelligence"})}),"\n",(0,r.jsxs)(n.admonition,{title:"Lesson Overview",type:"info",children:[(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duration"}),": 120 minutes | ",(0,r.jsx)(n.strong,{children:"Difficulty"}),": Advanced | ",(0,r.jsx)(n.strong,{children:"Hardware"}),": Jetson + microphone + camera"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Prerequisites"}),": Lesson 4.1 (LLM integration), Chapter 3 (perception)"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Learning Outcome"}),": Build robots that perceive their environment visually, understand spoken language, reason about multimodal inputs, and execute appropriate actions through a unified VLA (Vision-Language-Action) pipeline"]})]}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand vision-language models (VLMs) and multimodal learning"}),"\n",(0,r.jsx)(n.li,{children:"Compare VLM architectures (LLaVA, GPT-4V, Gemini Vision, Claude Vision)"}),"\n",(0,r.jsx)(n.li,{children:"Process images with language models for scene understanding"}),"\n",(0,r.jsx)(n.li,{children:"Integrate RealSpeaker USB microphone array for speech capture"}),"\n",(0,r.jsx)(n.li,{children:"Implement speech-to-text and text-to-speech pipelines"}),"\n",(0,r.jsx)(n.li,{children:"Build VLA agents combining perception, reasoning, and action"}),"\n",(0,r.jsx)(n.li,{children:"Create end-to-end conversational robot systems"}),"\n",(0,r.jsx)(n.li,{children:"Handle multimodal input errors and edge cases"}),"\n",(0,r.jsx)(n.li,{children:"Optimize latency for real-time interaction"}),"\n",(0,r.jsx)(n.li,{children:"Deploy production conversational robots on edge devices"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,r.jsxs)(n.admonition,{title:"Multimodal Perception Stack",type:"note",children:[(0,r.jsx)(n.p,{children:"True robot intelligence requires simultaneous processing of:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision"}),": Real-time camera feeds (RGB-D)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audio"}),": Speech capture with directional beamforming"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Language"}),": Natural language understanding with reasoning"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action"}),": Coordinated robot movement and manipulation"]}),"\n"]})]}),"\n",(0,r.jsx)(n.h3,{id:"realspeaker-usb-microphone-array",children:"RealSpeaker USB Microphone Array"}),"\n",(0,r.jsx)(n.p,{children:"The RealSpeaker Mic Array v2.0 is the standard for robot voice input:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Component"}),(0,r.jsx)(n.th,{children:"Specification"}),(0,r.jsx)(n.th,{children:"Impact"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Mic Count"})}),(0,r.jsx)(n.td,{children:"6 \xd7 MEMS microphones"}),(0,r.jsx)(n.td,{children:"Beamforming for speech direction"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Frequency"})}),(0,r.jsx)(n.td,{children:"20Hz - 20kHz"}),(0,r.jsx)(n.td,{children:"Full human speech spectrum"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"SNR"})}),(0,r.jsx)(n.td,{children:">60dB"}),(0,r.jsx)(n.td,{children:"Noise suppression capability"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Beam Angle"})}),(0,r.jsx)(n.td,{children:"\xb145\xb0"}),(0,r.jsx)(n.td,{children:"Pick up speech from front arc"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"USB"})}),(0,r.jsx)(n.td,{children:"2.0 (480 Mbps)"}),(0,r.jsx)(n.td,{children:"Works with all Jetson models"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Power"})}),(0,r.jsx)(n.td,{children:"100mA @ 5V"}),(0,r.jsx)(n.td,{children:"Powered via USB, no separate supply"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Latency"})}),(0,r.jsx)(n.td,{children:"<50ms"}),(0,r.jsx)(n.td,{children:"Minimal delay for real-time interaction"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"complete-hardware-stack-for-vla",children:"Complete Hardware Stack for VLA"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Robot Hardware Stack:\n\u251c\u2500 Camera (RealSense D435i)\n\u2502  \u251c\u2500 RGB input: 640\xd7480 @ 30Hz\n\u2502  \u251c\u2500 Depth input: 640\xd7480 @ 30Hz\n\u2502  \u2514\u2500 Output: /camera/color/image_raw, /camera/depth/image_rect_raw\n\u251c\u2500 Microphone (RealSpeaker Mic Array v2)\n\u2502  \u251c\u2500 Audio capture: 48kHz, 16-bit\n\u2502  \u2514\u2500 Output: /audio/raw (raw PCM audio)\n\u251c\u2500 Processor (Jetson Orin Nano)\n\u2502  \u251c\u2500 Vision processing: 8 CUDA cores\n\u2502  \u251c\u2500 Language processing: Local or cloud LLM\n\u2502  \u2514\u2500 Speech processing: Local or cloud TTS/STT\n\u2514\u2500 Actuators (Robot-specific)\n   \u251c\u2500 Motors: /motor/[left|right]_wheel, /motor/gripper\n   \u2514\u2500 Feedback: /encoder/[left|right]_wheel, /gripper/force\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-1-vision-language-models-vlms-fundamentals",children:"Part 1: Vision-Language Models (VLMs) Fundamentals"}),"\n",(0,r.jsx)(n.h3,{id:"what-vlms-can-do",children:"What VLMs Can Do"}),"\n",(0,r.jsx)(n.p,{children:"VLMs are neural networks trained on billions of image-text pairs, understanding the relationship between visual content and language:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'Traditional AI:\nImage \u2192 Detector \u2192 "person"\n        \u2192 Classifier \u2192 "sitting"\n        \u2192 Analyzer \u2192 "on chair"\nUser asks: "What is the person doing?"\nRobot: "Can\'t directly answer from detection outputs"\n\nVision-Language Model:\nImage + Question: "What is the person doing?"\n          \u2193\n        [VLM]\n          \u2193\nOutput: "The person is sitting on a chair and reading a book"\n(Generated from visual understanding + language)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"vlm-architecture-comparison",children:"VLM Architecture Comparison"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Provider"}),(0,r.jsx)(n.th,{children:"Size"}),(0,r.jsx)(n.th,{children:"Latency"}),(0,r.jsx)(n.th,{children:"Accuracy"}),(0,r.jsx)(n.th,{children:"Cost"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"LLaVA 1.5"})}),(0,r.jsx)(n.td,{children:"Open source"}),(0,r.jsx)(n.td,{children:"7B"}),(0,r.jsx)(n.td,{children:"500-1000ms"}),(0,r.jsx)(n.td,{children:"85%"}),(0,r.jsx)(n.td,{children:"Free (local)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"GPT-4V"})}),(0,r.jsx)(n.td,{children:"OpenAI"}),(0,r.jsx)(n.td,{children:"Proprietary"}),(0,r.jsx)(n.td,{children:"800-2000ms"}),(0,r.jsx)(n.td,{children:"95%"}),(0,r.jsx)(n.td,{children:"$0.01/image"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Gemini Pro Vision"})}),(0,r.jsx)(n.td,{children:"Google"}),(0,r.jsx)(n.td,{children:"Proprietary"}),(0,r.jsx)(n.td,{children:"600-1500ms"}),(0,r.jsx)(n.td,{children:"92%"}),(0,r.jsx)(n.td,{children:"Free tier limited"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Claude 3 Vision"})}),(0,r.jsx)(n.td,{children:"Anthropic"}),(0,r.jsx)(n.td,{children:"Proprietary"}),(0,r.jsx)(n.td,{children:"700-1500ms"}),(0,r.jsx)(n.td,{children:"94%"}),(0,r.jsx)(n.td,{children:"$0.003/image"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"key-vlm-capabilities-for-robotics",children:"Key VLM Capabilities for Robotics"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Visual Question Answering (VQA)"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'Image: [robot camera feed]\nQuestion: "What objects can I grasp?"\nVLM Output: "I see a red cube, blue cylinder, and yellow ball.\n            The red cube appears graspable. The blue cylinder is\n            too large. The yellow ball is on the shelf (unreachable)."\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Scene Understanding"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'Image: [robot view of room]\nQuestion: "Is the path to the door clear?"\nVLM Output: "The path has a chair at coordinates (2.5m, 1.2m)\n            blocking direct access. Recommend going around the left side."\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Spatial Reasoning"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'Image: [table with objects]\nQuestion: "What\'s to the left of the blue cube?"\nVLM Output: "A red sphere and a green pen are positioned to the\n            left of the blue cube."\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-2-realspeaker-integration-for-robot-hearing",children:"Part 2: RealSpeaker Integration for Robot Hearing"}),"\n",(0,r.jsx)(n.h3,{id:"installing-realspeaker-drivers",children:"Installing RealSpeaker Drivers"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Clone RealSpeaker repository\ngit clone https://github.com/respeaker/respeaker_ros.git\ncd respeaker_ros\n\n# Install dependencies\nsudo apt-get install -y libsndfile1-dev alsa-utils pulseaudio\n\n# Build ROS 2 package\nsource /opt/ros/humble/setup.bash\ncolcon build --symlink-install\n\n# Test microphone\narecord -D plughw:CARD=seeed2micvoicec,DEV=0 -r 16000 -f S16_LE test.wav\n# If successful, you'll hear audio recorded\n"})}),"\n",(0,r.jsx)(n.h3,{id:"realspeaker-ros-2-node",children:"RealSpeaker ROS 2 Node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nRealSpeaker Microphone Integration for ROS 2\nCaptures multi-channel audio and publishes as ROS messages\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport pyaudio\nimport numpy as np\nfrom collections import deque\n\nclass RealSpeakerNode(Node):\n    def __init__(self):\n        super().__init__('respeaker_node')\n\n        # Audio parameters\n        self.CHUNK = 1024  # Samples per frame\n        self.FORMAT = pyaudio.paInt16\n        self.CHANNELS = 6  # RealSpeaker has 6 microphones\n        self.RATE = 16000  # 16 kHz sampling rate\n\n        # Initialize PyAudio\n        self.audio = pyaudio.PyAudio()\n\n        # Open audio stream from RealSpeaker\n        try:\n            self.stream = self.audio.open(\n                format=self.FORMAT,\n                channels=self.CHANNELS,\n                rate=self.RATE,\n                input=True,\n                input_device_index=self._find_respeaker_device(),\n                frames_per_buffer=self.CHUNK,\n                exceptions=False\n            )\n        except Exception as e:\n            self.get_logger().error(f'Failed to open RealSpeaker: {str(e)}')\n            return\n\n        # Publishers\n        self.raw_audio_pub = self.create_publisher(\n            AudioData,\n            '/respeaker/raw_audio',\n            10\n        )\n\n        self.direction_pub = self.create_publisher(\n            String,\n            '/respeaker/sound_direction',\n            10\n        )\n\n        # Sound direction buffer (for beamforming analysis)\n        self.direction_history = deque(maxlen=10)\n\n        # Timer for audio capture\n        self.create_timer(\n            self.CHUNK / self.RATE,  # Timer period = one frame\n            self.capture_audio_callback\n        )\n\n        self.get_logger().info('RealSpeaker initialized (6-channel capture)')\n\n    def _find_respeaker_device(self):\n        \"\"\"Find RealSpeaker device index\"\"\"\n        for i in range(self.audio.get_device_count()):\n            info = self.audio.get_device_info_by_index(i)\n            if 'seeed' in info['name'].lower() or 'respeaker' in info['name'].lower():\n                self.get_logger().info(f'Found RealSpeaker at index {i}')\n                return i\n\n        self.get_logger().warn(\n            'RealSpeaker not found, using default input device'\n        )\n        return None\n\n    def capture_audio_callback(self):\n        \"\"\"Capture audio frame from RealSpeaker\"\"\"\n        try:\n            # Read audio frame\n            data = self.stream.read(self.CHUNK, exception_on_overflow=False)\n\n            # Convert to numpy array\n            audio_data = np.frombuffer(data, dtype=np.int16)\n            audio_data = audio_data.reshape(self.CHUNK, self.CHANNELS)\n\n            # Publish raw audio\n            audio_msg = AudioData()\n            audio_msg.data = audio_data.tobytes()\n            self.raw_audio_pub.publish(audio_msg)\n\n            # Estimate sound direction using cross-correlation\n            direction = self._estimate_direction(audio_data)\n            if direction:\n                direction_msg = String()\n                direction_msg.data = direction\n                self.direction_pub.publish(direction_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Audio capture error: {str(e)}')\n\n    def _estimate_direction(self, audio_data):\n        \"\"\"\n        Estimate sound direction using time-difference-of-arrival (TDOA)\n        RealSpeaker mics are arranged in circular pattern\n        \"\"\"\n        # Calculate energy in each frequency band\n        energy = np.mean(np.abs(audio_data), axis=0)\n\n        # Find channel with highest energy (rough direction estimate)\n        max_channel = np.argmax(energy)\n\n        # Map channel to direction (RealSpeaker mic layout)\n        directions = [\n            'front',      # Channel 0\n            'front-left', # Channel 1\n            'left',       # Channel 2\n            'rear-left',  # Channel 3\n            'rear-right', # Channel 4\n            'right'       # Channel 5\n        ]\n\n        direction = directions[max_channel]\n        self.direction_history.append(direction)\n\n        # Return majority direction from last 10 frames\n        from collections import Counter\n        if len(self.direction_history) >= 5:\n            majority_direction = Counter(self.direction_history).most_common(1)[0][0]\n            return majority_direction\n\n        return None\n\n    def __del__(self):\n        if hasattr(self, 'stream'):\n            self.stream.stop_stream()\n            self.stream.close()\n        if hasattr(self, 'audio'):\n            self.audio.terminate()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = RealSpeakerNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-3-speech-to-text-pipeline",children:"Part 3: Speech-to-Text Pipeline"}),"\n",(0,r.jsx)(n.h3,{id:"cloud-based-stt-google-cloud-speech-to-text",children:"Cloud-Based STT (Google Cloud Speech-to-Text)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nSpeech-to-Text using Google Cloud Speech-to-Text API\nReal-time streaming transcription from RealSpeaker\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport google.cloud.speech as speech\nimport queue\n\nclass SpeechToTextNode(Node):\n    def __init__(self):\n        super().__init__('speech_to_text')\n\n        # Initialize Google Speech-to-Text client\n        self.client = speech.SpeechClient()\n        self.config = speech.RecognitionConfig(\n            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n            sample_rate_hertz=16000,\n            language_code='en-US',\n            model='latest_long',\n            use_enhanced=True,\n            enable_automatic_punctuation=True\n        )\n\n        # Audio queue for streaming\n        self.audio_queue = queue.Queue()\n\n        # Subscribe to audio\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            '/respeaker/raw_audio',\n            self.audio_callback,\n            10\n        )\n\n        # Publish transcription\n        self.text_pub = self.create_publisher(\n            String,\n            '/speech/transcription',\n            10\n        )\n\n        self.confidence_pub = self.create_publisher(\n            String,\n            '/speech/confidence',\n            10\n        )\n\n        # Start streaming recognition in background thread\n        import threading\n        self.streaming_thread = threading.Thread(\n            target=self._streaming_recognize_loop,\n            daemon=True\n        )\n        self.streaming_thread.start()\n\n        self.get_logger().info('Speech-to-Text initialized')\n\n    def audio_callback(self, msg):\n        \"\"\"Queue incoming audio for STT processing\"\"\"\n        self.audio_queue.put(msg.data)\n\n    def _streaming_recognize_loop(self):\n        \"\"\"Continuous streaming speech recognition\"\"\"\n        def request_generator():\n            while True:\n                try:\n                    # Get audio from queue with timeout\n                    audio_content = self.audio_queue.get(timeout=1.0)\n\n                    yield speech.StreamingRecognizeRequest(\n                        audio_content=audio_content\n                    )\n                except queue.Empty:\n                    continue\n                except Exception as e:\n                    self.get_logger().error(f'STT error: {str(e)}')\n                    break\n\n        try:\n            # Start streaming request\n            requests = request_generator()\n            responses = self.client.streaming_recognize(\n                self.config,\n                requests\n            )\n\n            for response in responses:\n                if not response.results:\n                    continue\n\n                result = response.results[0]\n\n                if result.is_final:\n                    # Final transcription\n                    transcript = result.alternatives[0].transcript\n                    confidence = result.alternatives[0].confidence\n\n                    # Publish transcription\n                    text_msg = String()\n                    text_msg.data = transcript\n                    self.text_pub.publish(text_msg)\n\n                    # Publish confidence score\n                    conf_msg = String()\n                    conf_msg.data = f'{confidence:.2%}'\n                    self.confidence_pub.publish(conf_msg)\n\n                    self.get_logger().info(\n                        f'Speech: \"{transcript}\" ({confidence:.2%})'\n                    )\n                else:\n                    # Interim (partial) result\n                    interim = result.alternatives[0].transcript\n                    self.get_logger().debug(f'Interim: \"{interim}\"')\n\n        except Exception as e:\n            self.get_logger().error(f'Streaming recognition failed: {str(e)}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SpeechToTextNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-4-vision-language-model-integration",children:"Part 4: Vision-Language Model Integration"}),"\n",(0,r.jsx)(n.h3,{id:"llava-open-source-vlm-on-jetson",children:"LLaVA (Open-Source VLM) on Jetson"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nVision-Language Model (LLaVA) Integration for ROS 2\nProcesses camera images with language reasoning\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport cv2\nfrom PIL import Image as PILImage\nimport torch\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\nimport time\n\nclass LLaVAVisionNode(Node):\n    def __init__(self):\n        super().__init__('llava_vision')\n\n        self.bridge = CvBridge()\n\n        # Load LLaVA model (lightweight 7B version for Jetson)\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.get_logger().info(f'Using device: {self.device}')\n\n        try:\n            model_id = 'llava-hf/llava-1.5-7b-hf'\n            self.processor = AutoProcessor.from_pretrained(model_id)\n            self.model = LlavaForConditionalGeneration.from_pretrained(\n                model_id,\n                torch_dtype=torch.float16,\n                device_map='auto'\n            )\n            self.get_logger().info(f'LLaVA model loaded')\n        except Exception as e:\n            self.get_logger().error(f'Failed to load LLaVA: {str(e)}')\n            return\n\n        # Subscribe to camera and text input\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/color/image_raw',\n            self.image_callback,\n            1  # Single queuing for latest frame\n        )\n\n        self.query_sub = self.create_subscription(\n            String,\n            '/robot/query',\n            self.query_callback,\n            10\n        )\n\n        # Publishers\n        self.answer_pub = self.create_publisher(\n            String,\n            '/robot/answer',\n            10\n        )\n\n        self.latency_pub = self.create_publisher(\n            String,\n            '/robot/inference_latency',\n            10\n        )\n\n        # Store latest image\n        self.latest_image = None\n        self.latest_query = None\n\n    def image_callback(self, msg):\n        \"\"\"Store latest camera image\"\"\"\n        try:\n            self.latest_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n        except Exception as e:\n            self.get_logger().error(f'Image conversion error: {str(e)}')\n\n    def query_callback(self, msg):\n        \"\"\"Process VLM query when image is available\"\"\"\n        if self.latest_image is None:\n            self.get_logger().warn('No image available yet')\n            return\n\n        query = msg.data\n        self.get_logger().info(f'VLM Query: \"{query}\"')\n\n        try:\n            # Convert OpenCV image to PIL\n            pil_image = PILImage.fromarray(\n                cv2.cvtColor(self.latest_image, cv2.COLOR_BGR2RGB)\n            )\n\n            # Prepare input for VLM\n            prompt = f'<image>\\n{query}'\n\n            start_time = time.time()\n\n            inputs = self.processor(\n                text=prompt,\n                images=pil_image,\n                return_tensors='pt'\n            ).to(self.device)\n\n            # Generate answer\n            with torch.no_grad():\n                output = self.model.generate(\n                    **inputs,\n                    max_new_tokens=200,\n                    do_sample=False\n                )\n\n            answer = self.processor.decode(\n                output[0],\n                skip_special_tokens=True\n            )\n\n            latency = (time.time() - start_time) * 1000\n\n            # Extract answer from response\n            answer = answer.split('Assistant:')[-1].strip()\n\n            # Publish answer\n            answer_msg = String()\n            answer_msg.data = answer\n            self.answer_pub.publish(answer_msg)\n\n            # Publish latency\n            latency_msg = String()\n            latency_msg.data = f'{latency:.1f}ms'\n            self.latency_pub.publish(latency_msg)\n\n            self.get_logger().info(\n                f'Answer: \"{answer}\" (Latency: {latency:.1f}ms)'\n            )\n\n        except Exception as e:\n            self.get_logger().error(f'VLM processing error: {str(e)}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLaVAVisionNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-5-text-to-speech-output",children:"Part 5: Text-to-Speech Output"}),"\n",(0,r.jsx)(n.h3,{id:"tts-implementation",children:"TTS Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nText-to-Speech for Robot Responses\nConverts planned actions back to natural language\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport pyttsx3\nimport threading\n\nclass TextToSpeechNode(Node):\n    def __init__(self):\n        super().__init__('text_to_speech')\n\n        # Initialize TTS engine\n        self.engine = pyttsx3.init()\n        self.engine.setProperty('rate', 150)  # Speech rate (words/min)\n        self.engine.setProperty('volume', 0.9)  # Volume 0-1\n\n        # Set voice (prefer female voice if available)\n        voices = self.engine.getProperty('voices')\n        if len(voices) > 1:\n            self.engine.setProperty('voice', voices[1].id)\n\n        # Subscribe to text to speak\n        self.tts_sub = self.create_subscription(\n            String,\n            '/robot/response',\n            self.tts_callback,\n            10\n        )\n\n        self.get_logger().info('Text-to-Speech initialized')\n\n    def tts_callback(self, msg):\n        \"\"\"Speak response in background thread\"\"\"\n        text = msg.data\n\n        # Run TTS in background to not block ROS 2\n        tts_thread = threading.Thread(\n            target=self._speak,\n            args=(text,),\n            daemon=True\n        )\n        tts_thread.start()\n\n    def _speak(self, text):\n        \"\"\"Speak text asynchronously\"\"\"\n        try:\n            self.get_logger().info(f'Speaking: \"{text}\"')\n            self.engine.say(text)\n            self.engine.runAndWait()\n        except Exception as e:\n            self.get_logger().error(f'TTS error: {str(e)}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = TextToSpeechNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-6-complete-vla-agent-pipeline",children:"Part 6: Complete VLA Agent Pipeline"}),"\n",(0,r.jsx)(n.h3,{id:"end-to-end-conversational-robot",children:"End-to-End Conversational Robot"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nVision-Language-Action Agent\nComplete pipeline: Hear \u2192 Understand \u2192 See \u2192 Reason \u2192 Act\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport json\nimport time\n\nclass VLAAgent(Node):\n    def __init__(self):\n        super().__init__('vla_agent')\n\n        # Subscriptions\n        self.speech_sub = self.create_subscription(\n            String,\n            '/speech/transcription',\n            self.on_speech,\n            10\n        )\n\n        self.vision_sub = self.create_subscription(\n            String,\n            '/robot/answer',\n            self.on_vision_answer,\n            10\n        )\n\n        # Publishers\n        self.query_pub = self.create_publisher(String, '/robot/query', 10)\n        self.plan_pub = self.create_publisher(String, '/robot/plan', 10)\n        self.command_pub = self.create_publisher(String, '/robot/command', 10)\n        self.response_pub = self.create_publisher(String, '/robot/response', 10)\n        self.vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n\n        # State machine\n        self.state = 'idle'  # idle, listening, understanding, planning, executing\n        self.last_query = None\n        self.last_vision_answer = None\n\n        self.get_logger().info('VLA Agent initialized')\n\n    def on_speech(self, msg):\n        \"\"\"Process speech input\"\"\"\n        speech = msg.data\n        self.get_logger().info(f'[SPEECH] User: \"{speech}\"')\n\n        if self.state == 'idle' or self.state == 'executing':\n            self.state = 'understanding'\n            self.process_command(speech)\n\n    def process_command(self, command):\n        \"\"\"\n        Multi-step processing:\n        1. Ask vision system about environment\n        2. Generate plan based on vision + command\n        3. Execute plan\n        \"\"\"\n\n        # Step 1: Get scene understanding from vision\n        self.get_logger().info('[VLA] Step 1: Requesting scene understanding...')\n        self.last_query = command\n\n        query_msg = String()\n        query_msg.data = f'What do you see in this image? Help me: {command}'\n        self.query_pub.publish(query_msg)\n\n        # Wait for vision response (or timeout)\n        self.state = 'understanding'\n\n    def on_vision_answer(self, msg):\n        \"\"\"Process vision system response\"\"\"\n        vision_answer = msg.data\n        self.get_logger().info(f'[VISION] \"{vision_answer}\"')\n\n        if self.state != 'understanding':\n            return\n\n        self.last_vision_answer = vision_answer\n\n        # Step 2: Generate action plan based on speech + vision\n        self.get_logger().info('[VLA] Step 2: Generating action plan...')\n        self.generate_plan(self.last_query, vision_answer)\n\n    def generate_plan(self, command, vision_context):\n        \"\"\"Generate robot action plan\"\"\"\n\n        # Create context-aware prompt\n        plan_prompt = f\"\"\"Based on the user command and scene understanding, generate a robot action plan.\n\nUser Command: \"{command}\"\n\nScene Understanding: \"{vision_context}\"\n\nRobot Capabilities:\n- move_forward(distance) - Move forward X meters\n- turn(angle) - Turn X degrees\n- grasp(object) - Grasp detected object\n- release() - Open gripper\n- say(text) - Speak to user\n\nGenerate a JSON action sequence:\n{{\n  \"reasoning\": \"explanation\",\n  \"actions\": [\n    {{\"type\": \"move_forward\", \"distance\": 2.0}},\n    {{\"type\": \"grasp\", \"object\": \"cube\"}}\n  ],\n  \"say\": \"I've completed the task\"\n}}\"\"\"\n\n        # In real system, send to LLM for planning\n        # For demo, use simple rule-based planning\n        plan = {\n            'reasoning': f'Executing: {command}',\n            'actions': [\n                {'type': 'move_forward', 'distance': 1.0},\n                {'type': 'say', 'text': f'Understood. {vision_context}'}\n            ]\n        }\n\n        # Step 3: Execute plan\n        self.state = 'executing'\n        self.get_logger().info('[VLA] Step 3: Executing action plan...')\n        self.execute_plan(plan)\n\n    def execute_plan(self, plan):\n        \"\"\"Execute generated action plan\"\"\"\n        for action in plan['actions']:\n            action_type = action['type']\n\n            if action_type == 'move_forward':\n                self.get_logger().info(\n                    f'Moving forward {action[\"distance\"]}m'\n                )\n                twist = Twist()\n                twist.linear.x = 0.5\n                self.vel_pub.publish(twist)\n                time.sleep(action['distance'] / 0.5)\n                twist.linear.x = 0.0\n                self.vel_pub.publish(twist)\n\n            elif action_type == 'say':\n                response_msg = String()\n                response_msg.data = action['text']\n                self.response_pub.publish(response_msg)\n\n            elif action_type == 'grasp':\n                self.get_logger().info(\n                    f'Grasping: {action[\"object\"]}'\n                )\n\n        self.state = 'idle'\n        self.get_logger().info('[VLA] Plan executed. Ready for next command.')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    agent = VLAAgent()\n    rclpy.spin(agent)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-exercise-build-a-conversational-robot",children:"Hands-On Exercise: Build a Conversational Robot"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-voice-controlled-navigation",children:"Exercise 1: Voice-Controlled Navigation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: RealSpeaker mic\nros2 run perception_nodes respeaker_node.py\n\n# Terminal 2: Speech-to-text\nros2 run speech_nodes speech_to_text.py\n\n# Terminal 3: VLA Agent\nros2 run robot_agents vla_agent.py\n\n# Terminal 4: Test voice command\nros2 topic pub /robot/query std_msgs/String \"data: 'What objects are in front of me?'\"\n\n# Expected: Robot processes audio \u2192 converts to text \u2192 queries vision \u2192 speaks response\n"})}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-visual-question-answering",children:"Exercise 2: Visual Question Answering"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# Test VLM with different questions\n\nimport subprocess\nimport time\n\nquestions = [\n    "What color is the cube?",\n    "Is the path to the door clear?",\n    "How many objects do you see?",\n    "What\'s to the left of the red cube?",\n    "Can I grasp the item on the shelf?"\n]\n\nfor q in questions:\n    print(f"\\n[QUERY] {q}")\n    # Publish question\n    subprocess.run([\n        \'ros2\', \'topic\', \'pub\', \'/robot/query\',\n        \'std_msgs/String\', f\'data: "{q}"\'\n    ])\n    time.sleep(3)  # Wait for answer\n'})}),"\n",(0,r.jsx)(n.h3,{id:"exercise-3-multi-step-conversational-task",children:"Exercise 3: Multi-Step Conversational Task"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Simulate multi-turn conversation\n\nUser: "Is there a cup on the table?"\nRobot: [Looks at camera] "Yes, I see a white cup on the left side of the table."\n\nUser: "Pick it up and place it in the sink"\nRobot: [Plans path + grasping] "I\'ll navigate to the table, grasp the cup,\n       and move to the sink." [Executes]\n       [After success] "Task complete! The cup is now in the sink."\n\nUser: "What did you just do?"\nRobot: [References memory] "I navigated to the table, grasped a white cup,\n       and placed it in the sink as you requested."\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"common-vla-errors--debugging",children:"Common VLA Errors & Debugging"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Error"}),(0,r.jsx)(n.th,{children:"Cause"}),(0,r.jsx)(n.th,{children:"Fix"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:'"No audio device found"'})}),(0,r.jsx)(n.td,{children:"RealSpeaker not detected"}),(0,r.jsxs)(n.td,{children:["Check USB connection, run ",(0,r.jsx)(n.code,{children:"arecord -l"})]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:'"VLM timeout (>30s)"'})}),(0,r.jsx)(n.td,{children:"Model loading from internet"}),(0,r.jsx)(n.td,{children:"Pre-download LLaVA weights, use smaller model"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:'"Speech recognition failure"'})}),(0,r.jsx)(n.td,{children:"Noisy environment or accent"}),(0,r.jsx)(n.td,{children:"Adjust STT confidence threshold, fine-tune on local speech"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:'"Plan hallucination"'})}),(0,r.jsx)(n.td,{children:"LLM generates invalid actions"}),(0,r.jsx)(n.td,{children:"Use constraint checking, validate actions before execution"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Audio-vision sync issue"})}),(0,r.jsx)(n.td,{children:"Different frame rates"}),(0,r.jsx)(n.td,{children:"Add timestamp synchronization between audio/video"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"CUDA out of memory"})}),(0,r.jsx)(n.td,{children:"VLM + Vision pipeline too large"}),(0,r.jsx)(n.td,{children:"Use int8 quantization, reduce image resolution"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Vision-Language Models Enable True Multimodal Robots"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"VLMs understand images in natural language context"}),"\n",(0,r.jsx)(n.li,{children:"LLaVA is free and runs locally on Jetson"}),"\n",(0,r.jsx)(n.li,{children:"Cloud VLMs (GPT-4V) offer higher accuracy but cost $0.01/image"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Hearing + Speech Requires Multiple Components"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"RealSpeaker beamforming captures speech direction"}),"\n",(0,r.jsx)(n.li,{children:"Speech-to-text converts audio to commands"}),"\n",(0,r.jsx)(n.li,{children:"Text-to-speech provides natural robot responses"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"VLA Pipeline: Hear \u2192 Understand \u2192 See \u2192 Reason \u2192 Act"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Speech captured and transcribed"}),"\n",(0,r.jsx)(n.li,{children:"Vision models answer questions about scene"}),"\n",(0,r.jsx)(n.li,{children:"LLMs generate action plans"}),"\n",(0,r.jsx)(n.li,{children:"Robot executes and reports results"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Real-Time Constraints Are Critical"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Total latency must be <2 seconds for natural conversation"}),"\n",(0,r.jsx)(n.li,{children:"Break down into: STT (500ms) + Vision (1000ms) + Planning (200ms) + TTS (500ms)"}),"\n",(0,r.jsx)(n.li,{children:"Optimize slowest components first"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Edge Deployment is Practical"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"LLaVA 7B runs on Jetson in 500-1000ms"}),"\n",(0,r.jsx)(n.li,{children:"Local TTS via pyttsx3 is instant"}),"\n",(0,r.jsx)(n.li,{children:"Offline capability with graceful cloud fallback"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsx)(n.h3,{id:"vision-language-model-papers",children:"Vision-Language Model Papers"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2304.08485",children:"LLaVA: Large Language and Vision Assistant"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://cdn.openai.com/papers/GPTV_System_Card.pdf",children:"GPT-4V System Card"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2312.11805",children:"Gemini: A Family of Highly Capable Multimodal Models"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"robot-conversation--hri",children:"Robot Conversation & HRI"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2310.09549",children:"Towards Natural Human-Robot Interaction"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2202.07308",children:"Real-time Dialogue Systems for Robots"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"implementation-resources",children:"Implementation Resources"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/haotian-liu/LLaVA",children:"LLaVA GitHub"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/respeaker/respeaker_ros",children:"RealSpeaker Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://cloud.google.com/speech-to-text",children:"Google Cloud Speech-to-Text"})}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"next-lesson",children:"Next Lesson"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Lesson 4.3: Human-Robot Interaction & Safety Framework"})}),"\n",(0,r.jsxs)(n.p,{children:["Now that your robot can see, hear, and speak, we'll add the critical layer: ",(0,r.jsx)(n.strong,{children:"safety"}),". You'll learn:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Collision detection and emergency stop mechanisms"}),"\n",(0,r.jsx)(n.li,{children:"Safe human-robot collaboration"}),"\n",(0,r.jsx)(n.li,{children:"Ethical AI frameworks and bias mitigation"}),"\n",(0,r.jsx)(n.li,{children:"Real-world failure analysis from robot incidents"}),"\n",(0,r.jsx)(n.li,{children:"Production-ready safety checklist"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The robot must be safe BEFORE it's smart."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:function(e,n,i){i.d(n,{R:function(){return a},x:function(){return o}});var t=i(6540);const r={},s=t.createContext(r);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);