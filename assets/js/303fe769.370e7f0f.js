"use strict";(self.webpackChunkphysical_ai_textbook=self.webpackChunkphysical_ai_textbook||[]).push([[298],{8453:function(e,n,t){t.d(n,{R:function(){return i},x:function(){return a}});var s=t(6540);const r={},o=s.createContext(r);function i(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(o.Provider,{value:n},e.children)}},9740:function(e,n,t){t.r(n),t.d(n,{assets:function(){return l},contentTitle:function(){return a},default:function(){return p},frontMatter:function(){return i},metadata:function(){return s},toc:function(){return c}});var s=JSON.parse('{"id":"chapter-4/4-1-llm-brain","title":"LLM Integration for Robotics: Building the Robot\'s Cognitive Core","description":"Integrate large language models (GPT-4, Gemini, open-source) with ROS 2 for natural language understanding and reasoning","source":"@site/docs/chapter-4/4-1-llm-brain.md","sourceDirName":"chapter-4","slug":"/chapter-4/4-1-llm-brain","permalink":"/physical-ai-textbook/docs/chapter-4/4-1-llm-brain","draft":false,"unlisted":false,"editUrl":"https://github.com/Bil4l-Mehmood/physical-ai-textbook/edit/main/docs/chapter-4/4-1-llm-brain.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"sidebar_label":"Lesson 4.1: LLM Integration","title":"LLM Integration for Robotics: Building the Robot\'s Cognitive Core","description":"Integrate large language models (GPT-4, Gemini, open-source) with ROS 2 for natural language understanding and reasoning","duration":120,"difficulty":"Advanced","hardware":["Jetson Orin Nano 8GB","OpenAI API key OR local LLM","ROS 2 Humble","4GB free VRAM minimum"],"prerequisites":["Lesson 3.3: Computer Vision with Isaac ROS"]},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3.3: Computer Vision Isaac ROS","permalink":"/physical-ai-textbook/docs/chapter-3/3-3-computer-vision-isaac-ros"},"next":{"title":"Lesson 4.2: Conversational AI & VLA","permalink":"/physical-ai-textbook/docs/chapter-4/4-2-conversational-ai"}}'),r=t(4848),o=t(8453);const i={sidebar_position:1,sidebar_label:"Lesson 4.1: LLM Integration",title:"LLM Integration for Robotics: Building the Robot's Cognitive Core",description:"Integrate large language models (GPT-4, Gemini, open-source) with ROS 2 for natural language understanding and reasoning",duration:120,difficulty:"Advanced",hardware:["Jetson Orin Nano 8GB","OpenAI API key OR local LLM","ROS 2 Humble","4GB free VRAM minimum"],prerequisites:["Lesson 3.3: Computer Vision with Isaac ROS"]},a="Lesson 4.1: LLM Integration for Robotics - Building the Cognitive Core",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2},{value:"Cloud-Based LLMs (Recommended for beginners)",id:"cloud-based-llms-recommended-for-beginners",level:3},{value:"Edge-Based LLMs (Advanced)",id:"edge-based-llms-advanced",level:3},{value:"Part 1: LLM Fundamentals for Robotics",id:"part-1-llm-fundamentals-for-robotics",level:2},{value:"What LLMs Bring to Robots",id:"what-llms-bring-to-robots",level:3},{value:"Key Concepts for Robot Developers",id:"key-concepts-for-robot-developers",level:3},{value:"Part 2: Cloud-Based LLM Integration with OpenAI",id:"part-2-cloud-based-llm-integration-with-openai",level:2},{value:"Setting Up OpenAI API",id:"setting-up-openai-api",level:3},{value:"ROS 2 LLM Integration Node",id:"ros-2-llm-integration-node",level:3},{value:"Part 3: Open-Source LLM Deployment with Ollama",id:"part-3-open-source-llm-deployment-with-ollama",level:2},{value:"Installing Ollama on Jetson",id:"installing-ollama-on-jetson",level:3},{value:"Local LLM ROS 2 Node",id:"local-llm-ros-2-node",level:3},{value:"Part 4: Prompt Engineering for Robotic Tasks",id:"part-4-prompt-engineering-for-robotic-tasks",level:2},{value:"Template: Task Planning Prompt",id:"template-task-planning-prompt",level:3},{value:"Template: Safety Verification Prompt",id:"template-safety-verification-prompt",level:3},{value:"Part 5: Handling LLM Failure Cases",id:"part-5-handling-llm-failure-cases",level:2},{value:"Fallback Strategies",id:"fallback-strategies",level:3},{value:"Part 6: Cost Optimization Strategies",id:"part-6-cost-optimization-strategies",level:2},{value:"Token Budget Management",id:"token-budget-management",level:3},{value:"Hands-On Exercise: Build Your First LLM-Powered Robot",id:"hands-on-exercise-build-your-first-llm-powered-robot",level:2},{value:"Exercise 1: Simple Command-to-Action Conversion",id:"exercise-1-simple-command-to-action-conversion",level:3},{value:"Exercise 2: Create Custom Robot Instruction Set",id:"exercise-2-create-custom-robot-instruction-set",level:3},{value:"Exercise 3: Monitor and Optimize LLM Latency",id:"exercise-3-monitor-and-optimize-llm-latency",level:3},{value:"Common LLM Integration Errors &amp; Debugging",id:"common-llm-integration-errors--debugging",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Official Documentation",id:"official-documentation",level:3},{value:"Research Papers",id:"research-papers",level:3},{value:"Robotics Integration",id:"robotics-integration",level:3},{value:"Next Lesson",id:"next-lesson",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"lesson-41-llm-integration-for-robotics---building-the-cognitive-core",children:"Lesson 4.1: LLM Integration for Robotics - Building the Cognitive Core"})}),"\n",(0,r.jsxs)(n.admonition,{title:"Lesson Overview",type:"info",children:[(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duration"}),": 120 minutes | ",(0,r.jsx)(n.strong,{children:"Difficulty"}),": Advanced | ",(0,r.jsx)(n.strong,{children:"Hardware"}),": Jetson + LLM access (cloud or local)"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Prerequisites"}),": Chapter 3 complete (perception and navigation working)"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Learning Outcome"}),": Integrate large language models with ROS 2 to enable robots to understand natural language commands, reason about tasks, and generate semantic understanding of the environment"]})]}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand how large language models (LLMs) enhance robot decision-making"}),"\n",(0,r.jsx)(n.li,{children:"Compare cloud-based LLMs (GPT-4, Gemini) vs. edge-deployed LLMs"}),"\n",(0,r.jsx)(n.li,{children:"Integrate OpenAI API with ROS 2 for natural language processing"}),"\n",(0,r.jsx)(n.li,{children:"Deploy open-source LLMs locally using Ollama"}),"\n",(0,r.jsx)(n.li,{children:"Build prompt engineering strategies for robot task understanding"}),"\n",(0,r.jsx)(n.li,{children:"Create ROS 2 nodes that invoke LLMs for planning and reasoning"}),"\n",(0,r.jsx)(n.li,{children:"Parse LLM outputs into actionable robot commands"}),"\n",(0,r.jsx)(n.li,{children:"Handle API failures and implement fallback strategies"}),"\n",(0,r.jsx)(n.li,{children:"Measure LLM inference latency and optimize for real-time control"}),"\n",(0,r.jsx)(n.li,{children:"Build multi-turn conversation systems with context memory"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,r.jsx)(n.admonition,{title:"LLM Deployment Strategies",type:"note",children:(0,r.jsx)(n.p,{children:"LLMs present a choice: cloud convenience vs. edge autonomy. This lesson covers both approaches."})}),"\n",(0,r.jsx)(n.h3,{id:"cloud-based-llms-recommended-for-beginners",children:"Cloud-Based LLMs (Recommended for beginners)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Provider"}),(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Cost"}),(0,r.jsx)(n.th,{children:"Latency"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"OpenAI"})}),(0,r.jsx)(n.td,{children:"GPT-4 Turbo"}),(0,r.jsx)(n.td,{children:"$0.01/1K tokens"}),(0,r.jsx)(n.td,{children:"200-500ms"}),(0,r.jsx)(n.td,{children:"Complex reasoning, English"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"OpenAI"})}),(0,r.jsx)(n.td,{children:"GPT-3.5 Turbo"}),(0,r.jsx)(n.td,{children:"$0.001/1K tokens"}),(0,r.jsx)(n.td,{children:"100-300ms"}),(0,r.jsx)(n.td,{children:"Fast responses, cost-effective"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Google"})}),(0,r.jsx)(n.td,{children:"Gemini Pro"}),(0,r.jsx)(n.td,{children:"Free tier limited"}),(0,r.jsx)(n.td,{children:"300-600ms"}),(0,r.jsx)(n.td,{children:"Multimodal input (vision+text)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Anthropic"})}),(0,r.jsx)(n.td,{children:"Claude 3"}),(0,r.jsx)(n.td,{children:"$0.003/1K tokens"}),(0,r.jsx)(n.td,{children:"200-400ms"}),(0,r.jsx)(n.td,{children:"Long context, nuanced reasoning"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Advantages:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Latest models always available"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 No local resource burden"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Proven reliability at scale"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Easy integration via API"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Disadvantages:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u274c Internet dependency required"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Per-token cost accumulates"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Data sent to external servers"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Higher latency (100-500ms)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"edge-based-llms-advanced",children:"Edge-Based LLMs (Advanced)"}),"\n",(0,r.jsx)(n.p,{children:"Running LLMs locally on Jetson requires:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"VRAM"}),(0,r.jsx)(n.th,{children:"Latency"}),(0,r.jsx)(n.th,{children:"Speed"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Llama 2 7B Quantized"})}),(0,r.jsx)(n.td,{children:"4-6 GB"}),(0,r.jsx)(n.td,{children:"500-1000ms"}),(0,r.jsx)(n.td,{children:"10-20 tokens/sec"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Mistral 7B Quantized"})}),(0,r.jsx)(n.td,{children:"4-6 GB"}),(0,r.jsx)(n.td,{children:"400-800ms"}),(0,r.jsx)(n.td,{children:"15-25 tokens/sec"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Phi 2 3B"})}),(0,r.jsx)(n.td,{children:"2-3 GB"}),(0,r.jsx)(n.td,{children:"200-400ms"}),(0,r.jsx)(n.td,{children:"25-35 tokens/sec"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"TinyLlama 1B"})}),(0,r.jsx)(n.td,{children:"1-2 GB"}),(0,r.jsx)(n.td,{children:"100-200ms"}),(0,r.jsx)(n.td,{children:"50+ tokens/sec"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Deployment with Ollama:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ollama pull llama2\n# OR\nollama pull mistral\n# OR\nollama pull phi\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Advantages:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Zero latency from network"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Complete privacy (no data leaves robot)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Works offline"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 No per-token costs"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Disadvantages:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u274c Limited model selection"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Lower quality than cloud LLMs"}),"\n",(0,r.jsx)(n.li,{children:"\u274c High memory usage"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Slower inference"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-1-llm-fundamentals-for-robotics",children:"Part 1: LLM Fundamentals for Robotics"}),"\n",(0,r.jsx)(n.h3,{id:"what-llms-bring-to-robots",children:"What LLMs Bring to Robots"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Traditional Robot Programming:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'User Input: "Pick up the cube"\n    \u2193\nHardcoded Parser \u2192 Extract: object="cube", action="pick"\n    \u2193\nState Machine \u2192 Execute predefined sequence\n    \u2193\nIssue: Doesn\'t handle variations like "grab the red one" or "get the big cube"\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"LLM-Enhanced Robot:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'User Input: "Pick up the red cube if it\'s to your left, otherwise move to the table first"\n    \u2193\nLLM Processing:\n\u251c\u2500 Understand: Complex conditional logic\n\u251c\u2500 Reason: Check left side first, then table\n\u251c\u2500 Generate: Multi-step plan\n\u2514\u2500 Translate: [look_left, detect_cube, if_found: grasp, else: navigate_to_table, grasp]\n    \u2193\nRobot Execution\n    \u2193\n\u2705 Handles natural language variation and context\n'})}),"\n",(0,r.jsx)(n.h3,{id:"key-concepts-for-robot-developers",children:"Key Concepts for Robot Developers"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. Prompt Engineering for Robotics"})}),"\n",(0,r.jsx)(n.p,{children:"Effective prompts constrain LLM outputs to robot capabilities:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# BAD: Too open-ended\nprompt = "What should the robot do?"\n\n# GOOD: Specifies constraints\nprompt = """You are a robot control system.\nThe robot can:\n- Move forward/backward/left/right at 0-1 m/s\n- Grasp objects with gripper (open/close)\n- Detect objects in view (red, blue, yellow, green)\n\nUser command: "Pick up the red cube"\n\nGenerate a JSON action sequence like:\n{"actions": [\n  {"type": "move_towards", "object": "red cube"},\n  {"type": "grasp", "duration": 1000},\n  {"type": "move_to", "location": "drop zone"}\n]}\n"""\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. Token Economy"})}),"\n",(0,r.jsx)(n.p,{children:'LLMs process text as "tokens" (~4 characters = 1 token). Costs accumulate:'}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"GPT-4: $0.03 per 1K input tokens, $0.06 per 1K output tokens\n\nExample: 5-minute robot conversation\n\u251c\u2500 User commands: 100 tokens/min \xd7 5 = 500 tokens (input)\n\u251c\u2500 Robot responses: 150 tokens/min \xd7 5 = 750 tokens (output)\n\u251c\u2500 Cost: (500 \xd7 $0.03 / 1000) + (750 \xd7 $0.06 / 1000) = $0.06\n\u2514\u2500 Per 8-hour day: $0.06 \xd7 480 interactions = ~$29/day\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3. Context Window Size"})}),"\n",(0,r.jsx)(n.p,{children:"LLMs have memory limits:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Model          Context Window    Practical for Robots\nGPT-3.5 Turbo  4,096 tokens      Recent 10-15 commands\nGPT-4 Turbo    128,000 tokens    Full conversation history (8+ hours)\nLlama 2 7B     4,096 tokens      Last few interactions only\nMistral 7B     8,000 tokens      ~20 minutes of dialogue\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"4. Latency Constraints"})}),"\n",(0,r.jsx)(n.p,{children:"Robot control requires fast inference:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Acceptable Latency Thresholds:\n\u251c\u2500 Planning (before action): 200-500ms (human waits)\n\u251c\u2500 Reactive control (during action): 50-100ms (real-time required)\n\u251c\u2500 Conversation (dialogue): 500-1000ms (human conversation pace)\n\u251c\u2500 Analysis (after action): 1000ms+ (no real-time requirement)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-2-cloud-based-llm-integration-with-openai",children:"Part 2: Cloud-Based LLM Integration with OpenAI"}),"\n",(0,r.jsx)(n.h3,{id:"setting-up-openai-api",children:"Setting Up OpenAI API"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Step 1: Create OpenAI Account and API Key"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Visit: https://platform.openai.com/account/api-keys\n# Create new secret key\nexport OPENAI_API_KEY="sk-proj-xxxxxxxxxxxxxxxxxxxxxxxx"\n\n# Verify (don\'t hardcode keys!)\necho $OPENAI_API_KEY\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Step 2: Install Python Dependencies"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# OpenAI SDK\npip install openai\n\n# ROS 2 client library\npip install rclpy sensor-msgs std-msgs\n\n# For local LLMs (Ollama) later\npip install ollama\n"})}),"\n",(0,r.jsx)(n.h3,{id:"ros-2-llm-integration-node",children:"ROS 2 LLM Integration Node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nROS 2 node for LLM-based robot planning\nConnects to OpenAI GPT-4 Turbo for natural language understanding\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\nimport os\nfrom openai import OpenAI\n\nclass LLMRobotPlanner(Node):\n    def __init__(self):\n        super().__init__('llm_robot_planner')\n\n        # Initialize OpenAI client\n        api_key = os.getenv('OPENAI_API_KEY')\n        if not api_key:\n            self.get_logger().error(\n                'OPENAI_API_KEY not set. Run: export OPENAI_API_KEY=\"sk-...\"'\n            )\n            return\n\n        self.client = OpenAI(api_key=api_key)\n        self.model = 'gpt-4-turbo-preview'  # Latest reasoning model\n\n        # Robot capabilities (known to LLM via system prompt)\n        self.robot_capabilities = {\n            'movements': ['move_forward', 'move_backward', 'turn_left', 'turn_right'],\n            'actions': ['grasp', 'release', 'look_around'],\n            'sensors': ['camera', 'lidar', 'force_sensor'],\n            'max_speed': 1.0,  # m/s\n            'gripper_force': 10  # Newtons\n        }\n\n        # System prompt that constrains LLM behavior\n        self.system_prompt = self._build_system_prompt()\n\n        # Conversation history for context\n        self.conversation_history = []\n\n        # ROS 2 Interface\n        self.command_sub = self.create_subscription(\n            String,\n            '/human/command',\n            self.command_callback,\n            10\n        )\n\n        self.plan_pub = self.create_publisher(\n            String,\n            '/robot/plan',\n            10\n        )\n\n        self.explanation_pub = self.create_publisher(\n            String,\n            '/robot/explanation',\n            10\n        )\n\n        self.get_logger().info('LLM Robot Planner initialized with GPT-4')\n\n    def _build_system_prompt(self):\n        \"\"\"Create system prompt that constrains LLM to robot domain\"\"\"\n        return f\"\"\"You are an intelligent robot planning system. Your role is to understand human commands in natural language and convert them into executable robot actions.\n\nRobot Capabilities:\n- Movements: {', '.join(self.robot_capabilities['movements'])}\n- Actions: {', '.join(self.robot_capabilities['actions'])}\n- Sensors: {', '.join(self.robot_capabilities['sensors'])}\n- Max speed: {self.robot_capabilities['max_speed']} m/s\n- Gripper force: {self.robot_capabilities['gripper_force']} N\n\nResponse Format: You must respond with a JSON object containing:\n1. \"reasoning\": Your explanation of the command (1-2 sentences)\n2. \"actions\": A list of sequential actions, each with:\n   - \"type\": Action type (movement, sensor, manipulation)\n   - \"params\": Relevant parameters\n   - \"duration_ms\": Expected duration (optional)\n3. \"safety_checks\": List of pre-execution safety checks\n4. \"expected_outcome\": What should happen if successful\n\nExample response format:\n{{\n  \"reasoning\": \"Move to the table and grasp the red cube\",\n  \"actions\": [\n    {{\"type\": \"move_forward\", \"distance_m\": 2.0}},\n    {{\"type\": \"look_around\", \"scan_height\": \"table_level\"}},\n    {{\"type\": \"grasp\", \"duration_ms\": 1000}}\n  ],\n  \"safety_checks\": [\"Check path is clear\", \"Verify gripper is empty\"],\n  \"expected_outcome\": \"Red cube grasped and lifted\"\n}}\n\nAlways prioritize safety and feasibility. If a command seems impossible, explain why.\"\"\"\n\n    def command_callback(self, msg):\n        \"\"\"Process human command and generate plan\"\"\"\n        command = msg.data\n        self.get_logger().info(f'Received command: {command}')\n\n        try:\n            # Add user message to history\n            self.conversation_history.append({\n                'role': 'user',\n                'content': command\n            })\n\n            # Call GPT-4 with context\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=[\n                    {'role': 'system', 'content': self.system_prompt}\n                ] + self.conversation_history,\n                temperature=0.7,  # Balanced: creative but consistent\n                max_tokens=1000,\n                response_format={'type': 'json_object'}\n            )\n\n            # Extract response\n            response_text = response.choices[0].message.content\n            plan = json.loads(response_text)\n\n            # Add assistant response to history\n            self.conversation_history.append({\n                'role': 'assistant',\n                'content': response_text\n            })\n\n            # Limit history to last 10 exchanges (to manage token count)\n            if len(self.conversation_history) > 20:\n                self.conversation_history = self.conversation_history[-20:]\n\n            # Publish plan\n            plan_msg = String()\n            plan_msg.data = json.dumps(plan)\n            self.plan_pub.publish(plan_msg)\n\n            # Publish explanation\n            explanation_msg = String()\n            explanation_msg.data = plan['reasoning']\n            self.explanation_pub.publish(explanation_msg)\n\n            # Log statistics\n            self.get_logger().info(\n                f'Plan generated: {len(plan[\"actions\"])} actions, '\n                f'Tokens used: {response.usage.total_tokens}'\n            )\n\n        except json.JSONDecodeError:\n            self.get_logger().error('LLM response was not valid JSON')\n        except Exception as e:\n            self.get_logger().error(f'LLM error: {str(e)}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    planner = LLMRobotPlanner()\n    rclpy.spin(planner)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Usage:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Terminal 1: Start LLM planner\nexport OPENAI_API_KEY="sk-proj-..."\nros2 run my_robot llm_planner.py\n\n# Terminal 2: Send command\nros2 topic pub /human/command std_msgs/String "data: \'Move forward 2 meters and look for red objects\'"\n\n# Terminal 3: Monitor output\nros2 topic echo /robot/plan\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-3-open-source-llm-deployment-with-ollama",children:"Part 3: Open-Source LLM Deployment with Ollama"}),"\n",(0,r.jsx)(n.h3,{id:"installing-ollama-on-jetson",children:"Installing Ollama on Jetson"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Download Ollama (NVIDIA Jetson optimized)\ncurl https://ollama.ai/install.sh | sh\n\n# Start Ollama service\nollama serve\n\n# In another terminal, pull model\nollama pull mistral\n# Or smaller model for Jetson 8GB:\nollama pull phi\n"})}),"\n",(0,r.jsx)(n.h3,{id:"local-llm-ros-2-node",children:"Local LLM ROS 2 Node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nLocal LLM integration using Ollama\nNo API costs, zero latency from network, complete privacy\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\nimport requests\nimport time\n\nclass LocalLLMPlanner(Node):\n    def __init__(self):\n        super().__init__('local_llm_planner')\n\n        self.ollama_url = 'http://localhost:11434/api/generate'\n        self.model = 'phi'  # Fast 3B model for Jetson\n        # Alternative: 'mistral' (7B, better quality but slower)\n        # Alternative: 'neural-chat' (7B, optimized for dialogue)\n\n        self.command_sub = self.create_subscription(\n            String,\n            '/human/command',\n            self.command_callback,\n            10\n        )\n\n        self.plan_pub = self.create_publisher(String, '/robot/plan', 10)\n\n        # Check Ollama availability\n        try:\n            response = requests.get(\n                'http://localhost:11434/api/tags',\n                timeout=5\n            )\n            self.get_logger().info('Ollama service available')\n        except:\n            self.get_logger().error(\n                'Ollama not running. Run: ollama serve'\n            )\n\n    def command_callback(self, msg):\n        \"\"\"Process command with local LLM\"\"\"\n        command = msg.data\n        self.get_logger().info(f'Processing: {command}')\n\n        prompt = f\"\"\"You are a robot planning system. Generate a JSON action plan.\nRobot capabilities: move forward/backward, turn, grasp, release\n\nCommand: {command}\n\nRespond with:\n{{\n  \"reasoning\": \"explanation\",\n  \"actions\": [\n    {{\"type\": \"action_type\", \"params\": {{...}}}}\n  ]\n}}\"\"\"\n\n        try:\n            start_time = time.time()\n\n            # Call local Ollama\n            response = requests.post(\n                self.ollama_url,\n                json={\n                    'model': self.model,\n                    'prompt': prompt,\n                    'stream': False,\n                },\n                timeout=30\n            )\n\n            latency = (time.time() - start_time) * 1000\n\n            if response.status_code == 200:\n                result = response.json()\n                plan_text = result['response']\n\n                # Extract JSON from response\n                try:\n                    plan = json.loads(plan_text)\n                    plan_msg = String()\n                    plan_msg.data = json.dumps(plan)\n                    self.plan_pub.publish(plan_msg)\n\n                    self.get_logger().info(\n                        f'Plan generated in {latency:.1f}ms'\n                    )\n                except:\n                    self.get_logger().warn(\n                        f'Could not parse LLM response: {plan_text[:100]}'\n                    )\n            else:\n                self.get_logger().error(\n                    f'Ollama error: {response.status_code}'\n                )\n\n        except requests.exceptions.Timeout:\n            self.get_logger().error('Ollama request timeout (>30s)')\n        except Exception as e:\n            self.get_logger().error(f'Error: {str(e)}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    planner = LocalLLMPlanner()\n    rclpy.spin(planner)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-4-prompt-engineering-for-robotic-tasks",children:"Part 4: Prompt Engineering for Robotic Tasks"}),"\n",(0,r.jsx)(n.h3,{id:"template-task-planning-prompt",children:"Template: Task Planning Prompt"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def build_task_planning_prompt(task, robot_state, environment):\n    \"\"\"Generate a prompt for multi-step task planning\"\"\"\n\n    return f\"\"\"You are an expert robot task planner.\n\nCURRENT STATE:\n- Robot location: {robot_state['position']}\n- Gripper: {'holding ' + robot_state['held_object'] if robot_state['held_object'] else 'empty'}\n- Battery: {robot_state['battery_percent']}%\n- Detected objects: {', '.join(robot_state['visible_objects'])}\n\nENVIRONMENT:\n- Floor type: {environment['floor_type']}\n- Obstacles: {', '.join(environment['obstacles'])}\n- Safe zones: {', '.join(environment['safe_zones'])}\n\nTASK: {task}\n\nCONSTRAINTS:\n1. Movements must be feasible (no moving through walls)\n2. Only interact with detected objects\n3. Ensure battery level > 20% to complete task\n4. Verify gripper force sufficient for object weight\n\nGenerate step-by-step action plan with safety verification.\"\"\"\n\n# Usage\ntask = \"Pick up the blue cube from the table and place it on the shelf\"\nrobot_state = {{\n    'position': [0.0, 0.0],\n    'held_object': None,\n    'battery_percent': 75,\n    'visible_objects': ['blue cube', 'table', 'shelf']\n}}\nenvironment = {{\n    'floor_type': 'tile',\n    'obstacles': ['chair', 'wall'],\n    'safe_zones': ['open floor', 'shelf area']\n}}\n\nprompt = build_task_planning_prompt(task, robot_state, environment)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"template-safety-verification-prompt",children:"Template: Safety Verification Prompt"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def build_safety_verification_prompt(plan, robot_limits):\n    """Verify plan safety before execution"""\n\n    return f"""Review this robot action plan for safety violations.\n\nPLAN:\n{json.dumps(plan, indent=2)}\n\nROBOT LIMITS:\n- Max linear speed: {robot_limits[\'max_speed\']} m/s\n- Max angular speed: {robot_limits[\'max_angular\']} rad/s\n- Gripper force: {robot_limits[\'gripper_force\']} N (max)\n- Reach radius: {robot_limits[\'reach_radius\']} m\n\nSAFETY CHECKS:\n1. Does plan exceed speed limits? YES/NO\n2. Does plan ask gripper to exceed force? YES/NO\n3. Are movements within reach radius? YES/NO\n4. Could plan harm humans nearby? YES/NO\n\nIf any check is NO, provide corrected plan."""\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-5-handling-llm-failure-cases",children:"Part 5: Handling LLM Failure Cases"}),"\n",(0,r.jsx)(n.h3,{id:"fallback-strategies",children:"Fallback Strategies"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class RobustLLMPlanner(Node):\n    def __init__(self):\n        super().__init__('robust_llm_planner')\n        self.fallback_strategies = [\n            self.try_gpt4,           # Primary\n            self.try_gpt35,          # Secondary (faster, cheaper)\n            self.try_ollama,         # Tertiary (local backup)\n            self.hardcoded_actions   # Final fallback (always works)\n        ]\n\n    def process_command_with_fallback(self, command):\n        \"\"\"Try LLM methods in priority order\"\"\"\n\n        for strategy in self.fallback_strategies:\n            try:\n                plan = strategy(command)\n                if plan:\n                    self.get_logger().info(\n                        f'Generated plan using {strategy.__name__}'\n                    )\n                    return plan\n            except Exception as e:\n                self.get_logger().warn(\n                    f'{strategy.__name__} failed: {str(e)}'\n                )\n                continue\n\n        # All strategies failed\n        self.get_logger().error('All LLM strategies failed!')\n        return None\n\n    def hardcoded_actions(self, command):\n        \"\"\"Fallback: Match against known patterns\"\"\"\n        patterns = {\n            'move': {'type': 'move_forward', 'distance': 1.0},\n            'stop': {'type': 'stop'},\n            'grasp': {'type': 'grasp'},\n            'release': {'type': 'release'}\n        }\n\n        for keyword, action in patterns.items():\n            if keyword in command.lower():\n                return {'actions': [action], 'source': 'hardcoded'}\n\n        return None\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-6-cost-optimization-strategies",children:"Part 6: Cost Optimization Strategies"}),"\n",(0,r.jsx)(n.h3,{id:"token-budget-management",children:"Token Budget Management"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class TokenBudgetManager:\n    def __init__(self, daily_budget_dollars=5.0):\n        self.daily_budget = daily_budget_dollars\n        self.tokens_used_today = 0\n        self.cost_so_far = 0.0\n\n    def estimate_cost(self, input_tokens, output_tokens, model='gpt-4-turbo'):\n        \"\"\"Estimate API cost before making call\"\"\"\n        # Prices per 1K tokens\n        prices = {\n            'gpt-4-turbo': {'input': 0.01, 'output': 0.03},\n            'gpt-3.5-turbo': {'input': 0.0005, 'output': 0.0015},\n            'claude-3': {'input': 0.003, 'output': 0.015}\n        }\n\n        rate = prices.get(model, prices['gpt-3.5-turbo'])\n        cost = (input_tokens * rate['input'] +\n                output_tokens * rate['output']) / 1000\n\n        return cost\n\n    def should_call_llm(self, estimated_cost):\n        \"\"\"Check if we're within budget\"\"\"\n        if self.cost_so_far + estimated_cost > self.daily_budget:\n            self.logger.warn(\n                f'Would exceed daily budget. Cost: ${estimated_cost:.3f}, '\n                f'Used: ${self.cost_so_far:.2f}, Budget: ${self.daily_budget}'\n            )\n            return False\n        return True\n\n    def log_usage(self, input_tokens, output_tokens, model):\n        \"\"\"Track daily usage\"\"\"\n        cost = self.estimate_cost(input_tokens, output_tokens, model)\n        self.tokens_used_today += input_tokens + output_tokens\n        self.cost_so_far += cost\n\n        self.logger.info(\n            f'API used: ${cost:.4f} | Daily: ${self.cost_so_far:.2f} / '\n            f'${self.daily_budget} | Tokens: {self.tokens_used_today}'\n        )\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-exercise-build-your-first-llm-powered-robot",children:"Hands-On Exercise: Build Your First LLM-Powered Robot"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-simple-command-to-action-conversion",children:"Exercise 1: Simple Command-to-Action Conversion"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Start LLM node\nros2 run my_robot llm_planner.py\n\n# Terminal 2: Test various commands\nros2 topic pub /human/command std_msgs/String \"data: 'Move forward 1 meter'\"\nros2 topic pub /human/command std_msgs/String \"data: 'Look left and detect objects'\"\nros2 topic pub /human/command std_msgs/String \"data: 'Pick up the red cube from the table'\"\n\n# Terminal 3: Monitor generated plans\nros2 topic echo /robot/plan | head -20\n# You should see JSON with actions, reasoning, safety checks\n"})}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-create-custom-robot-instruction-set",children:"Exercise 2: Create Custom Robot Instruction Set"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# Create a domain-specific language for your robot\n\nROBOT_ACTIONS = {\n    'goto': {'params': ['x', 'y'], 'duration': 3000},\n    'grasp': {'params': ['object_id'], 'duration': 1000},\n    'release': {'params': [], 'duration': 500},\n    'scan': {'params': ['radius'], 'duration': 2000},\n    'wait': {'params': ['seconds'], 'duration': None}\n}\n\n# Create instruction set prompt\ninstruction_set = \"\"\"\nValid robot actions:\n- goto(x, y): Move to coordinate (x, y)\n- grasp(object): Close gripper around object\n- release(): Open gripper\n- scan(radius): Detect objects within radius meters\n- wait(seconds): Pause execution\n\nExample: [scan(2.0), grasp('red_cube'), goto(1.0, 1.0), release()]\n\"\"\"\n\n# Teach LLM about your action language\nprompt = f\"\"\"You are translating human commands to robot actions.\n{instruction_set}\n\nUser command: 'Pick up the object on the table'\n\nOutput the action sequence in the format:\n[action_name(param1, param2), action_name(param1)]\n\"\"\"\n\n# Test different LLM models\nmodels_to_test = ['gpt-4', 'gpt-3.5-turbo', 'local:phi']\nfor model in models_to_test:\n    response = call_llm(prompt, model)\n    actions = parse_action_sequence(response)\n    print(f'{model}: {actions}')\n"})}),"\n",(0,r.jsx)(n.h3,{id:"exercise-3-monitor-and-optimize-llm-latency",children:"Exercise 3: Monitor and Optimize LLM Latency"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Create a benchmarking script\npython3 benchmark_llm_latency.py \\\n  --model gpt-4 \\\n  --iterations 10 \\\n  --input-length 100 \\\n  --output-length 500\n\n# Expected output:\n# GPT-4 Turbo:\n#   Min latency: 180ms\n#   Max latency: 520ms\n#   Avg latency: 340ms\n#   P95 latency: 480ms\n#   Tokens/sec: 125\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"common-llm-integration-errors--debugging",children:"Common LLM Integration Errors & Debugging"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Error"}),(0,r.jsx)(n.th,{children:"Cause"}),(0,r.jsx)(n.th,{children:"Fix"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:'"API key invalid"'})}),(0,r.jsx)(n.td,{children:"Expired or revoked key"}),(0,r.jsxs)(n.td,{children:["Run ",(0,r.jsx)(n.code,{children:'export OPENAI_API_KEY="sk-..."'})]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:'"Rate limit exceeded"'})}),(0,r.jsx)(n.td,{children:"Too many requests per minute"}),(0,r.jsx)(n.td,{children:"Implement request queuing, backoff strategy"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"JSON parsing error"})}),(0,r.jsx)(n.td,{children:"LLM output not valid JSON"}),(0,r.jsx)(n.td,{children:"Add JSON validation, use response_format='json'"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Timeout (>30s)"})}),(0,r.jsx)(n.td,{children:"Network latency or LLM overloaded"}),(0,r.jsx)(n.td,{children:"Add retry logic, use faster model (GPT-3.5)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:'"Context window exceeded"'})}),(0,r.jsx)(n.td,{children:"Conversation history too long"}),(0,r.jsx)(n.td,{children:"Trim history to last 10-15 exchanges"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Offline (Ollama)"})}),(0,r.jsx)(n.td,{children:"Local LLM service not running"}),(0,r.jsxs)(n.td,{children:["Run ",(0,r.jsx)(n.code,{children:"ollama serve"})," in separate terminal"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Memory OOM (Ollama)"})}),(0,r.jsx)(n.td,{children:"Model too large for Jetson"}),(0,r.jsx)(n.td,{children:"Use smaller model (phi 3B instead of Mistral 7B)"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"LLM Architecture for Robots"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"LLMs are powerful for natural language understanding and reasoning"}),"\n",(0,r.jsx)(n.li,{children:"Cloud LLMs (GPT-4) offer best quality but require internet"}),"\n",(0,r.jsx)(n.li,{children:"Local LLMs (Ollama) offer privacy and offline capability but lower quality"}),"\n",(0,r.jsx)(n.li,{children:"Hybrid approach: Use cloud for complex reasoning, local for routine commands"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Prompt Engineering is Critical"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Define robot capabilities explicitly in system prompt"}),"\n",(0,r.jsx)(n.li,{children:"Provide examples of desired output format (JSON)"}),"\n",(0,r.jsx)(n.li,{children:"Include safety constraints and verification checks"}),"\n",(0,r.jsx)(n.li,{children:"Limit context window to manage token costs"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Real-Time Integration Requires Careful Design"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Latency budgets vary by use case (planning: 200-500ms, reactive: 50-100ms)"}),"\n",(0,r.jsx)(n.li,{children:"Implement fallback strategies for API failures"}),"\n",(0,r.jsx)(n.li,{children:"Monitor token usage and set daily budgets"}),"\n",(0,r.jsx)(n.li,{children:"Cache frequent responses to reduce API calls"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Cost Management for Cloud APIs"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"GPT-4 costs ~$0.06 per 5-minute conversation"}),"\n",(0,r.jsx)(n.li,{children:"GPT-3.5 is 60\xd7 cheaper but less capable"}),"\n",(0,r.jsx)(n.li,{children:"Compress prompts and responses to reduce token count"}),"\n",(0,r.jsx)(n.li,{children:"Consider local LLM for high-volume deployments"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Robot Task Planning with LLMs"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use structured prompts with state and constraints"}),"\n",(0,r.jsx)(n.li,{children:"Verify generated plans before execution"}),"\n",(0,r.jsx)(n.li,{children:"Handle failures gracefully with hardcoded fallback actions"}),"\n",(0,r.jsx)(n.li,{children:"Maintain conversation history for context awareness"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsx)(n.h3,{id:"official-documentation",children:"Official Documentation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://platform.openai.com/docs/",children:"OpenAI API Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://ollama.ai/library",children:"Ollama Model Library"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://ai.google.dev/",children:"Google Gemini API"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://console.anthropic.com/",children:"Anthropic Claude API"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"research-papers",children:"Research Papers"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2303.08774",children:"GPT-4 Technical Report"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2202.07539",children:"Prompting as a Fundamental Capability of LLMs"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2201.11903",children:"Chain-of-Thought Prompting"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2210.03629",children:"ReAct: Synergizing Reasoning and Acting in LLMs"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"robotics-integration",children:"Robotics Integration"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/humble/",children:"ROS 2 Official Tutorials"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2301.04871",children:"Language Models for Robot Planning"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2304.07978",children:"Embodied AI and Language Models"})}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"next-lesson",children:"Next Lesson"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Lesson 4.2: Conversational AI & Vision-Language-Action (VLA)"})}),"\n",(0,r.jsx)(n.p,{children:"Now that your robot has a cognitive core (LLMs), we'll add multimodal perception and action. You'll learn:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Integrate vision inputs with language models (vision-language models)"}),"\n",(0,r.jsx)(n.li,{children:"Build multimodal prompts combining images and text"}),"\n",(0,r.jsx)(n.li,{children:"Deploy VLA models (Llava, GPT-4V) for real-world understanding"}),"\n",(0,r.jsx)(n.li,{children:"Create conversational robots that see and understand their environment"}),"\n",(0,r.jsx)(n.li,{children:"Process robot actions from multimodal reasoning"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Integrated System:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'User (voice): "What\'s on the table?"\n    \u2193\n[Speech-to-text] \u2192 "What\'s on the table?"\n    \u2193\n[Robot Camera] \u2192 Captures image of table\n    \u2193\n[Vision-Language Model]\n\u251c\u2500 Input: Image + Question\n\u251c\u2500 Process: Understand scene with visual context\n\u251c\u2500 Output: "I see a red cube, a blue sphere, and a green cylinder"\n    \u2193\n[LLM Task Planning]\n\u251c\u2500 Input: "What\'s on the table?" + Visual understanding\n\u251c\u2500 Output: Action plan\n    \u2193\n[Text-to-Speech] \u2192 "I see a red cube, a blue sphere, and a green cylinder"\n    \u2193\nRobot speaks response to user\n'})})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);