"use strict";(self.webpackChunkphysical_ai_textbook=self.webpackChunkphysical_ai_textbook||[]).push([[30],{5413:function(e,n,i){i.r(n),i.d(n,{assets:function(){return c},contentTitle:function(){return a},default:function(){return m},frontMatter:function(){return r},metadata:function(){return t},toc:function(){return l}});var t=JSON.parse('{"id":"chapter-3/3-3-computer-vision-isaac-ros","title":"Computer Vision with Isaac ROS: Real-Time Perception for Humanoid Robots","description":"Implement hardware-accelerated object detection, semantic segmentation, and instance segmentation for real-time robot perception","source":"@site/docs/chapter-3/3-3-computer-vision-isaac-ros.md","sourceDirName":"chapter-3","slug":"/chapter-3/3-3-computer-vision-isaac-ros","permalink":"/physical-ai-textbook/docs/chapter-3/3-3-computer-vision-isaac-ros","draft":false,"unlisted":false,"editUrl":"https://github.com/Bil4l-Mehmood/physical-ai-textbook/edit/main/docs/chapter-3/3-3-computer-vision-isaac-ros.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"sidebar_label":"Lesson 3.3: Computer Vision Isaac ROS","title":"Computer Vision with Isaac ROS: Real-Time Perception for Humanoid Robots","description":"Implement hardware-accelerated object detection, semantic segmentation, and instance segmentation for real-time robot perception","duration":120,"difficulty":"Advanced","hardware":["Jetson Orin Nano 8GB","RealSense D435i","ROS 2 Humble","NVIDIA Isaac ROS"],"prerequisites":["Lesson 3.2: VSLAM & Navigation"]},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3.2: VSLAM & Navigation","permalink":"/physical-ai-textbook/docs/chapter-3/3-2-vslam-navigation"},"next":{"title":"Lesson 4.1: LLM Integration","permalink":"/physical-ai-textbook/docs/chapter-4/4-1-llm-brain"}}'),s=i(4848),o=i(8453);const r={sidebar_position:3,sidebar_label:"Lesson 3.3: Computer Vision Isaac ROS",title:"Computer Vision with Isaac ROS: Real-Time Perception for Humanoid Robots",description:"Implement hardware-accelerated object detection, semantic segmentation, and instance segmentation for real-time robot perception",duration:120,difficulty:"Advanced",hardware:["Jetson Orin Nano 8GB","RealSense D435i","ROS 2 Humble","NVIDIA Isaac ROS"],prerequisites:["Lesson 3.2: VSLAM & Navigation"]},a="Lesson 3.3: Computer Vision with Isaac ROS - Real-Time Perception",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2},{value:"Jetson Orin Nano Specifications for Vision",id:"jetson-orin-nano-specifications-for-vision",level:3},{value:"Recommended Camera",id:"recommended-camera",level:3},{value:"Part 1: Computer Vision Fundamentals for Robotics",id:"part-1-computer-vision-fundamentals-for-robotics",level:2},{value:"Vision Tasks Overview",id:"vision-tasks-overview",level:3},{value:"Object Detection with YOLO",id:"object-detection-with-yolo",level:3},{value:"Semantic Segmentation",id:"semantic-segmentation",level:3},{value:"Instance Segmentation",id:"instance-segmentation",level:3},{value:"Part 2: Isaac ROS Perception Architecture",id:"part-2-isaac-ros-perception-architecture",level:2},{value:"Isaac ROS Components for Vision",id:"isaac-ros-components-for-vision",level:3},{value:"TensorRT Optimization",id:"tensorrt-optimization",level:3},{value:"Part 3: Setting Up Computer Vision on Jetson Orin Nano",id:"part-3-setting-up-computer-vision-on-jetson-orin-nano",level:2},{value:"Step 1: Install NVIDIA Container Toolkit (Recommended)",id:"step-1-install-nvidia-container-toolkit-recommended",level:3},{value:"Step 2: Install Isaac ROS Perception Packages",id:"step-2-install-isaac-ros-perception-packages",level:3},{value:"Step 3: Download Pre-trained Models",id:"step-3-download-pre-trained-models",level:3},{value:"Step 4: Validate Perception Stack",id:"step-4-validate-perception-stack",level:3},{value:"Part 4: Real-Time Object Detection",id:"part-4-real-time-object-detection",level:2},{value:"YOLO Detection Node",id:"yolo-detection-node",level:3},{value:"Part 5: Semantic Segmentation",id:"part-5-semantic-segmentation",level:2},{value:"Semantic Segmentation Configuration",id:"semantic-segmentation-configuration",level:3},{value:"Semantic Segmentation Node",id:"semantic-segmentation-node",level:3},{value:"Part 6: Instance Segmentation (Mask R-CNN)",id:"part-6-instance-segmentation-mask-r-cnn",level:2},{value:"Instance Segmentation Node",id:"instance-segmentation-node",level:3},{value:"Part 7: Combining Perception with SLAM and Navigation",id:"part-7-combining-perception-with-slam-and-navigation",level:2},{value:"Full Perception Pipeline Launch File",id:"full-perception-pipeline-launch-file",level:3},{value:"Python Integration: Perception-Based Robot Control",id:"python-integration-perception-based-robot-control",level:3},{value:"Hands-On Exercise: Real-Time Object Detection and Segmentation",id:"hands-on-exercise-real-time-object-detection-and-segmentation",level:2},{value:"Exercise 1: Deploy YOLO and Measure Performance",id:"exercise-1-deploy-yolo-and-measure-performance",level:3},{value:"Exercise 2: Create Custom YOLO Training Dataset",id:"exercise-2-create-custom-yolo-training-dataset",level:3},{value:"Exercise 3: Debug Common Perception Errors",id:"exercise-3-debug-common-perception-errors",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Official Documentation",id:"official-documentation",level:3},{value:"Research Papers",id:"research-papers",level:3},{value:"Open Source Tools",id:"open-source-tools",level:3},{value:"Next Lesson",id:"next-lesson",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lesson-33-computer-vision-with-isaac-ros---real-time-perception",children:"Lesson 3.3: Computer Vision with Isaac ROS - Real-Time Perception"})}),"\n",(0,s.jsxs)(n.admonition,{title:"Lesson Overview",type:"info",children:[(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Duration"}),": 120 minutes | ",(0,s.jsx)(n.strong,{children:"Difficulty"}),": Advanced | ",(0,s.jsx)(n.strong,{children:"Hardware"}),": Jetson Orin Nano + RealSense D435i"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prerequisites"}),": Lesson 3.2 (VSLAM & Navigation) - understanding of Isaac ROS ecosystem"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Learning Outcome"}),": Deploy hardware-accelerated computer vision models for real-time object detection, segmentation, and scene understanding on edge devices"]})]}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand computer vision tasks: detection, segmentation, instance segmentation"}),"\n",(0,s.jsx)(n.li,{children:"Deploy pre-trained YOLO detection models on Jetson"}),"\n",(0,s.jsx)(n.li,{children:"Implement semantic segmentation for scene understanding"}),"\n",(0,s.jsx)(n.li,{children:"Use Isaac ROS perception nodes for hardware acceleration"}),"\n",(0,s.jsx)(n.li,{children:"Integrate TensorRT for inference optimization"}),"\n",(0,s.jsx)(n.li,{children:"Process camera streams with real-time perception"}),"\n",(0,s.jsx)(n.li,{children:"Debug perception pipelines with RViz visualization"}),"\n",(0,s.jsx)(n.li,{children:"Measure and optimize inference latency on edge hardware"}),"\n",(0,s.jsx)(n.li,{children:"Build robot perception systems combining SLAM and vision"}),"\n",(0,s.jsx)(n.li,{children:"Handle perception failures gracefully in robot applications"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,s.jsxs)(n.admonition,{title:"Perception on Edge Devices",type:"note",children:[(0,s.jsx)(n.p,{children:"Computer vision inference is computationally expensive. Edge deployment requires careful optimization:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time"}),": Object detection must run at 15-30 FPS"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Low latency"}),": <100ms from image capture to decision"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource constrained"}),": Jetson Orin Nano has 8GB VRAM shared with OS and SLAM"]}),"\n"]})]}),"\n",(0,s.jsx)(n.h3,{id:"jetson-orin-nano-specifications-for-vision",children:"Jetson Orin Nano Specifications for Vision"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Component"}),(0,s.jsx)(n.th,{children:"Specification"}),(0,s.jsx)(n.th,{children:"Impact"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"GPU Cores"})}),(0,s.jsx)(n.td,{children:"1024 NVIDIA CUDA cores"}),(0,s.jsx)(n.td,{children:"Can run ~8 TFLOPS INT8 inference"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"VRAM"})}),(0,s.jsx)(n.td,{children:"8 GB LPDDR5 (shared)"}),(0,s.jsx)(n.td,{children:"SLAM: 2-3GB, Vision: 2-3GB, OS: ~1GB"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Memory Bandwidth"})}),(0,s.jsx)(n.td,{children:"102 GB/s"}),(0,s.jsx)(n.td,{children:"Sufficient for HD image processing"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Power Budget"})}),(0,s.jsx)(n.td,{children:"15W typical"}),(0,s.jsx)(n.td,{children:"Fanless operation possible; passive cooling adequate"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Encoder/Decoder"})}),(0,s.jsx)(n.td,{children:"Hardware H.264/H.265"}),(0,s.jsx)(n.td,{children:"Can offload video encoding to NVENC"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"recommended-camera",children:"Recommended Camera"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Intel RealSense D435i"})," (Used throughout course)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resolution"}),": 1280\xd7720 @ 30 FPS (RGB + Depth)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Depth Accuracy"}),": \xb12% @ 2m range"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Field of View"}),": 87.3\xb0 \xd7 58\xb0 (RGB), 85.2\xb0 \xd7 58\xb0 (Depth)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IMU"}),": 6-axis (accelerometer + gyroscope)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"USB"}),": 3.1 bus (5Gbps) - sufficient for 720p @ 30Hz"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-1-computer-vision-fundamentals-for-robotics",children:"Part 1: Computer Vision Fundamentals for Robotics"}),"\n",(0,s.jsx)(n.h3,{id:"vision-tasks-overview",children:"Vision Tasks Overview"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Raw Image Input\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  COMPUTER VISION PROCESSING PIPELINE      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Detection (Bounding Boxes)  \u2502 Segmentation (Per-Pixel)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u2022 YOLO                       \u2502 \u2022 Semantic Segmentation       \u2502\n\u2502 \u2022 Faster R-CNN               \u2502   (class labels per pixel)    \u2502\n\u2502 \u2022 SSD                        \u2502 \u2022 Instance Segmentation       \u2502\n\u2502 \u2022 EfficientDet               \u2502   (individual objects)        \u2502\n\u2502 \u2022 Detectron2                 \u2502 \u2022 Panoptic Segmentation       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\nRobot Decision Making (Grasp, Navigate, Interact)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"object-detection-with-yolo",children:"Object Detection with YOLO"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"YOLO (You Only Look Once)"})," is the dominant real-time detection framework for robotics:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Single-shot detection"}),": Predicts all objects in one forward pass"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time speed"}),": YOLOv8n runs at 40-60 FPS on Jetson Orin Nano"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Various scales"}),": Nano (fastest), Small, Medium, Large (most accurate)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Training-friendly"}),": Can fine-tune on custom robot datasets"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"YOLO Detection Output"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'Detections:\n\u251c\u2500 Class: "humanoid" (confidence: 0.94)\n\u2502  \u2514\u2500 Bounding box: [x1, y1, x2, y2] = [125, 87, 342, 510]\n\u251c\u2500 Class: "obstacle" (confidence: 0.87)\n\u2502  \u2514\u2500 Bounding box: [x1, y1, x2, y2] = [450, 200, 580, 380]\n\u2514\u2500 Class: "target_object" (confidence: 0.91)\n   \u2514\u2500 Bounding box: [x1, y1, x2, y2] = [200, 150, 280, 320]\n'})}),"\n",(0,s.jsx)(n.h3,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Pixel-level classification"})," - assigns a class label to every pixel in the image:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Input Image    Segmentation Map\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \u2593\u2593\u2593\u2593\u2593   \u2502   \u2502 0 0 1 1 1 2  \u2502\n\u2502 \u2593\u2591\u2593\u2593\u2593   \u2502   \u2502 0 0 1 1 1 2  \u2502\n\u2502 \u2591\u2591\u2591\u2593\u2593   \u2502   \u2502 0 0 0 1 2 2  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             Classes: 0=Background, 1=Robot, 2=Obstacle\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Use cases for robots:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scene understanding"}),": What surfaces can the robot walk on?"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Obstacle avoidance"}),": Which pixels are navigable?"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Manipulation"}),": What parts of scene are graspable?"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"instance-segmentation",children:"Instance Segmentation"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Per-object pixel masks"})," - combines detection and segmentation:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Detection (Bounding Box) + Segmentation Mask = Instance Map\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502   \u2502 \u2593\u2593\u2593\u2593\u2593\u2593       \u2502   \u2502 1 1 1 1 1    \u2502\n\u2502 \u2502 obj1 \u2502     \u2502   \u2502 \u2593\u2593\u2593\u2593\u2593\u2593       \u2502   \u2502 1 1 1 1 1    \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502   \u2502 \u2593\u2593\u2593\u2593\u2593\u2593       \u2502   \u2502 1 1 1 1 1    \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2510  \u2502   \u2502   \u2593\u2593\u2593\u2593\u2593\u2593     \u2502   \u2502   2 2 2 2    \u2502\n\u2502   \u2502 obj2  \u2502  \u2502   \u2502   \u2593\u2593\u2593\u2593\u2593\u2593     \u2502   \u2502   2 2 2 2    \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502   \u2502   \u2593\u2593\u2593\u2593\u2593\u2593     \u2502   \u2502   2 2 2 2    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-2-isaac-ros-perception-architecture",children:"Part 2: Isaac ROS Perception Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-components-for-vision",children:"Isaac ROS Components for Vision"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"ROS 2 Network\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Isaac ROS Perception Pipeline     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                     \u2502\n\u2502  Input: /camera/color/image_raw    \u2502\n\u2502         /camera/depth/image_rect   \u2502\n\u2502              \u2193                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Image Preprocessing Node     \u2502  \u2502\n\u2502  \u2502 \u2022 Resize, normalize          \u2502  \u2502\n\u2502  \u2502 \u2022 Color space conversion     \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502              \u2193                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Inference Engine (TensorRT)  \u2502  \u2502\n\u2502  \u2502 \u2022 GPU acceleration            \u2502  \u2502\n\u2502  \u2502 \u2022 INT8/FP16 quantization      \u2502  \u2502\n\u2502  \u2502 \u2022 Batch processing            \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502              \u2193                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Post-processing Node         \u2502  \u2502\n\u2502  \u2502 \u2022 NMS (Non-Max Suppression)  \u2502  \u2502\n\u2502  \u2502 \u2022 Coordinate transformation  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502              \u2193                      \u2502\n\u2502  Output: /perception/detections    \u2502\n\u2502          /perception/segmentation  \u2502\n\u2502                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h3,{id:"tensorrt-optimization",children:"TensorRT Optimization"}),"\n",(0,s.jsx)(n.p,{children:"NVIDIA TensorRT converts standard models (PyTorch, ONNX) to optimized engines:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Metric"}),(0,s.jsx)(n.th,{children:"PyTorch Float32"}),(0,s.jsx)(n.th,{children:"TensorRT INT8"}),(0,s.jsx)(n.th,{children:"Speedup"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Latency (YOLOv8s)"}),(0,s.jsx)(n.td,{children:"45 ms"}),(0,s.jsx)(n.td,{children:"12 ms"}),(0,s.jsx)(n.td,{children:"3.75\xd7"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Memory"}),(0,s.jsx)(n.td,{children:"256 MB"}),(0,s.jsx)(n.td,{children:"64 MB"}),(0,s.jsx)(n.td,{children:"4\xd7 less"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Throughput"}),(0,s.jsx)(n.td,{children:"22 FPS"}),(0,s.jsx)(n.td,{children:"83 FPS"}),(0,s.jsx)(n.td,{children:"3.75\xd7"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Accuracy Loss"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"<1%"}),(0,s.jsx)(n.td,{children:"Acceptable"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Quantization Trade-offs:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"FP32 (Full Precision)\n\u2514\u2500 Latency: 45ms, Accuracy: 100%\n\nFP16 (Half Precision)\n\u2514\u2500 Latency: 25ms, Accuracy: 99.8%\n\nINT8 (Integer Quantization)\n\u2514\u2500 Latency: 12ms, Accuracy: 99.0%\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-3-setting-up-computer-vision-on-jetson-orin-nano",children:"Part 3: Setting Up Computer Vision on Jetson Orin Nano"}),"\n",(0,s.jsx)(n.h3,{id:"step-1-install-nvidia-container-toolkit-recommended",children:"Step 1: Install NVIDIA Container Toolkit (Recommended)"}),"\n",(0,s.jsx)(n.p,{children:"Using Docker ensures consistent dependencies and easy model updates:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Add Docker repository\ncurl https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\nsudo usermod -aG docker $USER\n\n# Install NVIDIA Container Runtime\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\\n  sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\nsudo apt-get update && sudo apt-get install -y nvidia-container-toolkit\nsudo systemctl restart docker\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-install-isaac-ros-perception-packages",children:"Step 2: Install Isaac ROS Perception Packages"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create workspace\nmkdir -p ~/isaac_ros_dev/src\ncd ~/isaac_ros_dev\n\n# Clone Isaac ROS repositories\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_common.git src/isaac_ros_common\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_perception.git src/isaac_ros_perception\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_image_proc.git src/isaac_ros_image_proc\n\n# Install dependencies\nsource /opt/ros/humble/setup.bash\nrosdep install -i --from-path src --rosdistro humble -y\n\n# Build workspace\ncolcon build --symlink-install\nsource install/setup.bash\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-download-pre-trained-models",children:"Step 3: Download Pre-trained Models"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create models directory\nmkdir -p ~/models/yolo ~/models/segmentation\n\n# Download YOLOv8 Nano (ONNX format)\ncd ~/models/yolo\nwget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt\n\n# Convert to ONNX (requires ultralytics package)\npip install ultralytics\nyolo export model=yolov8n.pt format=onnx\n# Output: yolov8n.onnx (6.3 MB)\n\n# Convert ONNX to TensorRT Engine\ntrtexec --onnx=yolov8n.onnx --saveEngine=yolov8n.engine --fp16\n# Outputs: yolov8n.engine (optimized for INT8, 1.5 MB)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-4-validate-perception-stack",children:"Step 4: Validate Perception Stack"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Terminal 1: RealSense camera\nros2 launch realsense2_camera rs_launch.py \\\n  rgb_camera.color_profile:="640,480,30" \\\n  enable_depth:=true\n\n# Terminal 2: Check topics\nros2 topic list\n# Expected output:\n# /camera/color/camera_info\n# /camera/color/image_raw\n# /camera/depth/image_rect_raw\n\n# Terminal 3: Verify camera publishing\nros2 run image_view image_view \\\n  --ros-args -r image:=/camera/color/image_raw\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-4-real-time-object-detection",children:"Part 4: Real-Time Object Detection"}),"\n",(0,s.jsx)(n.h3,{id:"yolo-detection-node",children:"YOLO Detection Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nYOLOv8 Real-Time Object Detection for ROS 2\nRuns inference on Jetson with TensorRT acceleration\n\"\"\"\n\nimport rclpy\nimport numpy as np\nimport cv2\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CompressedImage\nfrom vision_msgs.msg import Detection2DArray, Detection2D, ObjectHypothesisWithPose\nfrom cv_bridge import CvBridge\nfrom ultralytics import YOLO\nimport time\n\nclass YOLODetector(Node):\n    def __init__(self):\n        super().__init__('yolo_detector')\n\n        # Initialize YOLO with TensorRT engine (Nano model for Jetson)\n        self.model = YOLO('~/models/yolo/yolov8n.engine', task='detect')\n        self.bridge = CvBridge()\n        self.inference_times = []\n\n        # Subscriptions and publications\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/color/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.detections_pub = self.create_publisher(\n            Detection2DArray,\n            '/perception/detections',\n            10\n        )\n\n        self.debug_pub = self.create_publisher(\n            Image,\n            '/perception/detections_debug',\n            10\n        )\n\n        # Parameters (adjustable via ROS 2 CLI)\n        self.declare_parameter('confidence_threshold', 0.5)\n        self.declare_parameter('nms_threshold', 0.45)\n        self.declare_parameter('input_size', 640)\n        self.declare_parameter('enable_debug', True)\n\n        self.get_logger().info('YOLO Detector initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera image\"\"\"\n        try:\n            # Convert ROS Image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n            height, width = cv_image.shape[:2]\n\n            # Run inference\n            start_time = time.time()\n\n            results = self.model.predict(\n                source=cv_image,\n                conf=self.get_parameter('confidence_threshold').value,\n                iou=self.get_parameter('nms_threshold').value,\n                imgsz=self.get_parameter('input_size').value,\n                device=0,  # GPU device\n                verbose=False\n            )\n\n            inference_time = (time.time() - start_time) * 1000  # Convert to ms\n            self.inference_times.append(inference_time)\n\n            # Keep running average of last 30 inferences\n            if len(self.inference_times) > 30:\n                self.inference_times.pop(0)\n\n            avg_latency = np.mean(self.inference_times)\n            fps = 1000 / avg_latency if avg_latency > 0 else 0\n\n            # Parse detections\n            detections_msg = Detection2DArray()\n            detections_msg.header = msg.header\n\n            result = results[0]  # Single image\n\n            if result.boxes is not None:\n                for box in result.boxes:\n                    detection = Detection2D()\n                    detection.header = msg.header\n\n                    # Bounding box coordinates\n                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                    detection.bbox.center.x = float((x1 + x2) / 2)\n                    detection.bbox.center.y = float((y1 + y2) / 2)\n                    detection.bbox.size_x = float(x2 - x1)\n                    detection.bbox.size_y = float(y2 - y1)\n\n                    # Class and confidence\n                    class_id = int(box.cls.item())\n                    confidence = float(box.conf.item())\n                    class_name = result.names[class_id]\n\n                    hypothesis = ObjectHypothesisWithPose()\n                    hypothesis.hypothesis.class_name = class_name\n                    hypothesis.hypothesis.score = confidence\n                    detection.results.append(hypothesis)\n\n                    detections_msg.detections.append(detection)\n\n            # Publish detections\n            self.detections_pub.publish(detections_msg)\n\n            # Debug visualization (optional, CPU-intensive)\n            if self.get_parameter('enable_debug').value:\n                annotated_frame = result.plot()\n\n                # Add performance metrics\n                cv2.putText(\n                    annotated_frame,\n                    f'Latency: {avg_latency:.1f}ms | FPS: {fps:.1f}',\n                    (10, 30),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    0.7,\n                    (0, 255, 0),\n                    2\n                )\n\n                # Publish debug image\n                debug_msg = self.bridge.cv2_to_imgmsg(annotated_frame, 'bgr8')\n                debug_msg.header = msg.header\n                self.debug_pub.publish(debug_msg)\n\n            # Log statistics every 30 frames\n            if len(self.inference_times) % 30 == 0:\n                self.get_logger().info(\n                    f'Detections: {len(detections_msg.detections)} | '\n                    f'Latency: {avg_latency:.1f}ms | FPS: {fps:.1f}'\n                )\n\n        except Exception as e:\n            self.get_logger().error(f'Detection error: {str(e)}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    detector = YOLODetector()\n    rclpy.spin(detector)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Performance Metrics on Jetson Orin Nano:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"YOLOv8 Nano Model:\n\u251c\u2500 Model size: 6.3 MB\n\u251c\u2500 Parameters: 3.2M\n\u251c\u2500 FLOPs: 8.7 G (inference at 640\xd7480)\n\u251c\u2500 TensorRT Engine: 1.5 MB\n\u251c\u2500 Latency (INT8): 12-15 ms\n\u251c\u2500 FPS (input 640\xd7480 @ 30Hz): 60-80 FPS\n\u2514\u2500 Memory usage: 180-220 MB\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-5-semantic-segmentation",children:"Part 5: Semantic Segmentation"}),"\n",(0,s.jsx)(n.h3,{id:"semantic-segmentation-configuration",children:"Semantic Segmentation Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# semantic_segmentation_config.yaml\nsemantic_segmentation_node:\n  ros__parameters:\n    # Input topics\n    image_topic: '/camera/color/image_raw'\n    camera_info_topic: '/camera/color/camera_info'\n\n    # Model configuration\n    model_engine_file_path: '~/models/segmentation/deeplabv3_nano.engine'\n    input_size: [512, 512]  # Input resolution for model\n\n    # Inference settings\n    confidence_threshold: 0.5\n    device_id: 0  # GPU device\n\n    # Number of classes in segmentation\n    num_classes: 21  # PASCAL VOC dataset\n    # Classes: background, person, bicycle, car, dog, cat, etc.\n\n    # Output settings\n    output_colormap: 'pascal_voc'  # Standard robotics colormap\n    publish_mask: true\n    publish_colored: true\n\n    # Performance tuning\n    enable_tensorrt: true\n    quantization_type: 'INT8'  # FP32, FP16, INT8\n    max_batch_size: 1\n"})}),"\n",(0,s.jsx)(n.h3,{id:"semantic-segmentation-node",children:"Semantic Segmentation Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nSemantic Segmentation for Scene Understanding\nClassifies each pixel into semantic categories\n\"\"\"\n\nimport rclpy\nimport numpy as np\nimport cv2\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport time\n\nclass SemanticSegmentation(Node):\n    def __init__(self):\n        super().__init__('semantic_segmentation')\n\n        self.bridge = CvBridge()\n\n        # PASCAL VOC colormap (standard for robotics)\n        self.colormap = self.create_pascal_voc_colormap()\n\n        # Subscriptions\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/color/image_raw',\n            self.segment_callback,\n            10\n        )\n\n        # Publications\n        self.mask_pub = self.create_publisher(\n            Image,\n            '/perception/segmentation/mask',\n            10\n        )\n\n        self.colored_pub = self.create_publisher(\n            Image,\n            '/perception/segmentation/colored',\n            10\n        )\n\n        # Load pre-trained DeepLabV3 model\n        # For custom robotics classes, fine-tune on robot dataset\n        try:\n            import torch\n            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n            self.model = torch.hub.load(\n                'pytorch/vision:v0.10.0',\n                'deeplabv3_mobilenet_v3_large',\n                pretrained=True\n            )\n            self.model.to(self.device)\n            self.model.eval()\n            self.get_logger().info(f'Segmentation model loaded on {self.device}')\n        except Exception as e:\n            self.get_logger().error(f'Failed to load model: {str(e)}')\n\n    def segment_callback(self, msg):\n        \"\"\"Process image and generate segmentation mask\"\"\"\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, 'rgb8')\n            h, w = cv_image.shape[:2]\n\n            # Preprocess: resize to 512\xd7512 (DeepLabV3 standard input)\n            input_tensor = self.preprocess_image(cv_image)\n\n            # Inference\n            start = time.time()\n            with torch.no_grad():\n                output = self.model(input_tensor)\n            latency = (time.time() - start) * 1000\n\n            # Post-process: get predicted class per pixel\n            logits = output['out']  # Shape: [1, num_classes, 512, 512]\n            predicted_mask = torch.argmax(logits, dim=1)[0]  # Shape: [512, 512]\n\n            # Resize to original image dimensions\n            predicted_mask_np = predicted_mask.cpu().numpy().astype(np.uint8)\n            segmentation_map = cv2.resize(\n                predicted_mask_np,\n                (w, h),\n                interpolation=cv2.INTER_NEAREST\n            )\n\n            # Create colored visualization\n            colored_seg = self.colorize_segmentation(segmentation_map)\n\n            # Publish raw mask\n            mask_msg = self.bridge.cv2_to_imgmsg(segmentation_map, 'mono8')\n            mask_msg.header = msg.header\n            self.mask_pub.publish(mask_msg)\n\n            # Publish colored version\n            colored_msg = self.bridge.cv2_to_imgmsg(colored_seg, 'rgb8')\n            colored_msg.header = msg.header\n            self.colored_pub.publish(colored_msg)\n\n            if segmentation_map.size % 100 == 0:\n                self.get_logger().info(\n                    f'Segmentation latency: {latency:.1f}ms'\n                )\n\n        except Exception as e:\n            self.get_logger().error(f'Segmentation error: {str(e)}')\n\n    def preprocess_image(self, cv_image):\n        \"\"\"Normalize and resize for model\"\"\"\n        import torch\n        from torchvision import transforms\n\n        # Convert BGR to RGB\n        rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n\n        # Standard ImageNet normalization\n        preprocess = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Resize((512, 512)),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n\n        # Apply transforms\n        from PIL import Image as PILImage\n        pil_image = PILImage.fromarray(rgb_image)\n        tensor = preprocess(pil_image)\n\n        # Add batch dimension\n        tensor = tensor.unsqueeze(0).to(self.device)\n        return tensor\n\n    def create_pascal_voc_colormap(self):\n        \"\"\"Generate PASCAL VOC colormap (21 classes)\"\"\"\n        # Standard colormap used in computer vision\n        colormap = np.zeros((21, 3), dtype=np.uint8)\n\n        colors = [\n            (0, 0, 0),        # background\n            (128, 0, 0),      # person\n            (0, 128, 0),      # bicycle\n            (128, 128, 0),    # car\n            (0, 0, 128),      # dog\n            (128, 0, 128),    # cat\n            (0, 128, 128),    # cow\n            (128, 128, 128),  # horse\n            (64, 0, 0),       # sheep\n            (192, 0, 0),      # airplane\n            (64, 128, 0),     # train\n            (192, 128, 0),    # boat\n            (64, 0, 128),     # bottle\n            (192, 0, 128),    # chair\n            (64, 128, 128),   # table\n            (192, 128, 128),  # dog\n            (0, 64, 0),       # cat\n            (128, 64, 0),     # monitor\n            (0, 192, 0),      # keyboard\n            (128, 192, 0),    # mouse\n            (0, 64, 128),     # sofa\n        ]\n\n        for i, color in enumerate(colors[:21]):\n            colormap[i] = color\n\n        return colormap\n\n    def colorize_segmentation(self, mask):\n        \"\"\"Convert grayscale mask to colored using colormap\"\"\"\n        colored = np.zeros((*mask.shape, 3), dtype=np.uint8)\n        for class_id in np.unique(mask):\n            colored[mask == class_id] = self.colormap[class_id]\n        return colored\n\ndef main(args=None):\n    rclpy.init(args=args)\n    seg_node = SemanticSegmentation()\n    rclpy.spin(seg_node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-6-instance-segmentation-mask-r-cnn",children:"Part 6: Instance Segmentation (Mask R-CNN)"}),"\n",(0,s.jsx)(n.h3,{id:"instance-segmentation-node",children:"Instance Segmentation Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nInstance Segmentation: Detection + Segmentation Masks\nIdentifies individual objects with pixel-precise masks\n\"\"\"\n\nimport rclpy\nimport numpy as np\nimport cv2\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, Detection2D\nfrom cv_bridge import CvBridge\nimport torch\nfrom torchvision.models.detection import maskrcnn_resnet50_fpn\nimport time\n\nclass InstanceSegmentation(Node):\n    def __init__(self):\n        super().__init__('instance_segmentation')\n\n        self.bridge = CvBridge()\n\n        # Load Mask R-CNN (pre-trained on COCO)\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model = maskrcnn_resnet50_fpn(pretrained=True)\n        self.model.to(self.device)\n        self.model.eval()\n\n        # COCO class names (80 classes)\n        self.coco_classes = [\n            'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n            'bus', 'train', 'truck', 'boat', 'traffic light',\n            'fire hydrant', 'stop sign', 'parking meter', 'bench', 'cat',\n            'dog', 'horse', 'sheep', 'cow', 'elephant',\n            # ... (60 more classes)\n        ]\n\n        # Subscriptions\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/color/image_raw',\n            self.instance_segment_callback,\n            10\n        )\n\n        # Publications\n        self.detections_pub = self.create_publisher(\n            Detection2DArray,\n            '/perception/instance_detections',\n            10\n        )\n\n        self.mask_pub = self.create_publisher(\n            Image,\n            '/perception/instance_masks',\n            10\n        )\n\n        self.visualization_pub = self.create_publisher(\n            Image,\n            '/perception/instance_visualization',\n            10\n        )\n\n        self.get_logger().info('Instance Segmentation node initialized')\n\n    def instance_segment_callback(self, msg):\n        \"\"\"Process image and generate instance masks\"\"\"\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, 'rgb8')\n            h, w = cv_image.shape[:2]\n\n            # Preprocess: convert to tensor\n            image_tensor = torch.from_numpy(cv_image).permute(2, 0, 1).float() / 255.0\n            image_tensor = image_tensor.unsqueeze(0).to(self.device)\n\n            # Inference\n            start = time.time()\n            with torch.no_grad():\n                predictions = self.model(image_tensor)\n            latency = (time.time() - start) * 1000\n\n            pred = predictions[0]\n            masks = pred['masks']  # Shape: [num_objects, 1, H, W]\n            boxes = pred['boxes']\n            labels = pred['labels']\n            scores = pred['scores']\n\n            # Filter by confidence threshold\n            confidence_threshold = 0.5\n            keep_idx = torch.where(scores > confidence_threshold)[0]\n\n            masks = masks[keep_idx]\n            boxes = boxes[keep_idx]\n            labels = labels[keep_idx]\n            scores = scores[keep_idx]\n\n            # Create instance ID map (different color per object)\n            instance_map = np.zeros((h, w), dtype=np.uint8)\n            colored_instances = np.zeros((h, w, 3), dtype=np.uint8)\n\n            # Generate detections message\n            detections_msg = Detection2DArray()\n            detections_msg.header = msg.header\n\n            # Colors for visualization (distinct per instance)\n            colors = [\n                (0, 255, 0),      # Green\n                (0, 0, 255),      # Red\n                (255, 0, 0),      # Blue\n                (0, 255, 255),    # Cyan\n                (255, 0, 255),    # Magenta\n                (255, 255, 0),    # Yellow\n            ]\n\n            for inst_id, (mask, box, label, score) in enumerate(\n                zip(masks, boxes, labels, scores)\n            ):\n                # Get mask binary\n                mask_np = mask.squeeze(0).cpu().numpy() > 0.5\n                instance_map[mask_np] = inst_id + 1\n\n                # Assign color\n                color = colors[inst_id % len(colors)]\n                colored_instances[mask_np] = color\n\n                # Create detection message\n                detection = Detection2D()\n                detection.header = msg.header\n                detection.bbox.center.x = float((box[0] + box[2]) / 2)\n                detection.bbox.center.y = float((box[1] + box[3]) / 2)\n                detection.bbox.size_x = float(box[2] - box[0])\n                detection.bbox.size_y = float(box[3] - box[1])\n\n                # Class info\n                class_name = self.coco_classes[label.item() - 1] if label < len(self.coco_classes) else 'unknown'\n                from vision_msgs.msg import ObjectHypothesisWithPose\n                hyp = ObjectHypothesisWithPose()\n                hyp.hypothesis.class_name = class_name\n                hyp.hypothesis.score = float(score.item())\n                detection.results.append(hyp)\n\n                detections_msg.detections.append(detection)\n\n            # Publish results\n            self.detections_pub.publish(detections_msg)\n\n            mask_msg = self.bridge.cv2_to_imgmsg(instance_map, 'mono8')\n            mask_msg.header = msg.header\n            self.mask_pub.publish(mask_msg)\n\n            # Publish visualization\n            cv_image_bgr = cv2.cvtColor(cv_image, cv2.COLOR_RGB2BGR)\n            viz = cv2.addWeighted(cv_image_bgr, 0.7,\n                                 cv2.cvtColor(colored_instances, cv2.COLOR_RGB2BGR), 0.3, 0)\n\n            for box, label, score in zip(boxes, labels, scores):\n                x1, y1, x2, y2 = box.cpu().numpy().astype(int)\n                cv2.rectangle(viz, (x1, y1), (x2, y2), (0, 255, 0), 2)\n                text = f'{self.coco_classes[label.item()-1]}: {score.item():.2f}'\n                cv2.putText(viz, text, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n            viz_msg = self.bridge.cv2_to_imgmsg(viz, 'bgr8')\n            viz_msg.header = msg.header\n            self.visualization_pub.publish(viz_msg)\n\n            self.get_logger().info(\n                f'Instances: {len(detections_msg.detections)} | '\n                f'Latency: {latency:.1f}ms'\n            )\n\n        except Exception as e:\n            self.get_logger().error(f'Instance segmentation error: {str(e)}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    inst_seg = InstanceSegmentation()\n    rclpy.spin(inst_seg)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-7-combining-perception-with-slam-and-navigation",children:"Part 7: Combining Perception with SLAM and Navigation"}),"\n",(0,s.jsx)(n.h3,{id:"full-perception-pipeline-launch-file",children:"Full Perception Pipeline Launch File"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# perception_pipeline.launch.py\n\"\"\"\nComplete perception stack: SLAM + Vision + Navigation\nRuns all components with proper topic routing\n\"\"\"\n\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node, PushRosNamespace\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nimport os\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    # Camera launch\n    realsense_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            get_package_share_directory('realsense2_camera'),\n            '/launch/rs_launch.py'\n        ]),\n        launch_arguments={\n            'rgb_camera.color_profile': '640,480,30',\n            'depth_module.depth_profile': '640,480,30',\n            'enable_depth': 'true',\n            'enable_infra1': 'false',\n            'enable_infra2': 'false',\n            'align_depth': 'true',\n        }.items()\n    )\n\n    # SLAM node\n    slam_node = Node(\n        package='isaac_ros_visual_slam',\n        executable='visual_slam_node',\n        parameters=[{\n            'camera_info_topic': '/camera/color/camera_info',\n            'rgb_image_topic': '/camera/color/image_raw',\n            'depth_image_topic': '/camera/depth/image_rect_raw',\n            'enable_loop_closure': True,\n            'enable_depth_regularization': True,\n            'map_type': 'occupancy_grid',\n        }],\n        remappings=[\n            ('camera_info_in', '/camera/color/camera_info'),\n            ('image_rgb', '/camera/color/image_raw'),\n        ]\n    )\n\n    # Object detection\n    yolo_node = Node(\n        package='perception_nodes',  # Your custom package\n        executable='yolo_detector.py',\n        parameters=[{\n            'confidence_threshold': 0.5,\n            'input_size': 640,\n            'enable_debug': True,\n        }]\n    )\n\n    # Semantic segmentation\n    segmentation_node = Node(\n        package='perception_nodes',\n        executable='semantic_segmentation.py',\n        parameters=[{\n            'output_colormap': 'pascal_voc',\n            'publish_mask': True,\n        }]\n    )\n\n    # Instance segmentation\n    instance_node = Node(\n        package='perception_nodes',\n        executable='instance_segmentation.py'\n    )\n\n    # Navigation stack\n    nav2_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            get_package_share_directory('nav2_bringup'),\n            '/launch/bringup_launch.py'\n        ]),\n        launch_arguments={\n            'use_sim_time': 'false',\n            'params_file': os.path.join(\n                get_package_share_directory('my_robot'),\n                'config', 'nav2_params.yaml'\n            ),\n        }.items()\n    )\n\n    # RViz visualization\n    rviz_node = Node(\n        package='rviz2',\n        executable='rviz2',\n        arguments=['-d', os.path.join(\n            get_package_share_directory('my_robot'),\n            'rviz', 'perception.rviz'\n        )]\n    )\n\n    return LaunchDescription([\n        realsense_launch,\n        slam_node,\n        yolo_node,\n        segmentation_node,\n        instance_node,\n        nav2_launch,\n        rviz_node,\n    ])\n"})}),"\n",(0,s.jsx)(n.h3,{id:"python-integration-perception-based-robot-control",children:"Python Integration: Perception-Based Robot Control"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nHigh-level robot control using perception outputs\nExample: Navigate to detected objects and grasp them\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom nav2_msgs.action import NavigateToPose\nfrom vision_msgs.msg import Detection2DArray\nfrom geometry_msgs.msg import PoseStamped, Twist\nimport math\n\nclass PerceptionControlSystem(Node):\n    def __init__(self):\n        super().__init__(\'perception_control\')\n\n        # Navigation action client\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n\n        # Subscriptions to perception outputs\n        self.detections_sub = self.create_subscription(\n            Detection2DArray,\n            \'/perception/detections\',\n            self.detections_callback,\n            10\n        )\n\n        # Velocity publisher for reactive control\n        self.vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        self.target_object = None\n        self.robot_state = \'idle\'  # idle, navigating, grasping\n\n        self.get_logger().info(\'Perception control system initialized\')\n\n    def detections_callback(self, msg):\n        """React to detected objects"""\n\n        if len(msg.detections) == 0:\n            self.get_logger().debug(\'No objects detected\')\n            return\n\n        # Find "target_object" class\n        for detection in msg.detections:\n            if detection.results:\n                class_name = detection.results[0].hypothesis.class_name\n                confidence = detection.results[0].hypothesis.score\n\n                if class_name == \'target_object\' and confidence > 0.7:\n                    self.target_object = detection\n                    self.approach_object(detection)\n                    break\n\n    def approach_object(self, detection):\n        """Navigate to detected object"""\n\n        # Get object position in image\n        center_x = detection.bbox.center.x\n        center_y = detection.bbox.center.y\n\n        # Image-space error\n        image_width = 640\n        error_x = center_x - image_width / 2\n\n        if abs(error_x) > 50:\n            # Object not centered - rotate to face it\n            twist = Twist()\n            twist.angular.z = float(0.5 * error_x / image_width)\n            self.vel_pub.publish(twist)\n        else:\n            # Object centered - move towards it\n            twist = Twist()\n            twist.linear.x = 0.3  # Move forward at 30 cm/s\n            self.vel_pub.publish(twist)\n\n    def send_navigation_goal(self, goal_x, goal_y):\n        """Send autonomous navigation goal via Nav2"""\n\n        goal = NavigateToPose.Goal()\n        goal.pose = PoseStamped()\n        goal.pose.header.frame_id = \'map\'\n        goal.pose.pose.position.x = goal_x\n        goal.pose.pose.position.y = goal_y\n        goal.pose.pose.orientation.z = math.sin(0)  # 0 radians\n        goal.pose.pose.orientation.w = math.cos(0)\n\n        self.nav_client.wait_for_server()\n        self.nav_client.send_goal_async(goal)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    controller = PerceptionControlSystem()\n    rclpy.spin(controller)\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-exercise-real-time-object-detection-and-segmentation",children:"Hands-On Exercise: Real-Time Object Detection and Segmentation"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-1-deploy-yolo-and-measure-performance",children:"Exercise 1: Deploy YOLO and Measure Performance"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Terminal 1: Start RealSense camera\nros2 launch realsense2_camera rs_launch.py \\\n  rgb_camera.color_profile:="640,480,30" \\\n  enable_depth:=true\n\n# Terminal 2: Run YOLO detector\nros2 run perception_nodes yolo_detector.py\n\n# Terminal 3: Monitor performance\nros2 topic hz /perception/detections\n# Expected: 20-25 Hz (60-80 FPS inference rate)\n\n# Terminal 4: View detections\nros2 run rqt_image_view rqt_image_view \\\n  --topics /perception/detections_debug\n\n# Terminal 5: Profile resource usage\nnvidia-smi -l 1  # Monitor GPU usage\n# Expected: 2-3 GB VRAM, 40-50% GPU utilization\n\n# Challenge: Modify yolo_detector.py to use YOLOv8s (medium model)\n# Compare latency difference\n'})}),"\n",(0,s.jsx)(n.h3,{id:"exercise-2-create-custom-yolo-training-dataset",children:"Exercise 2: Create Custom YOLO Training Dataset"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nCollect training data from robot camera for custom object detection\n"""\n\nimport rclpy\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport os\nfrom datetime import datetime\n\nclass DataCollector(Node):\n    def __init__(self):\n        super().__init__(\'data_collector\')\n\n        self.bridge = CvBridge()\n        self.dataset_dir = f\'dataset_{datetime.now().strftime("%Y%m%d_%H%M%S")}\'\n        os.makedirs(self.dataset_dir, exist_ok=True)\n        self.frame_count = 0\n\n        self.create_subscription(Image, \'/camera/color/image_raw\', self.save_frame, 10)\n\n    def save_frame(self, msg):\n        """Save frames for manual annotation"""\n        cv_image = self.bridge.imgmsg_to_cv2(msg, \'bgr8\')\n\n        # Save every 10th frame\n        if self.frame_count % 10 == 0:\n            filename = os.path.join(self.dataset_dir, f\'frame_{self.frame_count:06d}.jpg\')\n            cv2.imwrite(filename, cv_image)\n            self.get_logger().info(f\'Saved {filename}\')\n\n        self.frame_count += 1\n\n# After collecting 500 frames, use label-img for annotation:\n# pip install labelImg\n# labelImg <dataset_dir> --yolo\n\n# Then fine-tune YOLO:\n# yolo detect train data=custom_data.yaml epochs=100 device=0\n'})}),"\n",(0,s.jsx)(n.h3,{id:"exercise-3-debug-common-perception-errors",children:"Exercise 3: Debug Common Perception Errors"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Error"}),(0,s.jsx)(n.th,{children:"Cause"}),(0,s.jsx)(n.th,{children:"Fix"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Very low FPS (<5)"})}),(0,s.jsx)(n.td,{children:"Model too large for Jetson"}),(0,s.jsx)(n.td,{children:"Use YOLOv8n instead of YOLOv8m"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Out of memory crash"})}),(0,s.jsx)(n.td,{children:"Models competing for VRAM"}),(0,s.jsx)(n.td,{children:"Reduce batch size, disable SLAM"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"High latency (>100ms)"})}),(0,s.jsx)(n.td,{children:"No TensorRT optimization"}),(0,s.jsx)(n.td,{children:"Convert ONNX to TensorRT engine"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Missed detections"})}),(0,s.jsx)(n.td,{children:"Low confidence threshold"}),(0,s.jsx)(n.td,{children:"Reduce confidence_threshold parameter"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"False positives"})}),(0,s.jsx)(n.td,{children:"Model overtrained on wrong data"}),(0,s.jsx)(n.td,{children:"Fine-tune on robot's target objects"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Topic lag"})}),(0,s.jsx)(n.td,{children:"ROS 2 network bottleneck"}),(0,s.jsx)(n.td,{children:"Use compressed image transport"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Computer Vision Tasks"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Detection identifies objects (bounding boxes)"}),"\n",(0,s.jsx)(n.li,{children:"Segmentation classifies pixels (scene understanding)"}),"\n",(0,s.jsx)(n.li,{children:"Instance segmentation combines both (precise object masks)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Edge Inference Optimization"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"TensorRT reduces inference latency by 3-4\xd7"}),"\n",(0,s.jsx)(n.li,{children:"INT8 quantization sacrifices <1% accuracy for 3-4\xd7 speedup"}),"\n",(0,s.jsx)(n.li,{children:"Model selection matters: YOLOv8n for Jetson, YOLOv8l for powerful GPUs"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"ROS 2 Integration Patterns"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Perception nodes subscribe to camera topics"}),"\n",(0,s.jsx)(n.li,{children:"Publish structured detection/segmentation messages"}),"\n",(0,s.jsx)(n.li,{children:"Navigation and manipulation subscribe to perception outputs"}),"\n",(0,s.jsx)(n.li,{children:"RViz visualizes perception pipeline for debugging"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Real-Time Constraints"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Jetson Orin Nano can run YOLO at 60+ FPS"}),"\n",(0,s.jsx)(n.li,{children:"Combined SLAM + Vision + Navigation requires careful resource allocation"}),"\n",(0,s.jsx)(n.li,{children:"Memory management is critical (8GB VRAM shared between OS, SLAM, vision)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Debugging Perception"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Always profile latency (nvidia-smi, ros2 node info)"}),"\n",(0,s.jsx)(n.li,{children:"Use RViz to visualize intermediate outputs"}),"\n",(0,s.jsx)(n.li,{children:"Test inference speed offline before deploying to robot"}),"\n",(0,s.jsx)(n.li,{children:"Keep separate confidence thresholds for different use cases"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsx)(n.h3,{id:"official-documentation",children:"Official Documentation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/",children:"NVIDIA Isaac ROS Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.nvidia.com/isaac-sim/",children:"NVIDIA Isaac Sim"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/ros-perception/vision_msgs",children:"ROS 2 Vision Messages"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.nvidia.com/deeplearning/tensorrt/",children:"NVIDIA TensorRT"})}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"research-papers",children:"Research Papers"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["YOLO: ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1506.02640",children:"You Only Look Once - Real-Time Object Detection"})]}),"\n",(0,s.jsxs)(n.li,{children:["Mask R-CNN: ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1703.06870",children:"Instance Segmentation by Detecting Objects and Segmenting Masks"})]}),"\n",(0,s.jsxs)(n.li,{children:["DeepLabV3: ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1706.05587",children:"Rethinking Atrous Convolution for Semantic Image Segmentation"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"open-source-tools",children:"Open Source Tools"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/ultralytics/ultralytics",children:"Ultralytics YOLOv8"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/facebookresearch/detectron2",children:"Detectron2 (Facebook)"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/opencv/opencv",children:"OpenCV for Computer Vision"})}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"next-lesson",children:"Next Lesson"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Lesson 4.1: Vision-Language-Action (VLA) Models"})}),"\n",(0,s.jsx)(n.p,{children:"Now that your humanoid robot can perceive its environment (SLAM + Object Detection), we'll add language understanding. You'll learn to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate large language models (GPT-4, Gemini, open-source alternatives)"}),"\n",(0,s.jsx)(n.li,{children:"Parse natural language commands into robot actions"}),"\n",(0,s.jsx)(n.li,{children:'Build end-to-end "visual question answering" for robots'}),"\n",(0,s.jsx)(n.li,{children:"Deploy LLMs efficiently on edge devices with quantization"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example Workflow:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'User: "Pick up the red cube and place it on the table"\n    \u2193\n[Speech-to-text] (not covered in this course)\n    \u2193\n"Pick up the red cube and place it on the table"\n    \u2193\n[Vision-Language Model]\n    \u251c\u2500 Detect: Red cube at (234, 456, 567, 789)\n    \u251c\u2500 Parse: Action sequence [grasp, place]\n    \u251c\u2500 Plan: Approach cube \u2192 Open gripper \u2192 Move to table \u2192 Close gripper\n    \u2193\n[Robot Execution]\n    \u251c\u2500 Navigate to cube location (SLAM + Nav2)\n    \u251c\u2500 Grasp cube (Manipulation controller)\n    \u251c\u2500 Navigate to table (SLAM + Nav2)\n    \u251c\u2500 Place cube (Gripper control)\n    \u2193\n\u2705 Task Complete\n'})})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:function(e,n,i){i.d(n,{R:function(){return r},x:function(){return a}});var t=i(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);