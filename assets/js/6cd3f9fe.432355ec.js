"use strict";(self.webpackChunkphysical_ai_textbook=self.webpackChunkphysical_ai_textbook||[]).push([[909],{6878:function(n,e,s){s.r(e),s.d(e,{assets:function(){return l},contentTitle:function(){return t},default:function(){return m},frontMatter:function(){return o},metadata:function(){return i},toc:function(){return c}});var i=JSON.parse('{"id":"chapter-2/2-3-sensors-unity","title":"Sensor Simulation and High-Fidelity Visualization with Unity","description":"Simulate realistic sensors in Gazebo and render robots with photorealistic graphics in Unity","source":"@site/docs/chapter-2/2-3-sensors-unity.md","sourceDirName":"chapter-2","slug":"/chapter-2/2-3-sensors-unity","permalink":"/physical-ai-textbook/docs/chapter-2/2-3-sensors-unity","draft":false,"unlisted":false,"editUrl":"https://github.com/Bil4l-Mehmood/physical-ai-textbook/edit/main/docs/chapter-2/2-3-sensors-unity.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"sidebar_label":"Lesson 2.3: Sensors & Unity","title":"Sensor Simulation and High-Fidelity Visualization with Unity","description":"Simulate realistic sensors in Gazebo and render robots with photorealistic graphics in Unity","duration":90,"difficulty":"Intermediate","hardware":["Ubuntu 22.04 LTS","ROS 2 Humble","Gazebo Harmonic","Unity 2022+ (optional)"],"prerequisites":["Lesson 2.1: Gazebo Fundamentals","Lesson 2.2: URDF/SDF & Physics"]},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2.2: URDF/SDF & Physics","permalink":"/physical-ai-textbook/docs/chapter-2/2-2-urdf-sdf-physics"},"next":{"title":"Lesson 3.1: NVIDIA Isaac Sim","permalink":"/physical-ai-textbook/docs/chapter-3/3-1-isaac-sim-basics"}}'),r=s(4848),a=s(8453);const o={sidebar_position:3,sidebar_label:"Lesson 2.3: Sensors & Unity",title:"Sensor Simulation and High-Fidelity Visualization with Unity",description:"Simulate realistic sensors in Gazebo and render robots with photorealistic graphics in Unity",duration:90,difficulty:"Intermediate",hardware:["Ubuntu 22.04 LTS","ROS 2 Humble","Gazebo Harmonic","Unity 2022+ (optional)"],prerequisites:["Lesson 2.1: Gazebo Fundamentals","Lesson 2.2: URDF/SDF & Physics"]},t="Lesson 2.3: Sensor Simulation & Unity Integration",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Part 1: Gazebo Sensor Simulation",id:"part-1-gazebo-sensor-simulation",level:2},{value:"Camera Sensor",id:"camera-sensor",level:3},{value:"LiDAR Sensor (2D Laser Scan)",id:"lidar-sensor-2d-laser-scan",level:3},{value:"IMU Sensor (Accelerometer + Gyroscope)",id:"imu-sensor-accelerometer--gyroscope",level:3},{value:"Part 2: Reading Sensor Data in ROS 2",id:"part-2-reading-sensor-data-in-ros-2",level:2},{value:"Python Script to Process Camera Images",id:"python-script-to-process-camera-images",level:3},{value:"Python Script to Process LiDAR Data",id:"python-script-to-process-lidar-data",level:3},{value:"Python Script to Process IMU Data",id:"python-script-to-process-imu-data",level:3},{value:"Part 3: Unity Integration for High-Fidelity Visualization",id:"part-3-unity-integration-for-high-fidelity-visualization",level:2},{value:"Setting Up ROS-Unity Bridge",id:"setting-up-ros-unity-bridge",level:3},{value:"Unity Script for Robot Control",id:"unity-script-for-robot-control",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Step 1: Create Sensor-Equipped Robot SDF",id:"step-1-create-sensor-equipped-robot-sdf",level:3},{value:"Step 2: Create ROS 2 Sensor Processing Nodes",id:"step-2-create-ros-2-sensor-processing-nodes",level:3},{value:"Step 3: Launch All Sensors",id:"step-3-launch-all-sensors",level:3},{value:"Step 4: Verify Sensor Data",id:"step-4-verify-sensor-data",level:3},{value:"Exercises",id:"exercises",level:3},{value:"Common Sensor Simulation Issues",id:"common-sensor-simulation-issues",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(n){const e={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"lesson-23-sensor-simulation--unity-integration",children:"Lesson 2.3: Sensor Simulation & Unity Integration"})}),"\n",(0,r.jsxs)(e.admonition,{title:"Lesson Overview",type:"info",children:[(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Duration"}),": 90 minutes | ",(0,r.jsx)(e.strong,{children:"Difficulty"}),": Intermediate | ",(0,r.jsx)(e.strong,{children:"Hardware"}),": Ubuntu 22.04 + ROS 2 Humble + Gazebo + Unity (optional)"]}),(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Prerequisites"}),": Lessons 2.1 and 2.2"]}),(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Learning Outcome"}),": Simulate realistic sensors in Gazebo and visualize robots with high-fidelity graphics in Unity"]})]}),"\n",(0,r.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(e.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Understand sensor types and their simulation models"}),"\n",(0,r.jsx)(e.li,{children:"Configure camera sensors with realistic parameters"}),"\n",(0,r.jsx)(e.li,{children:"Simulate LiDAR point clouds for navigation"}),"\n",(0,r.jsx)(e.li,{children:"Simulate IMU (accelerometer, gyroscope) data"}),"\n",(0,r.jsx)(e.li,{children:"Publish sensor data to ROS 2 topics"}),"\n",(0,r.jsx)(e.li,{children:"Integrate Gazebo with Unity for photorealistic rendering"}),"\n",(0,r.jsx)(e.li,{children:"Use ROS-Unity bridge for real-time data streaming"}),"\n",(0,r.jsx)(e.li,{children:"Optimize sensor simulation performance"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"part-1-gazebo-sensor-simulation",children:"Part 1: Gazebo Sensor Simulation"}),"\n",(0,r.jsxs)(e.admonition,{title:"Why Simulate Sensors?",type:"note",children:[(0,r.jsx)(e.p,{children:"Real sensors are expensive and slow to debug. Simulation allows:"}),(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Cost reduction"}),": No hardware damage during development"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Iteration speed"}),": Test algorithms at 100x faster than real-time"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Repeatability"}),": Identical conditions for each test"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Realism"}),": Add noise/lag to match real hardware"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Scaling"}),": Simulate multiple robots simultaneously"]}),"\n"]})]}),"\n",(0,r.jsx)(e.h3,{id:"camera-sensor",children:"Camera Sensor"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Add to SDF model --\x3e\n<link name="camera_link">\n  <inertial>\n    <mass>0.1</mass>\n    <inertia>\n      <ixx>0.001</ixx>\n      <iyy>0.001</iyy>\n      <izz>0.001</izz>\n    </inertia>\n  </inertial>\n\n  <visual name="visual">\n    <geometry>\n      <box>\n        <size>0.05 0.05 0.05</size>\n      </box>\n    </geometry>\n  </visual>\n\n  <collision name="collision">\n    <geometry>\n      <box>\n        <size>0.05 0.05 0.05</size>\n      </box>\n    </geometry>\n  </collision>\n\n  \x3c!-- Camera Sensor Plugin --\x3e\n  <sensor name="camera" type="camera">\n    <pose>0 0 0 0 0 0</pose>\n    <update_rate>30</update_rate>  \x3c!-- 30 Hz --\x3e\n    <camera>\n      <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees --\x3e\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.01</near>  \x3c!-- Minimum distance in meters --\x3e\n        <far>300</far>     \x3c!-- Maximum distance in meters --\x3e\n      </clip>\n      \x3c!-- Lens distortion (optional) --\x3e\n      <lens>\n        <type>quadratic</type>\n        <scale_to_hfov>true</scale_to_hfov>\n        <cutoff_angle>1.5707</cutoff_angle>\n        <intrinsics>\n          <fx>554.254</fx>   \x3c!-- Focal length in pixels --\x3e\n          <fy>554.254</fy>\n          <cx>320.0</cx>     \x3c!-- Principal point --\x3e\n          <cy>240.0</cy>\n          <s>0</s>           \x3c!-- Skew --\x3e\n        </intrinsics>\n      </lens>\n    </camera>\n\n    \x3c!-- ROS 2 Plugin for publishing camera data --\x3e\n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n      <ros>\n        \x3c!-- Camera topics will be published under this namespace --\x3e\n        <namespace>camera</namespace>\n        <remapping>image_raw:=image</remapping>\n        <remapping>camera_info:=info</remapping>\n      </ros>\n      <camera_name>camera</camera_name>\n      <frame_name>camera_link</frame_name>\n      <hack_baseline>0.07</hack_baseline>  \x3c!-- For stereo --\x3e\n      <distortion_k1>0.0</distortion_k1>   \x3c!-- Lens distortion --\x3e\n      <distortion_k2>0.0</distortion_k2>\n      <distortion_k3>0.0</distortion_k3>\n      <distortion_t1>0.0</distortion_t1>\n      <distortion_t2>0.0</distortion_t2>\n    </plugin>\n  </sensor>\n</link>\n\n\x3c!-- Joint to attach camera to robot --\x3e\n<joint name="camera_joint" type="fixed">\n  <parent>base_link</parent>\n  <child>camera_link</child>\n  <pose>0.05 0 0.1 0 0 0</pose>  \x3c!-- Mount on front-top of robot --\x3e\n</joint>\n'})}),"\n",(0,r.jsx)(e.h3,{id:"lidar-sensor-2d-laser-scan",children:"LiDAR Sensor (2D Laser Scan)"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:'\x3c!-- 2D LiDAR for range measurements --\x3e\n<link name="lidar_link">\n  <inertial>\n    <mass>0.05</mass>\n    <inertia>\n      <ixx>0.0001</ixx>\n      <iyy>0.0001</iyy>\n      <izz>0.0001</izz>\n    </inertia>\n  </inertial>\n\n  <visual name="visual">\n    <geometry>\n      <cylinder>\n        <radius>0.03</radius>\n        <length>0.05</length>\n      </cylinder>\n    </geometry>\n  </visual>\n\n  <collision name="collision">\n    <geometry>\n      <cylinder>\n        <radius>0.03</radius>\n        <length>0.05</length>\n      </cylinder>\n    </geometry>\n  </collision>\n\n  \x3c!-- Ray sensor (LIDAR) --\x3e\n  <sensor name="lidar" type="ray">\n    <pose>0 0 0 0 0 0</pose>\n    <update_rate>40</update_rate>  \x3c!-- 40 Hz --\x3e\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>360</samples>      \x3c!-- 360 rays --\x3e\n          <resolution>1</resolution>  \x3c!-- 1 degree per ray --\x3e\n          <min_angle>-3.14159</min_angle>\n          <max_angle>3.14159</max_angle>\n        </horizontal>\n        <vertical>\n          <samples>1</samples>        \x3c!-- 2D LIDAR only --\x3e\n        </vertical>\n      </scan>\n      <range>\n        <min>0.2</min>              \x3c!-- 20 cm minimum range --\x3e\n        <max>30</max>               \x3c!-- 30 m maximum range --\x3e\n        <resolution>0.01</resolution>\n      </range>\n      \x3c!-- Add noise to match real hardware --\x3e\n      <noise>\n        <type>gaussian</type>\n        <mean>0</mean>\n        <stddev>0.01</stddev>  \x3c!-- 1 cm std dev --\x3e\n      </noise>\n    </ray>\n\n    \x3c!-- ROS 2 Plugin for LaserScan topic --\x3e\n    <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n      <ros>\n        <namespace>lidar</namespace>\n        <remapping>~/out:=scan</remapping>\n      </ros>\n      <frame_name>lidar_link</frame_name>\n    </plugin>\n  </sensor>\n</link>\n\n<joint name="lidar_joint" type="fixed">\n  <parent>base_link</parent>\n  <child>lidar_link</child>\n  <pose>0 0 0.15 0 0 0</pose>\n</joint>\n'})}),"\n",(0,r.jsx)(e.h3,{id:"imu-sensor-accelerometer--gyroscope",children:"IMU Sensor (Accelerometer + Gyroscope)"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:'\x3c!-- IMU for motion sensing --\x3e\n<link name="imu_link">\n  <inertial>\n    <mass>0.01</mass>\n    <inertia>\n      <ixx>0.00001</ixx>\n      <iyy>0.00001</iyy>\n      <izz>0.00001</izz>\n    </inertia>\n  </inertial>\n\n  \x3c!-- IMU Sensor --\x3e\n  <sensor name="imu" type="imu">\n    <pose>0 0 0 0 0 0</pose>\n    <update_rate>200</update_rate>  \x3c!-- 200 Hz --\x3e\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0</mean>\n            <stddev>0.002</stddev>  \x3c!-- 0.2 deg/s noise --\x3e\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0</mean>\n            <stddev>0.002</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0</mean>\n            <stddev>0.002</stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0</mean>\n            <stddev>0.017</stddev>  \x3c!-- 0.017 m/s^2 noise --\x3e\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0</mean>\n            <stddev>0.017</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0</mean>\n            <stddev>0.017</stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n\n    \x3c!-- ROS 2 Plugin for IMU data --\x3e\n    <plugin name="imu_controller" filename="libgazebo_ros_imu_sensor.so">\n      <ros>\n        <namespace>imu</namespace>\n        <remapping>~/out:=data</remapping>\n      </ros>\n      <initial_orientation_as_reference>false</initial_orientation_as_reference>\n      <frame_name>imu_link</frame_name>\n    </plugin>\n  </sensor>\n</link>\n\n<joint name="imu_joint" type="fixed">\n  <parent>base_link</parent>\n  <child>imu_link</child>\n</joint>\n'})}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"part-2-reading-sensor-data-in-ros-2",children:"Part 2: Reading Sensor Data in ROS 2"}),"\n",(0,r.jsx)(e.h3,{id:"python-script-to-process-camera-images",children:"Python Script to Process Camera Images"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nSubscribe to camera images from Gazebo simulation\nand process them with OpenCV\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\n\nclass CameraProcessor(Node):\n    \"\"\"Process camera images from Gazebo\"\"\"\n\n    def __init__(self):\n        super().__init__('camera_processor')\n\n        # Subscribe to camera topic\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image',\n            self.image_callback,\n            10\n        )\n\n        # OpenCV bridge for ROS Image \u2194 OpenCV Mat conversion\n        self.bridge = CvBridge()\n\n        # For video output\n        self.fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        self.out = None\n\n        self.get_logger().info('Camera processor initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera frame\"\"\"\n        try:\n            # Convert ROS Image to OpenCV format\n            cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n            # Image processing example: edge detection\n            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n            edges = cv2.Canny(gray, 100, 200)\n\n            # Display processed image\n            cv2.imshow('Camera Feed', cv_image)\n            cv2.imshow('Edges', edges)\n            cv2.waitKey(1)\n\n            # Log statistics\n            self.get_logger().debug(\n                f'Image: {cv_image.shape}, Mean color: {cv_image.mean():.1f}'\n            )\n\n        except Exception as e:\n            self.get_logger().error(f'Image processing failed: {e}')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    processor = CameraProcessor()\n    rclpy.spin(processor)\n    processor.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(e.h3,{id:"python-script-to-process-lidar-data",children:"Python Script to Process LiDAR Data"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nSubscribe to LiDAR laser scans and detect obstacles\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nimport numpy as np\n\n\nclass LiDARProcessor(Node):\n    \"\"\"Process LiDAR scans from Gazebo\"\"\"\n\n    def __init__(self):\n        super().__init__('lidar_processor')\n\n        # Subscribe to laser scan\n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            '/lidar/scan',\n            self.scan_callback,\n            10\n        )\n\n        self.obstacle_threshold = 1.0  # Detect obstacles within 1 meter\n        self.get_logger().info('LiDAR processor initialized')\n\n    def scan_callback(self, msg):\n        \"\"\"Process incoming laser scan\"\"\"\n        try:\n            # msg.ranges is array of distances (float)\n            ranges = np.array(msg.ranges)\n\n            # Filter out invalid readings\n            valid_ranges = ranges[(ranges > msg.range_min) & (ranges < msg.range_max)]\n\n            # Detect obstacles\n            obstacles = valid_ranges[valid_ranges < self.obstacle_threshold]\n\n            if len(obstacles) > 0:\n                min_distance = np.min(obstacles)\n                self.get_logger().warn(\n                    f'\u26a0\ufe0f  Obstacle detected! Minimum distance: {min_distance:.2f}m'\n                )\n                # Could trigger emergency stop or obstacle avoidance here\n\n                # Find direction of closest obstacle\n                closest_idx = np.argmin(ranges)\n                angle = msg.angle_min + closest_idx * msg.angle_increment\n                self.get_logger().info(f'Obstacle at angle: {angle:.2f} rad')\n\n            else:\n                self.get_logger().debug('No obstacles in range')\n\n            # Statistics\n            self.get_logger().debug(\n                f'LiDAR: {len(valid_ranges)} valid readings, '\n                f'min={np.min(valid_ranges):.2f}m, '\n                f'max={np.max(valid_ranges):.2f}m'\n            )\n\n        except Exception as e:\n            self.get_logger().error(f'Scan processing failed: {e}')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    processor = LiDARProcessor()\n    rclpy.spin(processor)\n    processor.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(e.h3,{id:"python-script-to-process-imu-data",children:"Python Script to Process IMU Data"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nSubscribe to IMU data and detect motion/orientation\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nimport numpy as np\nimport math\n\n\nclass IMUProcessor(Node):\n    """Process IMU data from Gazebo"""\n\n    def __init__(self):\n        super().__init__(\'imu_processor\')\n\n        # Subscribe to IMU\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        self.get_logger().info(\'IMU processor initialized\')\n\n    def imu_callback(self, msg):\n        """Process incoming IMU data"""\n        # Linear acceleration (m/s^2)\n        accel = msg.linear_acceleration\n        accel_magnitude = math.sqrt(accel.x**2 + accel.y**2 + accel.z**2)\n\n        # Angular velocity (rad/s)\n        angular = msg.angular_velocity\n        angular_magnitude = math.sqrt(angular.x**2 + angular.y**2 + angular.z**2)\n\n        # Orientation (quaternion)\n        orientation = msg.orientation\n\n        # Convert quaternion to Euler angles\n        roll, pitch, yaw = self.quaternion_to_euler(\n            orientation.x, orientation.y, orientation.z, orientation.w\n        )\n\n        self.get_logger().info(\n            f\'Acceleration: {accel_magnitude:.2f} m/s^2 | \'\n            f\'Rotation: {angular_magnitude:.2f} rad/s | \'\n            f\'Orientation: Roll={roll:.2f}, Pitch={pitch:.2f}, Yaw={yaw:.2f}\'\n        )\n\n    @staticmethod\n    def quaternion_to_euler(x, y, z, w):\n        """Convert quaternion to Euler angles"""\n        # Roll (rotation around X axis)\n        sin_roll = 2.0 * (w * x + y * z)\n        cos_roll = 1.0 - 2.0 * (x * x + y * y)\n        roll = math.atan2(sin_roll, cos_roll)\n\n        # Pitch (rotation around Y axis)\n        sin_pitch = 2.0 * (w * y - z * x)\n        sin_pitch = np.clip(sin_pitch, -1.0, 1.0)\n        pitch = math.asin(sin_pitch)\n\n        # Yaw (rotation around Z axis)\n        sin_yaw = 2.0 * (w * z + x * y)\n        cos_yaw = 1.0 - 2.0 * (y * y + z * z)\n        yaw = math.atan2(sin_yaw, cos_yaw)\n\n        return roll, pitch, yaw\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    processor = IMUProcessor()\n    rclpy.spin(processor)\n    processor.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"part-3-unity-integration-for-high-fidelity-visualization",children:"Part 3: Unity Integration for High-Fidelity Visualization"}),"\n",(0,r.jsx)(e.admonition,{title:"Why Use Both Gazebo and Unity?",type:"tip",children:(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Gazebo"}),": Fast physics simulation, perfect for algorithm development"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Unity"}),": Photorealistic rendering, human-in-the-loop testing, demos"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Together"}),": Realistic simulation + beautiful visualization"]}),"\n"]})}),"\n",(0,r.jsx)(e.h3,{id:"setting-up-ros-unity-bridge",children:"Setting Up ROS-Unity Bridge"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Install ROS-Unity bridge in Unity project\n# Download from GitHub: https://github.com/Unity-Technologies/ROS-TCP-Connector\n\n# In Unity:\n# 1. Window \u2192 TextMesh Pro \u2192 Import TMP Essential Resources\n# 2. Assets \u2192 Import Package \u2192 Custom Package (ROS-TCP-Connector.unitypackage)\n# 3. Add ROSConnection GameObject to scene\n# 4. Configure ROS Master URI\n"})}),"\n",(0,r.jsx)(e.h3,{id:"unity-script-for-robot-control",children:"Unity Script for Robot Control"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Std;\nusing RosMessageTypes.Sensor;\nusing UnityEngine;\n\npublic class RobotController : MonoBehaviour\n{\n    private ROSConnection ros;\n\n    // Robot transform\n    private Transform robotBase;\n\n    // Sensor displays\n    private Texture2D cameraImage;\n    private RawImage imageDisplay;\n\n    void Start()\n    {\n        // Get ROS connection\n        ros = ROSConnection.GetOrCreateInstance();\n\n        // Register subscribers for sensor topics\n        ros.Subscribe<Image>("/camera/image", CameraImageCallback);\n        ros.Subscribe<LaserScan>("/lidar/scan", LiDARCallback);\n        ros.Subscribe<Imu>("/imu/data", IMUCallback);\n\n        // Find robot in scene\n        robotBase = transform.Find("robot_base");\n        imageDisplay = GetComponent<RawImage>();\n    }\n\n    void Update()\n    {\n        // Example: Move robot with keyboard input\n        float moveX = Input.GetAxis("Horizontal");\n        float moveZ = Input.GetAxis("Vertical");\n\n        robotBase.Translate(new Vector3(moveX, 0, moveZ) * 5f * Time.deltaTime);\n    }\n\n    void CameraImageCallback(Image image)\n    {\n        // Convert ROS Image to Unity Texture2D\n        // Process and display in UI\n        Debug.Log($"Camera received: {image.width}x{image.height}");\n    }\n\n    void LiDARCallback(LaserScan scan)\n    {\n        Debug.Log($"LiDAR received: {scan.ranges.Length} rays");\n\n        // Visualize point cloud\n        for (int i = 0; i < scan.ranges.Length; i++)\n        {\n            float distance = (float)scan.ranges[i];\n            float angle = (float)(scan.angle_min + i * scan.angle_increment);\n\n            // Convert polar to Cartesian\n            float x = Mathf.Cos(angle) * distance;\n            float z = Mathf.Sin(angle) * distance;\n\n            // Draw point in world\n            Debug.DrawLine(robotBase.position, robotBase.position + new Vector3(x, 0, z), Color.green);\n        }\n    }\n\n    void IMUCallback(Imu imuData)\n    {\n        Debug.Log($"IMU acceleration: {imuData.linear_acceleration.x}");\n\n        // Could update robot orientation based on IMU\n        // robotBase.rotation = ...\n    }\n}\n'})}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Create a robot with camera, LiDAR, and IMU sensors and process all sensor data."]}),"\n",(0,r.jsx)(e.h3,{id:"step-1-create-sensor-equipped-robot-sdf",children:"Step 1: Create Sensor-Equipped Robot SDF"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Create complete world with sensors\nmkdir -p ~/gazebo_ws/src/worlds\ncat > ~/gazebo_ws/src/worlds/sensors_world.sdf << 'EOF'\n# Copy complete SDF with camera, LiDAR, IMU from Parts 1-2\nEOF\n"})}),"\n",(0,r.jsx)(e.h3,{id:"step-2-create-ros-2-sensor-processing-nodes",children:"Step 2: Create ROS 2 Sensor Processing Nodes"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"mkdir -p ~/gazebo_ws/src/sensor_processing\n# Copy camera_processor.py, lidar_processor.py, imu_processor.py\nchmod +x ~/gazebo_ws/src/sensor_processing/*.py\n"})}),"\n",(0,r.jsx)(e.h3,{id:"step-3-launch-all-sensors",children:"Step 3: Launch All Sensors"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Terminal 1: Gazebo with sensors\nros2 launch gazebo_ros gazebo.launch.py world:=$HOME/gazebo_ws/src/worlds/sensors_world.sdf\n\n# Terminal 2: Camera processor\npython3 ~/gazebo_ws/src/sensor_processing/camera_processor.py\n\n# Terminal 3: LiDAR processor\npython3 ~/gazebo_ws/src/sensor_processing/lidar_processor.py\n\n# Terminal 4: IMU processor\npython3 ~/gazebo_ws/src/sensor_processing/imu_processor.py\n"})}),"\n",(0,r.jsx)(e.h3,{id:"step-4-verify-sensor-data",children:"Step 4: Verify Sensor Data"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Monitor all active topics\nros2 topic list\n\n# View camera images\nros2 topic echo /camera/image_raw | head -20\n\n# View LiDAR ranges\nros2 topic echo /lidar/scan | head -20\n\n# View IMU data\nros2 topic echo /imu/data | head -20\n"})}),"\n",(0,r.jsx)(e.h3,{id:"exercises",children:"Exercises"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Obstacle Detection"}),": Detect when LiDAR range < 1 meter"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Motion Detection"}),": Log when IMU acceleration > 2 m/s\xb2"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Object Detection"}),": Apply OpenCV blob detection to camera images"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Fall Detection"}),": Use IMU to detect if robot falls over"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Unity Integration"}),": Create a simple Unity scene displaying sensor data"]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"common-sensor-simulation-issues",children:"Common Sensor Simulation Issues"}),"\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"Issue"}),(0,r.jsx)(e.th,{children:"Cause"}),(0,r.jsx)(e.th,{children:"Solution"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"No camera images"}),(0,r.jsx)(e.td,{children:"Plugin not loaded"}),(0,r.jsxs)(e.td,{children:["Check ",(0,r.jsx)(e.code,{children:"libgazebo_ros_camera.so"})," path"]})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"LiDAR no data"}),(0,r.jsx)(e.td,{children:"Ray sensor misconfigured"}),(0,r.jsxs)(e.td,{children:["Verify ",(0,r.jsx)(e.code,{children:"<scan>"})," horizontal samples"]})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"IMU values zero"}),(0,r.jsx)(e.td,{children:"World lacks gravity"}),(0,r.jsxs)(e.td,{children:["Ensure gravity in physics ",(0,r.jsx)(e.code,{children:"<gravity>0 0 -9.81</gravity>"})]})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Noise too high"}),(0,r.jsx)(e.td,{children:"stddev too large"}),(0,r.jsx)(e.td,{children:"For cameras use 0.01, for IMU use 0.002"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Sensor slow"}),(0,r.jsx)(e.td,{children:"Update rate too high"}),(0,r.jsx)(e.td,{children:"Start with 30 Hz for camera, 40 for LiDAR"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"ROS topic empty"}),(0,r.jsx)(e.td,{children:"Plugin namespace wrong"}),(0,r.jsx)(e.td,{children:"Check topic names match plugin config"})]})]})]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(e.p,{children:["\u2705 ",(0,r.jsx)(e.strong,{children:"Sensor Types"}),": Camera (RGB), LiDAR (range), IMU (motion)"]}),"\n",(0,r.jsxs)(e.p,{children:["\u2705 ",(0,r.jsx)(e.strong,{children:"Realistic Simulation"}),": Add noise, lag, and field-of-view constraints"]}),"\n",(0,r.jsxs)(e.p,{children:["\u2705 ",(0,r.jsx)(e.strong,{children:"ROS 2 Publishing"}),": Gazebo plugins automatically publish to ROS topics"]}),"\n",(0,r.jsxs)(e.p,{children:["\u2705 ",(0,r.jsx)(e.strong,{children:"Data Processing"}),": Use Python + OpenCV for real-time sensor processing"]}),"\n",(0,r.jsxs)(e.p,{children:["\u2705 ",(0,r.jsx)(e.strong,{children:"Hybrid Simulation"}),": Gazebo for physics + Unity for photorealism"]}),"\n",(0,r.jsxs)(e.p,{children:["\u2705 ",(0,r.jsx)(e.strong,{children:"Sensor Fusion"}),": Combine camera + LiDAR + IMU for robust perception"]}),"\n",(0,r.jsxs)(e.p,{children:["\u2705 ",(0,r.jsx)(e.strong,{children:"Optimization"}),": Balance accuracy vs. computational cost"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\ud83d\udcd6 ",(0,r.jsx)(e.a,{href:"https://gazebosim.org/api/gazebo_plugins/index.html",children:"Gazebo Sensor Plugins"})]}),"\n",(0,r.jsxs)(e.li,{children:["\ud83d\udcd6 ",(0,r.jsx)(e.a,{href:"https://github.com/Unity-Technologies/ROS-TCP-Connector",children:"ROS-Unity Bridge Documentation"})]}),"\n",(0,r.jsxs)(e.li,{children:["\ud83d\udcd6 ",(0,r.jsx)(e.a,{href:"https://docs.ros.org/en/humble/Concepts/Intermediate/About-ROS-2-Messages.html#sensor-messages",children:"Sensor Message Formats"})]}),"\n",(0,r.jsxs)(e.li,{children:["\ud83d\udcd6 ",(0,r.jsx)(e.a,{href:"https://docs.ros.org/en/humble/Tutorials/Computer-Vision/OpenCV-Python-Example.html",children:"OpenCV with ROS 2"})]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Chapter 2 Complete!"})," Next: Chapter 3 - The AI-Robot Brain (NVIDIA Isaac)"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Questions?"})," See ",(0,r.jsx)(e.a,{href:"/physical-ai-textbook/docs/faq",children:"FAQ"})," or ",(0,r.jsx)(e.a,{href:"https://github.com/physical-ai-course/physical-ai-textbook/discussions",children:"GitHub Discussions"})]})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:function(n,e,s){s.d(e,{R:function(){return o},x:function(){return t}});var i=s(6540);const r={},a=i.createContext(r);function o(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);